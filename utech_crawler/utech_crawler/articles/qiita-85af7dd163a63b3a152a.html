<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>Inside of Deep Learning あるいは深層学習は何を変えるのか - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="シリコンバレーのエンジニアが一年ほどをDL(Deep Learning)を追いかけてみて思ったこと、感じたこと、伝えたいことをまとめてみました。とにかく伝えたいことは、DLはもはやその一言では片付けられないほどに構造やアプローチが多様化しているということ。そしてその進化スピードがえげつないほど速いということです。

将来のプログラミングや問題解決の仕方を変え、人を取り巻く環境を変えていくかもしれないというじりじりとした圧迫感。これを少しでも伝えられればと思っています。
..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="Jiny2001" name="twitter:creator" /><meta content="Inside of Deep Learning あるいは深層学習は何を変えるのか - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="シリコンバレーのエンジニアが一年ほどをDL(Deep Learning)を追いかけてみて思ったこと、感じたこと、伝えたいことをまとめてみました。とにかく伝えたいことは、DLはもはやその一言では片付けられないほどに構造やアプローチが多様..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="0bq2lIUh3GDdsFfZqWL1FEgYg5W4WtEIBIxXL+ohODnPMg960OuTAqKHXZkKwzmKNo+i9hrkP5eU6xP8ajM5BA==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"jiny2001","type":"items","id":"85af7dd163a63b3a152a"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-87357932-14b1-4bf2-9930-d3ed55f0bf49"></div>
    <div id="HeaderContainer-react-component-87357932-14b1-4bf2-9930-d3ed55f0bf49"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/DeepLearning",        "name": "DeepLearning"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">Inside of Deep Learning あるいは深層学習は何を変えるのか</h1><ul class="TagList"><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="229"><a class="u-link-unstyled TagList__label" href="/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92"><img alt="深層学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/9594cfcb56d9180f74c468e56c69ce9f69cbe6ee/medium.jpg?1480640899" /><span>深層学習</span></a></li><li class="TagList__item" data-count="814"><a class="u-link-unstyled TagList__label" href="/tags/MachineLearning"><img alt="MachineLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/b85c97772bddbfbb48a8b116669349c7ec92e4bf/medium.jpg?1395227038" /><span>MachineLearning</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">280</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="0 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>0</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:280,&quot;uuid&quot;:&quot;85af7dd163a63b3a152a&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="matarillo"><a itemprop="url" href="/matarillo"><img alt="matarillo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/2079/profile-images/1473680702" /></a></li><li class="js-hovercard" data-hovercard-target-name="Leonhalt2714"><a itemprop="url" href="/Leonhalt2714"><img alt="Leonhalt2714" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/23418/profile-images/1473683834" /></a></li><li class="js-hovercard" data-hovercard-target-name="toronya"><a itemprop="url" href="/toronya"><img alt="toronya" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31723/profile-images/1473685756" /></a></li><li class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></li><li class="js-hovercard" data-hovercard-target-name="inoory"><a itemprop="url" href="/inoory"><img alt="inoory" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/61468/profile-images/1473695664" /></a></li><li class="js-hovercard" data-hovercard-target-name="Tsutomu-KKE@github"><a itemprop="url" href="/Tsutomu-KKE@github"><img alt="Tsutomu-KKE@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/13955/profile-images/1473683126" /></a></li><li class="js-hovercard" data-hovercard-target-name="manabedaiki"><a itemprop="url" href="/manabedaiki"><img alt="manabedaiki" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/104597/profile-images/1473709346" /></a></li><li class="js-hovercard" data-hovercard-target-name="TokyoMickey"><a itemprop="url" href="/TokyoMickey"><img alt="TokyoMickey" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/141216/profile-images/1483272512" /></a></li><li class="js-hovercard" data-hovercard-target-name="ryo_grid"><a itemprop="url" href="/ryo_grid"><img alt="ryo_grid" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/12325/profile-images/1473682364" /></a></li><li class="js-hovercard" data-hovercard-target-name="GushiSnow"><a itemprop="url" href="/GushiSnow"><img alt="GushiSnow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/10496/profile-images/1473757289" /></a></li><li><a href="/jiny2001/items/85af7dd163a63b3a152a/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/jiny2001"><img class="thumb thumb--xs" src="https://avatars.githubusercontent.com/u/12959344?v=3" alt="12959344?v=3" /></a> <a class="u-link-unstyled" href="/jiny2001">jiny2001</a> </div><div class="ArticleAsideHeader__date"><meta content="2017-01-14T19:25:11+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2017-01-14">Edited at <time datetime="2017-01-27T18:40:42+09:00" itemprop="dateModified">2017-01-27</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/jiny2001/items/85af7dd163a63b3a152a/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">46</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/jiny2001/items/85af7dd163a63b3a152a/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(46)</span></a></li><li><a href="/jiny2001/items/85af7dd163a63b3a152a.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-85af7dd163a63b3a152a" itemprop="articleBody"><p>シリコンバレーのエンジニアが一年ほどをDL(Deep Learning)を追いかけてみて思ったこと、感じたこと、伝えたいことをまとめてみました。とにかく伝えたいことは、DLはもはやその一言では片付けられないほどに構造やアプローチが多様化しているということ。そしてその進化スピードがえげつないほど速いということです。</p>

<p>将来のプログラミングや問題解決の仕方を変え、人を取り巻く環境を変えていくかもしれないというじりじりとした圧迫感。これを少しでも伝えられればと思っています。</p>

<h1>
<span id="このポストの方針" class="fragment"></span><a href="#%E3%81%93%E3%81%AE%E3%83%9D%E3%82%B9%E3%83%88%E3%81%AE%E6%96%B9%E9%87%9D"><i class="fa fa-link"></i></a>このポストの方針</h1>

<ul>
<li><p>技術部分の説明は初心者向け。各構成要素など基礎から解説します。今からDLをキャッチアップしていく人には多分丁度良いです。</p></li>
<li><p>最初と最後だけ読むのも良いですが、各部の技術的な部分や難しさはできるだけ短く分かりやすく書くつもりですしここが一番大事なところです。できれば時間のあるときにじっくり読んでもらえればと思います。</p></li>
</ul>

<h1>
<span id="内容" class="fragment"></span><a href="#%E5%86%85%E5%AE%B9"><i class="fa fa-link"></i></a>内容</h1>

<p><strong>DLは何を変えたか：</strong>あるいはDLとは何か？<br>
<strong>基本要素：</strong>DLを構成する要素とその仕組み。<br>
<strong>DLのデメリット：</strong>DLシステムを構築するのがいかに難しいか。<br>
<strong>各種改善手法・最新事例：</strong>各種のモデルや手法の紹介。ここがメインです。<br>
<strong>DLは何を変えるのか?：</strong>エンジニアを取り巻く環境はどう変わるか？</p>

<h1>
<span id="dlは何を変えたか" class="fragment"></span><a href="#dl%E3%81%AF%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B"><i class="fa fa-link"></i></a>DLは何を変えたか</h1>

<p>DLは各分野で飛躍的な性能を発揮していますが、何故そのようなジャンプが可能になったのでしょうか。ここでは僕の感じるポイントをまとめてみます。</p>

<h2>
<span id="1-ビッグデータ対応" class="fragment"></span><a href="#1-%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%AF%BE%E5%BF%9C"><i class="fa fa-link"></i></a>1. ビッグデータ対応</h2>

<p>既存の一般的な機械学習のモデルに対して飛躍的に大きなデータを入力として扱えるようになりました。<br>
例えば碁のAIであるAlphaGoでは従来とは比べものにならない量の対戦データを初期フェーズで学習しています。今までの手法ではメモリやコンピューティングパワーの限界にあったデータしか扱えませんでした。</p>

<p>DLは超大量のデータでも学習できる工夫があります。さらに、データの量が多ければ多いほど性能を上げることができるという特徴があります。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/d8105ec3-1c63-874f-eb45-00702f315246.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/d8105ec3-1c63-874f-eb45-00702f315246.png"></a></div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://www.slideshare.net/ExtractConf/andrew-ng-chief-scientist-at-baidu/30" rel="nofollow noopener" target="_blank">Extract Data Conference from SlideShare</a>
</li>
</ul>

<h2>
<span id="2-複雑多層モデルの実現" class="fragment"></span><a href="#2-%E8%A4%87%E9%9B%91%E5%A4%9A%E5%B1%A4%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AE%9F%E7%8F%BE"><i class="fa fa-link"></i></a>2. 複雑／多層モデルの実現</h2>

<p>今までの機械学習アルゴリズムでは例えば人間がパラメータをチューニングしていたとします。これに対して破壊的に複雑なモデルが可能になりました。具体的なモデルは後で説明しますが、システム全体で数千万以上のパラメータや10以上のレイヤーを持っていても学習できるようになりました。</p>

<p>例えば初期の物体認識ネットであるAlexNet ではパラメータが約 6000 万個もあります。表現力の高い複雑なモデルでも学習してしまうことができるのです。（下図はAlexNetのモデル図）</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/d6cca561-f00d-ca92-8925-74f557ef571b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/d6cca561-f00d-ca92-8925-74f557ef571b.png"></a></div>

<h2>
<span id="3-汎化性能非線形システムへの信頼性の向上" class="fragment"></span><a href="#3-%E6%B1%8E%E5%8C%96%E6%80%A7%E8%83%BD%E9%9D%9E%E7%B7%9A%E5%BD%A2%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%B8%E3%81%AE%E4%BF%A1%E9%A0%BC%E6%80%A7%E3%81%AE%E5%90%91%E4%B8%8A"><i class="fa fa-link"></i></a>3. 汎化性能（非線形システムへの信頼性）の向上</h2>

<p>DLは非線形システムです。これは従来扱いが難しいものでした。さらに過学習という問題があって、例えば学習時に使ったのとは違うシチュエーションの入力データが入った時にとんでもない答えを出す可能性が出てしまうのです。これだと実用のシステムとしては信頼しにくいのです。</p>

<p>DLでは非線形なシステムでも過学習を抑え信頼性を上げることができました。汎化性能は大きく向上しています。</p>

<h2>
<span id="4-時系列データや多次元データへの拡張" class="fragment"></span><a href="#4-%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E3%82%84%E5%A4%9A%E6%AC%A1%E5%85%83%E3%83%87%E3%83%BC%E3%82%BF%E3%81%B8%E3%81%AE%E6%8B%A1%E5%BC%B5"><i class="fa fa-link"></i></a>4. 時系列データや多次元データへの拡張</h2>

<p>音声や文章、人の行動など、世の中のデータは時系列、可変長、多次元の複雑なデータです。DLではRNNやLSTMなどの構造を持つことで音声認識や自然言語処理などで精度の良い学習を可能にしました。これは従来プログラミング技術的にもいろいろと難しい分野でした。</p>

<h2>
<span id="dlとは何か何を変えたか" class="fragment"></span><a href="#dl%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B"><i class="fa fa-link"></i></a>DLとは何か？何を変えたか？</h2>

<p>大枠的には上記で説明したように「非線形で、複雑なモデルあるいは大量のパラメータを持ち、大規模なデータや時系列のデータなどにも対応できるなどの工夫を備えたニューラルネットベースの機械学習システム」という認識で問題ないでしょう。</p>

<p>ただ、当初使われていたDLという言葉が指すモデルとは大きく離れたモデルがどんどん提案されてきています。とにかくここがエキサイティングなところで、後にじっくりと見ていきたいと思います。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/bbc3d40a-0b8c-7cf7-8acf-346a81e6cd50.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/bbc3d40a-0b8c-7cf7-8acf-346a81e6cd50.png"></a></div>

<h4>
<span id="何を変えたか" class="fragment"></span><a href="#%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B"><i class="fa fa-link"></i></a>何を変えたか？</h4>

<p>上記の特徴により画像処理や音声認識、自然言語処理の分野で大きな性能の飛躍を可能にしました。これはいくつかの分野で単に性能を押し上げたということだけでは留まらなかったのです。</p>

<p>上記分野は写真や音声、文章とどれも人間が外界とのインタフェースとして使っているものです。つまりは人間と直接交わる部分であって、人間の生活に関わるものをAIで豊かにしていく可能性が開いたわけです。</p>

<p>そしてこのロマンと可能性が世界中の優秀な科学者・技術者を大量にDLの道に引き込む導線になりました。この変化がある意味一番大きな変化のようにも思えます。シリコンバレーではほぼ毎日のようにどこかでＤLや機械学習のミートアップが開催されてます。一過性のブームでは終わらないエンジニアを惹きつけるものがあるのだと感じます。</p>

<h1>
<span id="基本要素" class="fragment"></span><a href="#%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><i class="fa fa-link"></i></a>基本要素</h1>

<p>全体が長くなってしまったのでこのパートは分割してもう一つのポストにしました。DLはブラックボックスで性能改善が難しいと良く言われていますが、いろいろな方法論が提唱されています。各要素の持つ特徴や役割を理解しておけばDLモデルの改善はそれほど難解なものではありません。</p>

<p><a href="http://qiita.com/jiny2001/items/e3961a009690af0c435c" id="reference-36e917e830a0f2b54e43">Inside of Deep Learning （ディープラーニングの基本要素）</a> </p>

<h1>
<span id="dlのデメリット" class="fragment"></span><a href="#dl%E3%81%AE%E3%83%87%E3%83%A1%E3%83%AA%E3%83%83%E3%83%88"><i class="fa fa-link"></i></a>DLのデメリット</h1>

<h2>
<span id="過学習しやすい" class="fragment"></span><a href="#%E9%81%8E%E5%AD%A6%E7%BF%92%E3%81%97%E3%82%84%E3%81%99%E3%81%84"><i class="fa fa-link"></i></a>過学習しやすい</h2>

<p>機械学習システムの本質的な問題は<br>
誤：「学習データに対してシステムを最適化する」ことではなく<br>
正：「<strong>学習データに含まれない未知の入力データ</strong>に対してシステムを最適化する」ことです。</p>

<p>過学習と言ってトレーニング用のデータに対して解答を最適化しすぎると逆に実際の性能が落ちてしまうという問題があります。下の図は購入するときの家の大きさと価格のサンプルです。y = ax<sup>2</sup> + bx + c の2次式（中央）で推定すれば全体的にフィットしますが、より表現力の高い6次式（右）で推定しようとすると実際とは大きく離れた推定モデルになってしまいます。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/dc21319e-6fb5-aa76-34b4-b9e18891553d.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/dc21319e-6fb5-aa76-34b4-b9e18891553d.png"></a></div>

<p>from <a href="http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/general_concepts.html" rel="nofollow noopener" target="_blank">scikit-learn:Machine Learning 101</a> </p>

<p>特に非線形システムは線形システムに比べて過学習しやすく、予測範囲外の出力が出る可能性が高くなります。例えば下の図で、線形のシステム（左）ではp1とp2の間の入力に対する出力値はp1とp2の間になりますが、非線形のシステム（右）では予期しない値が出ることがあります。これは自動運転など安全が優先されるシステムでは特に大きな問題となります。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/15fa60b7-d504-54ad-3db8-04dc911424eb.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/15fa60b7-d504-54ad-3db8-04dc911424eb.jpeg"></a></div>

<h2>
<span id="大量の学習用データが必要" class="fragment"></span><a href="#%E5%A4%A7%E9%87%8F%E3%81%AE%E5%AD%A6%E7%BF%92%E7%94%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E5%BF%85%E8%A6%81"><i class="fa fa-link"></i></a>大量の学習用データが必要</h2>

<p>教師なし学習や強化学習などもありますが、やはり教師データを与えるシステムが多いです。特にディープラーニングでは汎化性能をあげるためにも従来に比べてかなり多くのデータが必要になります。規模や対象にもよりますが例えば画像認識では最低でも数万件以上の学習データが必要と言われています。<br>
また、例えば年齢や職業などから既婚状態、車の所持、年収（5段階）を予測する場合、出力クラスの全ての組み合わせである2x2x5=20のそれぞれの場合で十分な質と数のデータが必要になります。</p>

<p>正解データの作成に人手が必要な場合などでは非常にコストがかかってしまいます。<strong>このためにニューラルネットベースのシステムに移行できないという話は非常によく聞きます。</strong>またDLのブームの火付け役ともなった物体認識のタスクですが、これは当時に非常に大規模で質の良いデータセットが提供されていたこともあってDLと相性の良いタスクだったのでした。</p>

<h2>
<span id="膨大なコンピューティングリソースが必要" class="fragment"></span><a href="#%E8%86%A8%E5%A4%A7%E3%81%AA%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8C%E5%BF%85%E8%A6%81"><i class="fa fa-link"></i></a>膨大なコンピューティングリソースが必要</h2>

<p>さらにDLではモデルがディープで複雑なものになることが多く、学習用データが多いことも相まって計算に非常に時間がかかります。GPUも必須になりますし、クラウドを使って気軽に分散化できるものではありません。</p>

<p>特に学習フェーズでは実験に数日から一週間ほどかかるような論文も少なくないです。この場合はモデルの改善に何度も試行錯誤しながら試すのが難しくなります。例えば本番環境でソフトを走らせるまでには通常何回テストランを走らせるでしょうか。一回の実行に一日かかると考えればその難しさが伝わるかと思います。</p>

<h2>
<span id="パラメータの収束が難しい" class="fragment"></span><a href="#%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E5%8F%8E%E6%9D%9F%E3%81%8C%E9%9B%A3%E3%81%97%E3%81%84"><i class="fa fa-link"></i></a>パラメータの収束が難しい</h2>

<p>多数のレイヤーを持つ場合など、もともと数百万以上ある多数のパラメータをそれぞれ最適値へ収束させていくのは非常に難しくなります。バックプロパゲーションをかけて出力に近い層から学習させていった場合、正解との誤差がどんどん分散してしまって学習できなくなってしまうからです。またモデルが複雑になるほど鞍点と呼ばれる数次元で損失が極値になる点へ陥り、学習が進みにくくなるポイントも増えるからです。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png"></a></div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://www.cs.unc.edu/%7Ewliu/papers/GoogLeNet.pdf" rel="nofollow noopener" target="_blank">Going Deeper with Convolutions</a>
</li>
</ul>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png"></a></div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1512.03385v1.pdf" rel="nofollow noopener" target="_blank">Deep Residual Learning for Image Recognition</a>
</li>
</ul>

<p>上はInception-Net v1(GoogLeNet)のモデル図でレイヤーは合計で22層、計算要素は100程度あります。下のResNet-152は極端な例ではありますがレイヤー数が152層にもなります。Inception-Netの2年前に物体認識でDLが注目されるきっかけを作ったAlexNetは8層構造でした。まさに"The Deeper, the Better"と言われる所以です。</p>

<h2>
<span id="性能改善が難しい" class="fragment"></span><a href="#%E6%80%A7%E8%83%BD%E6%94%B9%E5%96%84%E3%81%8C%E9%9B%A3%E3%81%97%E3%81%84"><i class="fa fa-link"></i></a>性能改善が難しい</h2>

<p>DLのモデルはブラックボックスであるなどと良く表現されます。例えば下図の物体認識の場合、既存手法では特徴抽出や学習判別機などそれぞれの機能で問題があった場合はそれを検出して各ステップごとに改善を試みることができます。<br>
ところがDLの場合はこれらの処理を全部まとめて学習してしまいます。問題があった場合にどのステップの能力が低いのか判断しにくく、また改善するための方法も多岐に渡るためどれを選択して良いのか分かりません。また問題を把握するために数学的な技術が必要になることも多いです。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/91331a26-5aaf-c090-c4d5-1f0544cb97f1.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/91331a26-5aaf-c090-c4d5-1f0544cb97f1.png"></a></div>

<p>同様に<strong>既存手法の場合は人間が持っている知識を組み込むことができますがDLでは非常に難しくなることが多くなってしまいます。</strong>総じて言えばとにかくコストが高いということです。高い技術レベルのエンジニア、学習データ、コンピュータリソース、時間、ほぼ全ての領域において既存技術の数倍〜数十倍のコストがかかり、ハイリスク／ハイリターンのシステムという感覚がぴったりです。</p>

<p><em>またこれは日本人にしか当てはまらないことですが、最新論文が全て英語であり特にアメリカ、あるいは欧米諸国や中国でしか最新事例が生まれていないこともあると思います。とにかくDLシーンの進化のスピードが早いので、言葉と文化の壁や産業構造の違いが悪い方向へ出てしまっているように感じます。皆でDLやろう！</em></p>

<h1>
<span id="各種改善手法最新事例" class="fragment"></span><a href="#%E5%90%84%E7%A8%AE%E6%94%B9%E5%96%84%E6%89%8B%E6%B3%95%E6%9C%80%E6%96%B0%E4%BA%8B%E4%BE%8B"><i class="fa fa-link"></i></a>各種改善手法・最新事例</h1>

<p>いろいろなモデルや改善手法をみて見ましょう。ここからがやっと本ポストで書きたかった部分です。改善手法として大まかに4つの項目に分けてみました。</p>

<p>↑ Best<br>
* より表現力があり発散しにくいモデル、フレームワークを考案する<br>
* より多くのデータあるいはより精度の良いデータを使う<br>
* パラメータを収束させるための工夫<br>
* 汎化性能をあげるための工夫<br>
↓ Better</p>

<h2>
<span id="モデルの進化" class="fragment"></span><a href="#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%80%B2%E5%8C%96"><i class="fa fa-link"></i></a>モデルの進化</h2>

<ul>
<li>Inception Net (GoogLeNet)</li>
</ul>

<p>Googleが開発し大規模画像認識のコンテストILSVRC 2014で優勝した物体認識のためのモデルです。22層のレイヤーを持ち複数のCNNを並列に並べているのが特徴で、CNNのサイズやパラメータ数を抑える代わりに非常にディープにした構造です。3つの判定結果を使って総合的に判断し非常に高い性能を持っています。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png"></a></div>

<p>現在では順調に進化を繰り返し、Inception-Net v4やInception-ResNet v2が公開されています。Inception-Net v3はgithub上でソースコードが公開され、AndroidやiPhone上でビルドして世界最高峰の物体認識機能を利用することも可能です。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://github.com/tensorflow/models/tree/master/inception" rel="nofollow noopener" target="_blank">Inception in TensorFlow</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/mainyaa/items/4b4e4d47868133410a96" id="reference-0c8c1ddad34e00f7e938">お手軽にrealtime画像認識をAndroid/iOSで動かす</a>
</li>
</ul>

<hr>

<ul>
<li>ResNet</li>
</ul>

<p>ResNetはDeep Residual Learningという論文でMicrosoft Research Asiaにより発表されたもので、Residual(残差あるいは差分)をだけを学習するようにして非常に深いモデルでも効率良く学習できるようにしたモデルです。<br>
2014年のILSVRCではGoogleがInception Netで飛躍的な成果を出して優勝したところをその翌年にはMicrosoftがさらに飛躍的な成果を出してひっくり返すと言う動きの速さがDLシーンの見どころです。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/c6b87277-fc83-e050-d203-f3a68dc34a95.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/c6b87277-fc83-e050-d203-f3a68dc34a95.png"></a></div>

<p>上図 左の画像がその一部を図にしたもので、上図 中央がそれを簡略化したものです。入力データを保持しておいてCNNの結果に加えることで各学習器は入力データとの変化分のみを学習すればよくなります。また上図 右の画像は比較のためのモジュールです。3x3x64のCNNフィルタを2枚重ねるよりも、1x1x64, 3x3x64, 1x1x256の3枚のフィルタを使うことで同等の表現力をもちながらもパラメータの数を抑えています。</p>

<p>直感的な説明にすると次に見るRNNとLSTMの関係に似ています。例えば今までのモデルでは入力されたデータを後ろのレイヤーに伝えるためのウェイトも特徴抽出の為のウェイトと同時に学習せねばなりませんでした。そうするともともとのウェイトが高くなり層が深くなると発散しやすくなってしまうということのようです。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png" target="_blank" rel="nofollow noopener"><img width="632" alt="ResNet152.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png"></a></div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1512.03385v1.pdf" rel="nofollow noopener" target="_blank">Deep Residual Learning for Image Recognition</a>
</li>
</ul>

<p>Residual Ｎetも他で応用されています。例えば下のリンクにある単眼超解像という画像の解像度を上げる論文では、これを利用することで飛躍的に深く、かつ学習係数を従来の1万倍にして短時間でも学習できてしまうというモデルが提案されています。下の図では横軸が学習エポック数で縦軸が性能です。赤色の線（従来モデル）に対して青色の線（Residual Model）ではほんの数エポックで高い性能が学習できているのが分かります。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/ea5f1a54-6d8b-ab8e-fc20-e6f8c6789ee3.png" target="_blank" rel="nofollow noopener"><img width="635" alt="res-learning.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/ea5f1a54-6d8b-ab8e-fc20-e6f8c6789ee3.png"></a></div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1511.04587v2.pdf" rel="nofollow noopener" target="_blank">Accurate Image Super-Resolution Using Very Deep Convolutional Networks</a>
</li>
</ul>

<hr>

<ul>
<li>RNN</li>
</ul>

<p>RNN (Recurrent Neural Network)は前回の出力データ（の一つ前の中間層出力）を保持しておき、次の入力データと共に別の入力値として利用する構造です。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/4770920d-495b-4d6e-2682-15d98a2eb3d7.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/4770920d-495b-4d6e-2682-15d98a2eb3d7.png"></a></div>

<p>文章や音声のような時系列のデータを処理できます。例えば "a" という単語が入ってきた場合にその直前に "I" と "have" が来ていたら "pen" を返す様にネットワークをトレーニングできます。状態メモリーを持つことに近く、DLのテキストや音声理解への道を開きました。</p>

<p>またRNNは必ずしもテキストや音声のような時系列のものが対象ではありません。例えば下の左の画像はDeepMindにより発表されたもので、住所の地番の画像を見せると注目するべき点の流れをRNNで生成します。人間の目のように左から右下へ順に番号を読んでいくように学習しました。右の画像ではRNNを使って数字の画像を生成するよう学習されたものです。</p>

<div align="center">
<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/cdad834d-86cc-3a42-1697-1455c2ae5a4e.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/cdad834d-86cc-3a42-1697-1455c2ae5a4e.gif"></a><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/42103360-e661-0b16-bd16-3fe3c497ecb6.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/42103360-e661-0b16-bd16-3fe3c497ecb6.gif"></a>
</div>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/KojiOhki/items/397f157342e0def06a9b" id="reference-2fb77360ed510c7c4422">リカレントニューラルネットワークの理不尽な効力（翻訳）</a>
</li>
</ul>

<hr>

<ul>
<li>LSTM</li>
</ul>

<p>例えばRNNで文章を処理する場合、ずっと前の段落で言及された内容を整理して覚えておくことは不可能でした。LSTM(Long Short Term Memory) では一度保持した内容をずっと覚えておいて毎回の処理時に入力として与えることができます。さらに忘却ゲートという機構を備え、これを使うことで一度覚えた情報を消去することが可能になります。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/570a8277-a736-7ca7-3af6-1c2a0f5b9157.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/570a8277-a736-7ca7-3af6-1c2a0f5b9157.png"></a></div>

<p>上図は下記のリンクから引用した初代LSTMの図に幾つか説明を加えたものです。input gateでは今回得られた情報から現在の保持情報をどのように変更するかを決定します。forget gateでは現在の保持情報からどの情報を消すかを選択し、output gateでは今回の入力と現在の保持情報からどのような出力を行うかを選択する役割を持っています。</p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca" id="reference-cba9378a7ed0333fea87">LSTMネットワークの概要</a></p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/t_Signull/items/21b82be280b46f467d1b" id="reference-a0280e6c861136658a3a">わかるLSTM ～ 最近の動向と共に</a></p></li>
</ul>

<p>LSTMも内部に覗き窓を持つようになるなどの改良版が考案されており音声認識や機械翻訳で大きな成果を上げています。</p>

<p>実はLSTMが提案されたのは1995年と、結構古くからあるモデルなのでした。<br>
現在でも沢山のRNN/LSTMの派生版が提案されています。例えばBidirectional LSTM, Multi Dimensional LSTM, Hierarchical Subsampling RNN, Convolutional LSTM, PyraMiD LSTM, Grid LSTM などです。時間方向や空間方向に多彩に情報を解析、集約していきます。</p>

<p>例えば3次元の脳のスキャン画像について、PyraMiD LSTMは各方向からローカルな特徴を分析しつつその情報を蓄積したまま集約していって脳内の各領域を推測するというタスクを実現しました。</p>

<div align="center">

<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/9de17ea1-d2af-7a07-a40d-192c878c61b5.png" target="_blank" rel="nofollow noopener"><img alt="pyramid0.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/9de17ea1-d2af-7a07-a40d-192c878c61b5.png"></a>

<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/05494d75-b911-27d3-0060-f676f4a9bdaf.png" target="_blank" rel="nofollow noopener"><img alt="pyramid1.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/05494d75-b911-27d3-0060-f676f4a9bdaf.png"></a>
 
<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/b466b279-a847-e65b-9ff0-7a3c319dffd6.png" target="_blank" rel="nofollow noopener"><img alt="pyramid3.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/b466b279-a847-e65b-9ff0-7a3c319dffd6.png"></a>
</div>

<p>もちろん画像以外でも幅広く活用されていて特にテキストや音声に強く、2015年にはLSTMを使ってIT製品のヘルプデスクの応答を自動で作成するチャットボットが発表され話題となりました。</p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1612.02130v1.pdf" rel="nofollow noopener" target="_blank">Predictive Business Process Monitoring with LSTM Neural Networks<br>
</a> </p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" rel="nofollow noopener" target="_blank">DEEP LEARNING FOR CHATBOTS, PART 1 – INTRODUCTION</a></p></li>
</ul>

<hr>

<ul>
<li>DQN</li>
</ul>

<p>AlphaGoやDeep Dreamを開発したDeepMindの構築したdeep Q-network あるいは Deep Reinforcement Learningと呼ばれるモデルです。<br>
ある状態でエージェントがとれる行動が有限の種類だけあったとします。その場合にどの行動を取れば将来的に最大の報酬が得られるかを経験的に学習していくものです。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/3327b55e-e9c3-34fe-ad68-fd0e01429261.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/3327b55e-e9c3-34fe-ad68-fd0e01429261.jpeg"></a></div>

<p>Q Learningと呼ばれる古典的な強化学習の手法に対して、CNNを用いた画像認識や、後に説明するようなミニバッチや最新のオプティマイザなどのDL技術を組み合わせたモデルです。ロボットの制御やボードゲームのAIなどに利用できます。</p>

<p>最近ではさらにLSTMの手法を追加して自分が過去にとった行動を考慮するDeep Recurrent Q-Learningなども提案されています。DQNの強化版は特にGoogle DeepMindが力を入れている分野のようにみえます。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://ir.hit.edu.cn/%7Ejguo/docs/notes/dqn-atari.pdf" rel="nofollow noopener" target="_blank">Human-level control through deep reinforcement learning</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1507.06527v4.pdf" rel="nofollow noopener" target="_blank">Deep Recurrent Q-Learning for Partially Observable MDPs</a>
</li>
</ul>

<hr>

<ul>
<li>DCGAN</li>
</ul>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/fc16fea8-00be-5174-9069-77200ff78ece.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/fc16fea8-00be-5174-9069-77200ff78ece.png"></a></div>

<p>DCGAN = deep convolutional generative adversarial networks です。二つの目的が正反対のモデルを交互に学習させながらお互いを鍛えていき、画像生成で革命的な成果を出したモデルです。IndicoというマシンラーニングのAPIを提供しているスタートアップとfacebookの研究所が出したものです。</p>

<p>Generatorこと"G" (僕の中では"爺さん")と、Discriminatorこと"D" (僕の中では"ディスラー") の2つのモデルを使って画像生成をするGANという技術にDLのCNN技術を応用したものです。ここでは3種類の学習を行います。</p>

<ol>
<li><p>ディスラー（上図 下) は学習データとして与えられた顔画像を見て正解と答えるように学習します。構造的にはCNNを用いた画像分類器と似ています。</p></li>
<li><p>爺さん（上図 上) は与えられたランダムなパラメータから画像を生成します。この画像をディスラーに見せたときにディスラーが正解と答えるように学習します。</p></li>
<li><p>ディスラーは爺さんの作った画像を見たときは正解とせずに正しくディスるように学習します。</p></li>
</ol>

<p>上記を繰り返していくことで学習していきます。また、ただCNNを使うというだけでなくてバッチの正規化や伝達関数に工夫などをして精度を高めているようでした。</p>

<p>ここでさらに興味深いのはこの爺さんが獲得した生成機では各パラメータがword2vecのような顔の特徴を掴んだものになることです。学習後にパラメータをランダムではなく手動で変更することで色々な特徴の顔をミックスして自由に画像を生成することができます。</p>

<p>顔以外にも風景などに適用でき、また近年では複数のGeneratorとDiscriminatorを使った複雑な<a href="https://arxiv.org/pdf/1612.04357v1.pdf" rel="nofollow noopener" target="_blank">Stacked GAN</a>なども提案されています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/7867a5ab-b034-e917-9d52-8aa9b5dad1eb.png" target="_blank" rel="nofollow noopener"><img width="693" alt="Screen Shot 2017-01-18 at 6.02.55 AM.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/7867a5ab-b034-e917-9d52-8aa9b5dad1eb.png"></a></p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1511.06434.pdf" rel="nofollow noopener" target="_blank">UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS</a></p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://memo.sugyan.com/entry/20160516/1463359395" rel="nofollow noopener" target="_blank">TensorFlowによるDCGANでアイドルの顔画像生成</a></p></li>
</ul>

<hr>

<ul>
<li>SyntaxNet</li>
</ul>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/5d0b2dd5-142b-c2ad-5f10-6ec89e66a3f3.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/5d0b2dd5-142b-c2ad-5f10-6ec89e66a3f3.png"></a></div>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/0f7c147c-9001-d332-0b11-7f52e2c729e0.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/0f7c147c-9001-d332-0b11-7f52e2c729e0.png"></a></div>

<p>SyntaxNetはGoogleの開発した自然言語用の構文解析器です。残念ながら僕はNLPについては全然分からないのでSyntaxNetの仕組みは説明できません。</p>

<p>ただ簡単に説明を読む限りではRNNのように順に単語を入れていくのではなく、ある程度まとめてタグをつけた上でネットワークに入力するようです。それをどうやって学習させているのかは分かりませんが、従来の構文解析器に比べて非常に高い精度を持ち、さらにどんな言語にでも学習可能というなかなか凄いふれ込みです。実際に最近になってgoogle翻訳の精度が大きく向上したと話題になっていましたが、ここら辺の技術が応用されている可能性も高そうです。</p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://github.com/tensorflow/models/tree/master/syntaxnet" rel="nofollow noopener" target="_blank">SyntaxNet - Github</a></p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://www.nowhere.co.jp/blog/archives/20160524-121743.html" rel="nofollow noopener" target="_blank">Google発 世界最高精度の構文解析器 “SyntaxNet” はどこがすごいのか</a></p></li>
</ul>

<hr>

<ul>
<li>アンサンブル学習</li>
</ul>

<p>二つのネットワークを用いて画像生成を行うDCGANもその一種として良いと思います。近年とみに有効な手段として用いられているのがアンサンブル学習です。これは複数のネットワークを作ってそれぞれで個別にトレーニングを行い実行時にはそれらの多数決や協調により判断を行う仕組みです。</p>

<p>例えばAlphaGoでは初手の探索にはディープなネットワークを用いて高精度な予想を行い、手を読むときには返答の速い浅いネットワークを協調させて大きな性能を達成しています。また物体認識のための初期のInception netでは3つの出力を使って多数決で決めていました。一つのモデルを巨大にすると細部までの学習ができなくなるため、別々のケースを各モデルに個別に学習させるアンサンブル学習は分かりやすいです。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://univprof.com/archives/16-06-15-3949163.html" rel="nofollow noopener" target="_blank">アンサンブル学習の２つのメリット・利点(回帰分析・クラス分類)</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://deepmind.com/research/alphago/" rel="nofollow noopener" target="_blank">AlphaGo | Deep Mind</a>
</li>
</ul>

<p>個人的にですが、異なる目的のネットワークが協調動作する様はまさに生命と言って良い面白さがあります。下記は2016年のILSVRCの結果ですが、ほとんどのモデルがアンサンブルモデルとなっていて大きなトレンドをなしている状態です。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/863d16c7-f62b-7b12-f4c2-b2312d9d1cf9.png" target="_blank" rel="nofollow noopener"><img alt="ensemble.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/863d16c7-f62b-7b12-f4c2-b2312d9d1cf9.png"></a></div>

<h2>
<span id="フレームの進化" class="fragment"></span><a href="#%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%81%AE%E9%80%B2%E5%8C%96"><i class="fa fa-link"></i></a>フレームの進化</h2>

<hr>

<ul>
<li>学習の仕方（ウェイトの更新方法）を学習するフレームワーク</li>
</ul>

<p>現在のDLの根幹はバックプロパゲーションによるウェイト／バイアスの学習アルゴリズムです。但し今の方法だとレイヤーが増えた場合に勾配消失が発生して発散してしまいますし、何より利用する関数が微分可能でないといけないという制約があります。</p>

<p>下記の論文はネイチャーにも載って話題になった論文です。今までの手法ではウェイトを更新して行く時に現在のウェイト値を使っていくのですが、代わりにランダムなウェイトを用意して値の更新に利用しそのウェイトを学習させて行く方式です。これによってMNISTのような手書き数字の認識で通常のバックプロパゲーションよりも高い性能が得られたとしています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/5c133bb1-8aac-3752-6e91-dbac5f2a8ca9.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/5c133bb1-8aac-3752-6e91-dbac5f2a8ca9.png" alt="ncomms13276-f1.png"></a></p>

<p>Learning to learn とも言うべき手法で、どのように学習すれば良いかを学習する手法はいろいろ提案されています。上記はFeedback Alignmenと呼ばれる手法ですが、他にもSynthetic Gradientや Target Propという方法が提案されています。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://www.nature.com/articles/ncomms13276" rel="nofollow noopener" target="_blank">Random synaptic feedback weights support error backpropagation for deep learning</a>
</li>
</ul>

<hr>

<ul>
<li>ネットワークを作るネットワーク</li>
</ul>

<p>この論文も非常に面白いです。ハイパーパラメータの自動チューニングとも言うべきものではありますが、RNNを用いてネットワークを自動構築・強化していきます。</p>

<div align="center">
<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/1cae0a62-4390-8724-84d6-030004b409c2.png" target="_blank" rel="nofollow noopener"><img alt="REINFORCEMENT LEARNING1.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/1cae0a62-4390-8724-84d6-030004b409c2.png"></a>

<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/2c62ef25-9c3e-456d-cb53-bda80e5b0e16.png" target="_blank" rel="nofollow noopener"><img alt="REINFORCEMENT LEARNING2.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/2c62ef25-9c3e-456d-cb53-bda80e5b0e16.png"></a>
</div>

<p>CIFAR-10という、ILSVRCよりも簡単めな物体認識のタスクを解くモデルを生成し強化していきます。結果的に人間が設計し優勝した過去のモデルとほぼ同等のモデルを作成できたとのこと。</p>

<div align="center">
<a href="https://qiita-image-store.s3.amazonaws.com/0/145675/77d071e6-2b27-8cb6-2f71-a36b18c01eaa.png" target="_blank" rel="nofollow noopener"><img alt="REINFORCEMENT LEARNING3.png" src="https://qiita-image-store.s3.amazonaws.com/0/145675/77d071e6-2b27-8cb6-2f71-a36b18c01eaa.png"></a>
</div>

<p>上記は左がLSTMのモデルで、右が生成されたネットワークの一部です。単なるパラメータチューニングの話題ともとれますが、これは応用して行けばより複雑なネットワークを自動構築でき、生成できるモデルの表現力はかなり高くなる可能性があります。強化学習を用いた同様の論文は幾つかあり、面白い可能性を秘めていると思います。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1611.01578v1.pdf" rel="nofollow noopener" target="_blank">Neural Architecture Search with Reinforcement Learning</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/abs/1611.05763" rel="nofollow noopener" target="_blank">Learning to reinforcement learn</a>
</li>
</ul>

<hr>

<ul>
<li>Differentiable Neural Computer</li>
</ul>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/bf71d6d2-2f8a-b428-584f-58cffb2dd18c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/bf71d6d2-2f8a-b428-584f-58cffb2dd18c.png"></a></div>

<p>Differentiable Neural Computer もDeepMindから発表されたものです。従来のDLの枠組みにはまらない新しいもので、イメージとしてはチューリングマシンに非常に近いです。大きな特徴として自由なメモリ領域を持ちます。</p>

<p>DNCはデータが入力されるとそれに合わせてヘッドを動かしてメモリからデータを読み込みます。さらにリンク情報に従って順次別のメモリも読んで行きます。読み込みが終わったら出力を出すと同時に空いているメモリを探してそこに情報を書き込んだりメモリの消去を行います。メモリから出力された情報は次に来た入力データと合わせてRNNのように処理されます。</p>

<p>論文ではパズルを解いたり地下鉄の路線図や家系図を解析するなどいろいろなタスクが可能になったとされており今後の大きな可能性を感じます。</p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/eaff4cb9-c7db-b85b-3eb5-f32d04ae5ab3.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/eaff4cb9-c7db-b85b-3eb5-f32d04ae5ab3.png"></a></div>

<p><a href="https://deepmind.com/blog/differentiable-neural-computers/" rel="nofollow noopener" target="_blank">Differentiable Neural Computer</a></p>

<h2>
<span id="the-deeper-the-better" class="fragment"></span><a href="#the-deeper-the-better"><i class="fa fa-link"></i></a>The Deeper, The Better</h2>

<p>ここまで見てきたモデルはどれも話題になった有名なものですが、これ以外にも非常に沢山のモデルが考案されています。現在のトレンドとしては2つあり、パラメータを少なくして学習しやすくなる代わりに層を深くしていく方向と、複数のモデルを使っていく方向です。例えば下記の図は各年のILSVRCの優勝モデルの深さの変化です。但しどちらかというと単体のモデル自体は複雑にするのではなくシンプルにしつつ深くしていって、アンサンブルで組み合わせるというのが多いように思います。<em>(そうは言ってもまだまだ湯水のごとく複雑なモデルが考案されていますが)</em></p>

<div align="center"><a href="https://qiita-image-store.s3.amazonaws.com/0/145675/69db4b3d-f5fa-fb28-f956-8c0573dcbf47.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/145675/69db4b3d-f5fa-fb28-f956-8c0573dcbf47.png"></a></div>

<h1>
<span id="性能改善手法" class="fragment"></span><a href="#%E6%80%A7%E8%83%BD%E6%94%B9%E5%96%84%E6%89%8B%E6%B3%95"><i class="fa fa-link"></i></a>性能改善手法</h1>

<p>ここもちょっと長くなったので別のページとしてまとめました。</p>

<p><a href="http://qiita.com/jiny2001/items/1d3f2d0370b2689d2da7" id="reference-d46fb84ddb4bf08b1e70">Inside of Deep Learning （ディープラーニングの性能改善手法 一覧）</a> </p>

<h1>
<span id="dlは何を変えるのか" class="fragment"></span><a href="#dl%E3%81%AF%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%82%8B%E3%81%AE%E3%81%8B"><i class="fa fa-link"></i></a>DLは何を変えるのか?</h1>

<p>DLの進化は非常に早く、今回紹介したモデルや手法はほとんどが既に過去のものです。それぞれ多くの派生モデルが提案されています。</p>

<p>下記のページがよくまとまっていますが、2016年も恐ろしい数の論文が提出されその話題の幅も非常に幅広いものでした。是非ちらっとでも確認されてみると良いと思います。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" id="reference-9549ce0a2d1d429599ba">2016年のディープラーニング論文100選</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" id="reference-e5346047e7598fb2bace">DeepLearning研究 2016年のまとめ</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7" id="reference-ebecfa045dad9568f2d4">foobarNet: ディープラーニング関連の○○Netまとめ</a>
</li>
</ul>

<h2>
<span id="dlは本当に賢いのか-弱点は" class="fragment"></span><a href="#dl%E3%81%AF%E6%9C%AC%E5%BD%93%E3%81%AB%E8%B3%A2%E3%81%84%E3%81%AE%E3%81%8B-%E5%BC%B1%E7%82%B9%E3%81%AF"><i class="fa fa-link"></i></a>DLは本当に"賢い"のか？ 弱点は？</h2>

<p>大手メディアの記事を読んでいると非常に賢いと思えてしまうDLを利用したAIですが、実際はちょっとイメージが違うと思います。</p>

<p>例えばDQNを用いてブロック崩しを解くAIが発表された時、幾つかのメディアは”ルールを勝手に推測して”プレイすると記事にしていました。ですが実際はルールなどは理解せずにただその状況ではどの行動をとれば良かったかを経験から学習して選んでいるだけです。（もちろんCNNを用いてゲーム中の画像から現在の状態把握ができるようになったという点は大きな進歩ではありますが）</p>

<p>あるいはAlphaGoの時も"棋士が次に打つ手を57%の精度で予測"などと書かれていました。この表現だと<strong>まるで相手の打つ手を高確率で予測できるように聞こえます</strong>が、実際はそうではありません。次にコンピュータが選んだ手が、過去のプロの対戦の中で使われたようなまともな手である確率が57%です。43%の確率で変な手を選んでしまうのです。</p>

<p>とにかく言いたかったことは、メディアの記事は鵜呑みにせず興味がある場合はできるだけ引用元を読みましょうということと、今のAIはあくまで限定的な状況での最適化がメイントピックであるということです。</p>

<p><strong>そして弱点というとまず思い浮かぶのは、どうしても学習用データが必要になることです。</strong>教師データが作れない分野はまだまだ多く、また教師なし学習でできることは非常に限られているように思えます。</p>

<h2>
<span id="医療革命生産革命エネルギー問題そして" class="fragment"></span><a href="#%E5%8C%BB%E7%99%82%E9%9D%A9%E5%91%BD%E7%94%9F%E7%94%A3%E9%9D%A9%E5%91%BD%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC%E5%95%8F%E9%A1%8C%E3%81%9D%E3%81%97%E3%81%A6"><i class="fa fa-link"></i></a>医療革命、生産革命、エネルギー問題、そして...?</h2>

<p>知能の実現はまだ難しそうですが、逆に言えば非常に単純なタスクか、あるいはスペシャリストが対応するような限定された問題の両極端なところから実用化が始まっていきそうです。</p>

<p>現在でも既に各種の画像処理、翻訳や文章解析、音声認識などで一気に技術シーンを塗り替えていますが、これからは各種医療技術に応用されガンの予測や遺伝子解析、病気の診断などが一気に進んで行くでしょう。</p>

<p>PFNやnVidiaさんが勧めているIoT機器の知能化も非常に面白いです。モバイルやIoT機器にDL処理用のチップを使って知能化し、ネットに繋がなくても手元で高度なAIが動いたり、あるいはデバイスが得たデータをクラウドに送ってさらなる学習を中央で行いデバイスにアップデートして行くことが可能になります。工場や各種の生産技術はさらなる知能化や無人化が進み生産能力が大きく上がるのは間違いなさそうです。</p>

<ul>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://captricity.com" rel="nofollow noopener" target="_blank">手紙やフォームに入力された文章を従来の10倍の速度で認識する Captricity</a>
</li>
<li class="task-list-item">
<input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://www.ut-ec.co.jp/portfolio/mujin" rel="nofollow noopener" target="_blank">産業用ロボットの知能化を行う東大発ベンチャー MUJIN</a>
</li>
</ul>

<p>DLは計算量も多く、大量に電気を消費します。最後に残された問題はエネルギー問題でしょうか。ただしそれもAIがエネルギー消費の効率化を進めています。おそらく電力が足りなくなるのは過渡期だけで、最終的には全てが効率化されエネルギーがむしろ余りそうな気もします。需要予測や最適化はAIの得意分野ですし、余ったエネルギーは無駄なく効率よくAIが使ってくれることでしょう。</p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/" rel="nofollow noopener" target="_blank">DeepMind AI Reduces Google Data Centre Cooling Bill by 40%</a></p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="http://www.mdpi.com/1996-1073/9/9/721/htm" rel="nofollow noopener" target="_blank">Short-Term Price Forecasting Models Based on Artificial Neural Networks for Intraday Sessions in the Iberian Electricity Market</a></p></li>
</ul>

<p>それで、そんな風に世の中の生産効率が上がり続けて行くとどうなるか？<br>
これはDLが話題になり始めた数年前から考えていたことなのですが、人の時間の使い方としてはより原始的な方向に戻るのではないかと思ってます。</p>

<p>単に自分だけや家族、友人、会社のためだけに時間や労力を割くのではなく、見ず知らずの人にさえ時間を使って貢献するような社会です。AIが世の中を管理すると言ってもそれは恐らく一部の指標に対して社会の中の変更して良い部分を最適化するという話であって、そんなに悪い社会になるとは僕には思えません。</p>

<p>アメリカの人々の生活を見ていると、進行する格差の拡大が大きな問題に思えます。この拡大を社会が真剣に止めようとできるかどうかが分かれ目で、結局は良い方向にAIを使う道へ進むと信じてます。</p>

<h2>
<span id="ソフトエンジニアの在り方は変わるか" class="fragment"></span><a href="#%E3%82%BD%E3%83%95%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E5%9C%A8%E3%82%8A%E6%96%B9%E3%81%AF%E5%A4%89%E3%82%8F%E3%82%8B%E3%81%8B"><i class="fa fa-link"></i></a>ソフトエンジニアの在り方は変わるか？</h2>

<p>「深層学習は何を変えるのか」なんていう立派なお題をつけてしまったわりには、実を言うと世の中がどう変わっていくかについてはそんなに良く分かりません。実はエンジニアの身の回りの変化についての考察がメインでした。</p>

<p>一番気になるのは、<strong>「自分の子どもたちにエンジニアになることを勧めるべきか否か」</strong>です。</p>

<p>ぱっと見ではAIを作る側にいるエンジニアは安泰なようにも思えますが...</p>

<p>これは究極的に言えば、"AIを作るAI"を作ることは現実的なのかという問題に近いと思います。</p>

<p>例えば現時点で既にLSTMでプログラムコードを読んで実行する”Learning to Execute”という論文があります。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>
Input:
  j=8584
  for x in range(8):
    j+=920
  b=(1500+j)
  print((b+7567))
</pre></div></div>

<p>上記のようなテキスト行を入力した場合に、<code>Target: 25011.</code> という答えを予測して出力できるのです（ちなみに上記の出力結果は25011です）。これはソースコードとその出力を学習させてやるだけでそれぞれの演算子やfor文の意味をシミュレートできるようになったという面白い論文です。（実際はそこまで簡単には実現できなかったっぽいですが）</p>

<p>あるいは「オプティマイザのオプティマイザ」とも言うべきもので、やはりLSTMを用いてオプティマイザ自体を学習してしまう論文も発表されています。上記で説明したようなモデルを自動生成するLearning to Learnと言われる、学習方法自体を学習するアイデア・実験も沢山あります。</p>

<p>つまりは既にその兆候は現れています。</p>

<ul>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1410.4615.pdf" rel="nofollow noopener" target="_blank">Learning to Execute</a></p></li>
<li class="task-list-item"><p><input type="checkbox" class="task-list-item-checkbox" disabled><a href="https://arxiv.org/pdf/1606.04474v2.pdf" rel="nofollow noopener" target="_blank">Learning to learn by gradient descent by gradient descent</a></p></li>
</ul>

<p>今まで見てきた進化の速度から考えるに、恐らく子供達が今の僕の年になる30年後には、今のようなソフトウェア職への需要は大きく下がっているのではと感じています。ベイエリアで年収2000万円もらってひーはーやってるエンジニアの職は多分なくなりません。問題なのはIT土方なんて言われるような大部分の中産階級のエンジニアです。（僕のようなニートは真っ先に消されます :) ）</p>

<p>少なくともソースコードを書く部分はかなりの領域をAIが担当すると思います。人間や社会のことを理解し、ソフトの仕様を定義する"人間"は一定数必要になるのではと思いますが、仕様を箇条書きで書くだけなので難しい要素は全くありません。</p>

<p>仕様をいかにコンピュータが分かる形の知識表現に落とすか、という問題はそんなに難しくない気がします。人間の書いた仕様テキストをそのまま入力として理解できるようになるのもそう遠くないのではないでしょうか。仕様ができたら後はAIがコードを生成し、もし矛盾があれば自動で直してくれるか、優先順位やさらなる仕様を要求してもらいます。もちろんAIはこちらの性格を把握済みですから、こちらが気を悪くせずに仕事ができるよう最大限の配慮をしてくれます。もしかしたら今の上司や同僚よりも気持ちよく仕事ができたりするかもしれません。</p>

<p>AIのチューニングはほとんどAIがやります。人間も一部手伝うとは思いますが、今のようなプログラムコードを書くやりかたではなく、何かビジュアルなツールを用いてモデルの構造や組み合わせを変えたりするような感じになるのではと思いました。</p>

<p>結論としては、子供にプログラミングを教えるのはちょっと躊躇っています。</p>

<p>僕自身は高校時代からプログラミングを始めて凄いハマったのでちょっと寂しい気はします。もちろん論理的思考やクリエイティビティを刺激するようなものは積極的に与えたいです。かつての僕がしたように、CPUの動作やアドレスバスの動作を理解させてコンピュータの動作原理を理解させていくのも良いと思います。ただし今や車のことを分からなくても運転できるのと同じことが起こるだろうという感覚はあります。</p>

<p>プログラミングの形は大きく変わり、開発に必要な人数も大きく減るだろうなとは感じます。コンピュータの仕組みやサイエンスを教えずにプログラミングだけを教えるのはやはり30年後の子供達にとってリスクになるのではないでしょうか。</p>

<h2>
<span id="最後に" class="fragment"></span><a href="#%E6%9C%80%E5%BE%8C%E3%81%AB"><i class="fa fa-link"></i></a>最後に</h2>

<p>この道を行けば、どうなるものか。</p>

<p>危ぶむなかれ。危ぶめば道はなし。</p>

<p>迷わず行けよ、行けばわかるさ。</p>

<p>みんなでやろう、ＤL！</p>
<div class="hidden"><form class="js-task-list-update" action="/jiny2001/items/85af7dd163a63b3a152a" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="ToNkTk1JGNPJ/34l0cjrkZhSKfzOrtHaZZaFofDyP9FQC92gGINXsbbIdGVyaScP5sUIn2wQP0X18cFycOA+7A==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1485510042" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
シリコンバレーのエンジニアが一年ほどをDL(Deep Learning)を追いかけてみて思ったこと、感じたこと、伝えたいことをまとめてみました。とにかく伝えたいことは、DLはもはやその一言では片付けられないほどに構造やアプローチが多様化しているということ。そしてその進化スピードがえげつないほど速いということです。

将来のプログラミングや問題解決の仕方を変え、人を取り巻く環境を変えていくかもしれないというじりじりとした圧迫感。これを少しでも伝えられればと思っています。

# このポストの方針
* 技術部分の説明は初心者向け。各構成要素など基礎から解説します。今からDLをキャッチアップしていく人には多分丁度良いです。
 
* 最初と最後だけ読むのも良いですが、各部の技術的な部分や難しさはできるだけ短く分かりやすく書くつもりですしここが一番大事なところです。できれば時間のあるときにじっくり読んでもらえればと思います。


# 内容
__DLは何を変えたか：__あるいはDLとは何か？
__基本要素：__DLを構成する要素とその仕組み。
__DLのデメリット：__DLシステムを構築するのがいかに難しいか。
__各種改善手法・最新事例：__各種のモデルや手法の紹介。ここがメインです。
__DLは何を変えるのか?：__エンジニアを取り巻く環境はどう変わるか？

# DLは何を変えたか
DLは各分野で飛躍的な性能を発揮していますが、何故そのようなジャンプが可能になったのでしょうか。ここでは僕の感じるポイントをまとめてみます。

## 1. ビッグデータ対応
既存の一般的な機械学習のモデルに対して飛躍的に大きなデータを入力として扱えるようになりました。
例えば碁のAIであるAlphaGoでは従来とは比べものにならない量の対戦データを初期フェーズで学習しています。今までの手法ではメモリやコンピューティングパワーの限界にあったデータしか扱えませんでした。

DLは超大量のデータでも学習できる工夫があります。さらに、データの量が多ければ多いほど性能を上げることができるという特徴があります。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/d8105ec3-1c63-874f-eb45-00702f315246.png&quot;&gt;&lt;/div&gt;


- [ ] [Extract Data Conference from SlideShare](http://www.slideshare.net/ExtractConf/andrew-ng-chief-scientist-at-baidu/30)

## 2. 複雑／多層モデルの実現
今までの機械学習アルゴリズムでは例えば人間がパラメータをチューニングしていたとします。これに対して破壊的に複雑なモデルが可能になりました。具体的なモデルは後で説明しますが、システム全体で数千万以上のパラメータや10以上のレイヤーを持っていても学習できるようになりました。

例えば初期の物体認識ネットであるAlexNet ではパラメータが約 6000 万個もあります。表現力の高い複雑なモデルでも学習してしまうことができるのです。（下図はAlexNetのモデル図）

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/d6cca561-f00d-ca92-8925-74f557ef571b.png&quot;&gt;&lt;/div&gt;


## 3. 汎化性能（非線形システムへの信頼性）の向上
DLは非線形システムです。これは従来扱いが難しいものでした。さらに過学習という問題があって、例えば学習時に使ったのとは違うシチュエーションの入力データが入った時にとんでもない答えを出す可能性が出てしまうのです。これだと実用のシステムとしては信頼しにくいのです。

DLでは非線形なシステムでも過学習を抑え信頼性を上げることができました。汎化性能は大きく向上しています。


## 4. 時系列データや多次元データへの拡張
音声や文章、人の行動など、世の中のデータは時系列、可変長、多次元の複雑なデータです。DLではRNNやLSTMなどの構造を持つことで音声認識や自然言語処理などで精度の良い学習を可能にしました。これは従来プログラミング技術的にもいろいろと難しい分野でした。

## DLとは何か？何を変えたか？
大枠的には上記で説明したように「非線形で、複雑なモデルあるいは大量のパラメータを持ち、大規模なデータや時系列のデータなどにも対応できるなどの工夫を備えたニューラルネットベースの機械学習システム」という認識で問題ないでしょう。

ただ、当初使われていたDLという言葉が指すモデルとは大きく離れたモデルがどんどん提案されてきています。とにかくここがエキサイティングなところで、後にじっくりと見ていきたいと思います。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/bbc3d40a-0b8c-7cf7-8acf-346a81e6cd50.png&quot;&gt;&lt;/div&gt;


#### 何を変えたか？ ####

上記の特徴により画像処理や音声認識、自然言語処理の分野で大きな性能の飛躍を可能にしました。これはいくつかの分野で単に性能を押し上げたということだけでは留まらなかったのです。

上記分野は写真や音声、文章とどれも人間が外界とのインタフェースとして使っているものです。つまりは人間と直接交わる部分であって、人間の生活に関わるものをAIで豊かにしていく可能性が開いたわけです。

そしてこのロマンと可能性が世界中の優秀な科学者・技術者を大量にDLの道に引き込む導線になりました。この変化がある意味一番大きな変化のようにも思えます。シリコンバレーではほぼ毎日のようにどこかでＤLや機械学習のミートアップが開催されてます。一過性のブームでは終わらないエンジニアを惹きつけるものがあるのだと感じます。


# 基本要素
全体が長くなってしまったのでこのパートは分割してもう一つのポストにしました。DLはブラックボックスで性能改善が難しいと良く言われていますが、いろいろな方法論が提唱されています。各要素の持つ特徴や役割を理解しておけばDLモデルの改善はそれほど難解なものではありません。

[Inside of Deep Learning （ディープラーニングの基本要素）](http://qiita.com/jiny2001/items/e3961a009690af0c435c) 

# DLのデメリット

## 過学習しやすい
機械学習システムの本質的な問題は
誤：「学習データに対してシステムを最適化する」ことではなく
正：「__学習データに含まれない未知の入力データ__に対してシステムを最適化する」ことです。

過学習と言ってトレーニング用のデータに対して解答を最適化しすぎると逆に実際の性能が落ちてしまうという問題があります。下の図は購入するときの家の大きさと価格のサンプルです。y = ax&lt;sup&gt;2&lt;/sup&gt; + bx + c の2次式（中央）で推定すれば全体的にフィットしますが、より表現力の高い6次式（右）で推定しようとすると実際とは大きく離れた推定モデルになってしまいます。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/dc21319e-6fb5-aa76-34b4-b9e18891553d.png&quot;&gt;&lt;/div&gt;

from [scikit-learn:Machine Learning 101](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/general_concepts.html) 

特に非線形システムは線形システムに比べて過学習しやすく、予測範囲外の出力が出る可能性が高くなります。例えば下の図で、線形のシステム（左）ではp1とp2の間の入力に対する出力値はp1とp2の間になりますが、非線形のシステム（右）では予期しない値が出ることがあります。これは自動運転など安全が優先されるシステムでは特に大きな問題となります。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/15fa60b7-d504-54ad-3db8-04dc911424eb.jpeg&quot;&gt;&lt;/div&gt;

## 大量の学習用データが必要
教師なし学習や強化学習などもありますが、やはり教師データを与えるシステムが多いです。特にディープラーニングでは汎化性能をあげるためにも従来に比べてかなり多くのデータが必要になります。規模や対象にもよりますが例えば画像認識では最低でも数万件以上の学習データが必要と言われています。
また、例えば年齢や職業などから既婚状態、車の所持、年収（5段階）を予測する場合、出力クラスの全ての組み合わせである2x2x5=20のそれぞれの場合で十分な質と数のデータが必要になります。

正解データの作成に人手が必要な場合などでは非常にコストがかかってしまいます。__このためにニューラルネットベースのシステムに移行できないという話は非常によく聞きます。__またDLのブームの火付け役ともなった物体認識のタスクですが、これは当時に非常に大規模で質の良いデータセットが提供されていたこともあってDLと相性の良いタスクだったのでした。


## 膨大なコンピューティングリソースが必要
さらにDLではモデルがディープで複雑なものになることが多く、学習用データが多いことも相まって計算に非常に時間がかかります。GPUも必須になりますし、クラウドを使って気軽に分散化できるものではありません。

特に学習フェーズでは実験に数日から一週間ほどかかるような論文も少なくないです。この場合はモデルの改善に何度も試行錯誤しながら試すのが難しくなります。例えば本番環境でソフトを走らせるまでには通常何回テストランを走らせるでしょうか。一回の実行に一日かかると考えればその難しさが伝わるかと思います。

## パラメータの収束が難しい
多数のレイヤーを持つ場合など、もともと数百万以上ある多数のパラメータをそれぞれ最適値へ収束させていくのは非常に難しくなります。バックプロパゲーションをかけて出力に近い層から学習させていった場合、正解との誤差がどんどん分散してしまって学習できなくなってしまうからです。またモデルが複雑になるほど鞍点と呼ばれる数次元で損失が極値になる点へ陥り、学習が進みにくくなるポイントも増えるからです。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png&quot;&gt;&lt;/div&gt;

- [ ] [Going Deeper with Convolutions](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png&quot;&gt;&lt;/div&gt;

- [ ] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf)

上はInception-Net v1(GoogLeNet)のモデル図でレイヤーは合計で22層、計算要素は100程度あります。下のResNet-152は極端な例ではありますがレイヤー数が152層にもなります。Inception-Netの2年前に物体認識でDLが注目されるきっかけを作ったAlexNetは8層構造でした。まさに&quot;The Deeper, the Better&quot;と言われる所以です。


## 性能改善が難しい
DLのモデルはブラックボックスであるなどと良く表現されます。例えば下図の物体認識の場合、既存手法では特徴抽出や学習判別機などそれぞれの機能で問題があった場合はそれを検出して各ステップごとに改善を試みることができます。
ところがDLの場合はこれらの処理を全部まとめて学習してしまいます。問題があった場合にどのステップの能力が低いのか判断しにくく、また改善するための方法も多岐に渡るためどれを選択して良いのか分かりません。また問題を把握するために数学的な技術が必要になることも多いです。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/91331a26-5aaf-c090-c4d5-1f0544cb97f1.png&quot;&gt;&lt;/div&gt;


同様に__既存手法の場合は人間が持っている知識を組み込むことができますがDLでは非常に難しくなることが多くなってしまいます。__総じて言えばとにかくコストが高いということです。高い技術レベルのエンジニア、学習データ、コンピュータリソース、時間、ほぼ全ての領域において既存技術の数倍〜数十倍のコストがかかり、ハイリスク／ハイリターンのシステムという感覚がぴったりです。

_またこれは日本人にしか当てはまらないことですが、最新論文が全て英語であり特にアメリカ、あるいは欧米諸国や中国でしか最新事例が生まれていないこともあると思います。とにかくDLシーンの進化のスピードが早いので、言葉と文化の壁や産業構造の違いが悪い方向へ出てしまっているように感じます。皆でDLやろう！_



# 各種改善手法・最新事例
いろいろなモデルや改善手法をみて見ましょう。ここからがやっと本ポストで書きたかった部分です。改善手法として大まかに4つの項目に分けてみました。

↑ Best
* より表現力があり発散しにくいモデル、フレームワークを考案する
* より多くのデータあるいはより精度の良いデータを使う
* パラメータを収束させるための工夫
* 汎化性能をあげるための工夫
↓ Better

## モデルの進化

* Inception Net (GoogLeNet)

Googleが開発し大規模画像認識のコンテストILSVRC 2014で優勝した物体認識のためのモデルです。22層のレイヤーを持ち複数のCNNを並列に並べているのが特徴で、CNNのサイズやパラメータ数を抑える代わりに非常にディープにした構造です。3つの判定結果を使って総合的に判断し非常に高い性能を持っています。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/045270b4-303d-6106-abf6-2938e1c17679.png&quot;&gt;&lt;/div&gt;

現在では順調に進化を繰り返し、Inception-Net v4やInception-ResNet v2が公開されています。Inception-Net v3はgithub上でソースコードが公開され、AndroidやiPhone上でビルドして世界最高峰の物体認識機能を利用することも可能です。

- [ ] [Inception in TensorFlow](https://github.com/tensorflow/models/tree/master/inception)
- [ ] [お手軽にrealtime画像認識をAndroid/iOSで動かす](http://qiita.com/mainyaa/items/4b4e4d47868133410a96)

***
* ResNet

ResNetはDeep Residual Learningという論文でMicrosoft Research Asiaにより発表されたもので、Residual(残差あるいは差分)をだけを学習するようにして非常に深いモデルでも効率良く学習できるようにしたモデルです。
2014年のILSVRCではGoogleがInception Netで飛躍的な成果を出して優勝したところをその翌年にはMicrosoftがさらに飛躍的な成果を出してひっくり返すと言う動きの速さがDLシーンの見どころです。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/c6b87277-fc83-e050-d203-f3a68dc34a95.png&quot;&gt;&lt;/div&gt;

上図 左の画像がその一部を図にしたもので、上図 中央がそれを簡略化したものです。入力データを保持しておいてCNNの結果に加えることで各学習器は入力データとの変化分のみを学習すればよくなります。また上図 右の画像は比較のためのモジュールです。3x3x64のCNNフィルタを2枚重ねるよりも、1x1x64, 3x3x64, 1x1x256の3枚のフィルタを使うことで同等の表現力をもちながらもパラメータの数を抑えています。

直感的な説明にすると次に見るRNNとLSTMの関係に似ています。例えば今までのモデルでは入力されたデータを後ろのレイヤーに伝えるためのウェイトも特徴抽出の為のウェイトと同時に学習せねばなりませんでした。そうするともともとのウェイトが高くなり層が深くなると発散しやすくなってしまうということのようです。

&lt;div align=&quot;center&quot;&gt;&lt;img width=&quot;632&quot; alt=&quot;ResNet152.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/7abfcd72-d504-bc6f-2008-342a2d9817d2.png&quot;&gt;&lt;/div&gt;

- [ ] [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385v1.pdf)


Residual Ｎetも他で応用されています。例えば下のリンクにある単眼超解像という画像の解像度を上げる論文では、これを利用することで飛躍的に深く、かつ学習係数を従来の1万倍にして短時間でも学習できてしまうというモデルが提案されています。下の図では横軸が学習エポック数で縦軸が性能です。赤色の線（従来モデル）に対して青色の線（Residual Model）ではほんの数エポックで高い性能が学習できているのが分かります。

&lt;div align=&quot;center&quot;&gt;&lt;img width=&quot;635&quot; alt=&quot;res-learning.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/ea5f1a54-6d8b-ab8e-fc20-e6f8c6789ee3.png&quot;&gt;&lt;/div&gt;


- [ ] [Accurate Image Super-Resolution Using Very Deep Convolutional Networks](https://arxiv.org/pdf/1511.04587v2.pdf)

***
* RNN

RNN (Recurrent Neural Network)は前回の出力データ（の一つ前の中間層出力）を保持しておき、次の入力データと共に別の入力値として利用する構造です。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/4770920d-495b-4d6e-2682-15d98a2eb3d7.png&quot;&gt;&lt;/div&gt;

文章や音声のような時系列のデータを処理できます。例えば &quot;a&quot; という単語が入ってきた場合にその直前に &quot;I&quot; と &quot;have&quot; が来ていたら &quot;pen&quot; を返す様にネットワークをトレーニングできます。状態メモリーを持つことに近く、DLのテキストや音声理解への道を開きました。

またRNNは必ずしもテキストや音声のような時系列のものが対象ではありません。例えば下の左の画像はDeepMindにより発表されたもので、住所の地番の画像を見せると注目するべき点の流れをRNNで生成します。人間の目のように左から右下へ順に番号を読んでいくように学習しました。右の画像ではRNNを使って数字の画像を生成するよう学習されたものです。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/cdad834d-86cc-3a42-1697-1455c2ae5a4e.gif&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/42103360-e661-0b16-bd16-3fe3c497ecb6.gif&quot;&gt;&lt;/div&gt;

- [ ] [リカレントニューラルネットワークの理不尽な効力（翻訳）](http://qiita.com/KojiOhki/items/397f157342e0def06a9b)


***
* LSTM

例えばRNNで文章を処理する場合、ずっと前の段落で言及された内容を整理して覚えておくことは不可能でした。LSTM(Long Short Term Memory) では一度保持した内容をずっと覚えておいて毎回の処理時に入力として与えることができます。さらに忘却ゲートという機構を備え、これを使うことで一度覚えた情報を消去することが可能になります。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/570a8277-a736-7ca7-3af6-1c2a0f5b9157.png&quot;&gt;&lt;/div&gt;

上図は下記のリンクから引用した初代LSTMの図に幾つか説明を加えたものです。input gateでは今回得られた情報から現在の保持情報をどのように変更するかを決定します。forget gateでは現在の保持情報からどの情報を消すかを選択し、output gateでは今回の入力と現在の保持情報からどのような出力を行うかを選択する役割を持っています。

- [ ] [LSTMネットワークの概要](http://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca)

- [ ] [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)

LSTMも内部に覗き窓を持つようになるなどの改良版が考案されており音声認識や機械翻訳で大きな成果を上げています。

実はLSTMが提案されたのは1995年と、結構古くからあるモデルなのでした。
現在でも沢山のRNN/LSTMの派生版が提案されています。例えばBidirectional LSTM, Multi Dimensional LSTM, Hierarchical Subsampling RNN, Convolutional LSTM, PyraMiD LSTM, Grid LSTM などです。時間方向や空間方向に多彩に情報を解析、集約していきます。

例えば3次元の脳のスキャン画像について、PyraMiD LSTMは各方向からローカルな特徴を分析しつつその情報を蓄積したまま集約していって脳内の各領域を推測するというタスクを実現しました。

&lt;div align=&quot;center&quot;&gt;

&lt;img alt=&quot;pyramid0.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/9de17ea1-d2af-7a07-a40d-192c878c61b5.png&quot;&gt;

&lt;img alt=&quot;pyramid1.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/05494d75-b911-27d3-0060-f676f4a9bdaf.png&quot;&gt;
 
&lt;img alt=&quot;pyramid3.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/b466b279-a847-e65b-9ff0-7a3c319dffd6.png&quot;&gt;
&lt;/div&gt;

もちろん画像以外でも幅広く活用されていて特にテキストや音声に強く、2015年にはLSTMを使ってIT製品のヘルプデスクの応答を自動で作成するチャットボットが発表され話題となりました。

- [ ] [Predictive Business Process Monitoring with LSTM Neural Networks
](https://arxiv.org/pdf/1612.02130v1.pdf) 

- [ ] [DEEP LEARNING FOR CHATBOTS, PART 1 – INTRODUCTION](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)


***
* DQN

AlphaGoやDeep Dreamを開発したDeepMindの構築したdeep Q-network あるいは Deep Reinforcement Learningと呼ばれるモデルです。
ある状態でエージェントがとれる行動が有限の種類だけあったとします。その場合にどの行動を取れば将来的に最大の報酬が得られるかを経験的に学習していくものです。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/3327b55e-e9c3-34fe-ad68-fd0e01429261.jpeg&quot;&gt;&lt;/div&gt;

Q Learningと呼ばれる古典的な強化学習の手法に対して、CNNを用いた画像認識や、後に説明するようなミニバッチや最新のオプティマイザなどのDL技術を組み合わせたモデルです。ロボットの制御やボードゲームのAIなどに利用できます。

最近ではさらにLSTMの手法を追加して自分が過去にとった行動を考慮するDeep Recurrent Q-Learningなども提案されています。DQNの強化版は特にGoogle DeepMindが力を入れている分野のようにみえます。

- [ ] [Human-level control through deep reinforcement learning](http://ir.hit.edu.cn/~jguo/docs/notes/dqn-atari.pdf)
- [ ] [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/pdf/1507.06527v4.pdf)


***
* DCGAN

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/fc16fea8-00be-5174-9069-77200ff78ece.png&quot;&gt;&lt;/div&gt;


DCGAN = deep convolutional generative adversarial networks です。二つの目的が正反対のモデルを交互に学習させながらお互いを鍛えていき、画像生成で革命的な成果を出したモデルです。IndicoというマシンラーニングのAPIを提供しているスタートアップとfacebookの研究所が出したものです。

Generatorこと&quot;G&quot; (僕の中では&quot;爺さん&quot;)と、Discriminatorこと&quot;D&quot; (僕の中では&quot;ディスラー&quot;) の2つのモデルを使って画像生成をするGANという技術にDLのCNN技術を応用したものです。ここでは3種類の学習を行います。

1. ディスラー（上図 下) は学習データとして与えられた顔画像を見て正解と答えるように学習します。構造的にはCNNを用いた画像分類器と似ています。

2. 爺さん（上図 上) は与えられたランダムなパラメータから画像を生成します。この画像をディスラーに見せたときにディスラーが正解と答えるように学習します。

3. ディスラーは爺さんの作った画像を見たときは正解とせずに正しくディスるように学習します。

上記を繰り返していくことで学習していきます。また、ただCNNを使うというだけでなくてバッチの正規化や伝達関数に工夫などをして精度を高めているようでした。

ここでさらに興味深いのはこの爺さんが獲得した生成機では各パラメータがword2vecのような顔の特徴を掴んだものになることです。学習後にパラメータをランダムではなく手動で変更することで色々な特徴の顔をミックスして自由に画像を生成することができます。

顔以外にも風景などに適用でき、また近年では複数のGeneratorとDiscriminatorを使った複雑な[Stacked GAN](https://arxiv.org/pdf/1612.04357v1.pdf)なども提案されています。

&lt;img width=&quot;693&quot; alt=&quot;Screen Shot 2017-01-18 at 6.02.55 AM.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/7867a5ab-b034-e917-9d52-8aa9b5dad1eb.png&quot;&gt;


- [ ] [UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS](https://arxiv.org/pdf/1511.06434.pdf)

- [ ] [TensorFlowによるDCGANでアイドルの顔画像生成](http://memo.sugyan.com/entry/20160516/1463359395)

***
* SyntaxNet

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/5d0b2dd5-142b-c2ad-5f10-6ec89e66a3f3.png&quot;&gt;&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/0f7c147c-9001-d332-0b11-7f52e2c729e0.png&quot;&gt;&lt;/div&gt;

SyntaxNetはGoogleの開発した自然言語用の構文解析器です。残念ながら僕はNLPについては全然分からないのでSyntaxNetの仕組みは説明できません。

ただ簡単に説明を読む限りではRNNのように順に単語を入れていくのではなく、ある程度まとめてタグをつけた上でネットワークに入力するようです。それをどうやって学習させているのかは分かりませんが、従来の構文解析器に比べて非常に高い精度を持ち、さらにどんな言語にでも学習可能というなかなか凄いふれ込みです。実際に最近になってgoogle翻訳の精度が大きく向上したと話題になっていましたが、ここら辺の技術が応用されている可能性も高そうです。


- [ ] [SyntaxNet - Github](https://github.com/tensorflow/models/tree/master/syntaxnet)

- [ ] [Google発 世界最高精度の構文解析器 “SyntaxNet” はどこがすごいのか](http://www.nowhere.co.jp/blog/archives/20160524-121743.html)

***
* アンサンブル学習

二つのネットワークを用いて画像生成を行うDCGANもその一種として良いと思います。近年とみに有効な手段として用いられているのがアンサンブル学習です。これは複数のネットワークを作ってそれぞれで個別にトレーニングを行い実行時にはそれらの多数決や協調により判断を行う仕組みです。

例えばAlphaGoでは初手の探索にはディープなネットワークを用いて高精度な予想を行い、手を読むときには返答の速い浅いネットワークを協調させて大きな性能を達成しています。また物体認識のための初期のInception netでは3つの出力を使って多数決で決めていました。一つのモデルを巨大にすると細部までの学習ができなくなるため、別々のケースを各モデルに個別に学習させるアンサンブル学習は分かりやすいです。

- [ ] [アンサンブル学習の２つのメリット・利点(回帰分析・クラス分類)](http://univprof.com/archives/16-06-15-3949163.html)
- [ ] [AlphaGo | Deep Mind](https://deepmind.com/research/alphago/)


個人的にですが、異なる目的のネットワークが協調動作する様はまさに生命と言って良い面白さがあります。下記は2016年のILSVRCの結果ですが、ほとんどのモデルがアンサンブルモデルとなっていて大きなトレンドをなしている状態です。

&lt;div align=&quot;center&quot;&gt;&lt;img alt=&quot;ensemble.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/863d16c7-f62b-7b12-f4c2-b2312d9d1cf9.png&quot;&gt;&lt;/div&gt;


## フレームの進化

***
* 学習の仕方（ウェイトの更新方法）を学習するフレームワーク

現在のDLの根幹はバックプロパゲーションによるウェイト／バイアスの学習アルゴリズムです。但し今の方法だとレイヤーが増えた場合に勾配消失が発生して発散してしまいますし、何より利用する関数が微分可能でないといけないという制約があります。

下記の論文はネイチャーにも載って話題になった論文です。今までの手法ではウェイトを更新して行く時に現在のウェイト値を使っていくのですが、代わりにランダムなウェイトを用意して値の更新に利用しそのウェイトを学習させて行く方式です。これによってMNISTのような手書き数字の認識で通常のバックプロパゲーションよりも高い性能が得られたとしています。

![ncomms13276-f1.png](https://qiita-image-store.s3.amazonaws.com/0/145675/5c133bb1-8aac-3752-6e91-dbac5f2a8ca9.png)

Learning to learn とも言うべき手法で、どのように学習すれば良いかを学習する手法はいろいろ提案されています。上記はFeedback Alignmenと呼ばれる手法ですが、他にもSynthetic Gradientや Target Propという方法が提案されています。

- [ ] [Random synaptic feedback weights support error backpropagation for deep learning](http://www.nature.com/articles/ncomms13276)


***
* ネットワークを作るネットワーク

この論文も非常に面白いです。ハイパーパラメータの自動チューニングとも言うべきものではありますが、RNNを用いてネットワークを自動構築・強化していきます。

&lt;div align=&quot;center&quot;&gt;
&lt;img alt=&quot;REINFORCEMENT LEARNING1.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/1cae0a62-4390-8724-84d6-030004b409c2.png&quot;&gt;

&lt;img alt=&quot;REINFORCEMENT LEARNING2.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/2c62ef25-9c3e-456d-cb53-bda80e5b0e16.png&quot;&gt;
&lt;/div&gt;

CIFAR-10という、ILSVRCよりも簡単めな物体認識のタスクを解くモデルを生成し強化していきます。結果的に人間が設計し優勝した過去のモデルとほぼ同等のモデルを作成できたとのこと。

&lt;div align=&quot;center&quot;&gt;
&lt;img alt=&quot;REINFORCEMENT LEARNING3.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/77d071e6-2b27-8cb6-2f71-a36b18c01eaa.png&quot;&gt;
&lt;/div&gt;

上記は左がLSTMのモデルで、右が生成されたネットワークの一部です。単なるパラメータチューニングの話題ともとれますが、これは応用して行けばより複雑なネットワークを自動構築でき、生成できるモデルの表現力はかなり高くなる可能性があります。強化学習を用いた同様の論文は幾つかあり、面白い可能性を秘めていると思います。

- [ ] [Neural Architecture Search with Reinforcement Learning](https://arxiv.org/pdf/1611.01578v1.pdf)
- [ ] [Learning to reinforcement learn](https://arxiv.org/abs/1611.05763)

***
* Differentiable Neural Computer

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/bf71d6d2-2f8a-b428-584f-58cffb2dd18c.png&quot;&gt;&lt;/div&gt;

Differentiable Neural Computer もDeepMindから発表されたものです。従来のDLの枠組みにはまらない新しいもので、イメージとしてはチューリングマシンに非常に近いです。大きな特徴として自由なメモリ領域を持ちます。

DNCはデータが入力されるとそれに合わせてヘッドを動かしてメモリからデータを読み込みます。さらにリンク情報に従って順次別のメモリも読んで行きます。読み込みが終わったら出力を出すと同時に空いているメモリを探してそこに情報を書き込んだりメモリの消去を行います。メモリから出力された情報は次に来た入力データと合わせてRNNのように処理されます。

論文ではパズルを解いたり地下鉄の路線図や家系図を解析するなどいろいろなタスクが可能になったとされており今後の大きな可能性を感じます。

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/eaff4cb9-c7db-b85b-3eb5-f32d04ae5ab3.png&quot;&gt;&lt;/div&gt;

[Differentiable Neural Computer](https://deepmind.com/blog/differentiable-neural-computers/)



## The Deeper, The Better

ここまで見てきたモデルはどれも話題になった有名なものですが、これ以外にも非常に沢山のモデルが考案されています。現在のトレンドとしては2つあり、パラメータを少なくして学習しやすくなる代わりに層を深くしていく方向と、複数のモデルを使っていく方向です。例えば下記の図は各年のILSVRCの優勝モデルの深さの変化です。但しどちらかというと単体のモデル自体は複雑にするのではなくシンプルにしつつ深くしていって、アンサンブルで組み合わせるというのが多いように思います。_(そうは言ってもまだまだ湯水のごとく複雑なモデルが考案されていますが)_

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://qiita-image-store.s3.amazonaws.com/0/145675/69db4b3d-f5fa-fb28-f956-8c0573dcbf47.png&quot;&gt;&lt;/div&gt;


# 性能改善手法
ここもちょっと長くなったので別のページとしてまとめました。

[Inside of Deep Learning （ディープラーニングの性能改善手法 一覧）](http://qiita.com/jiny2001/items/1d3f2d0370b2689d2da7) 


# DLは何を変えるのか?

DLの進化は非常に早く、今回紹介したモデルや手法はほとんどが既に過去のものです。それぞれ多くの派生モデルが提案されています。

下記のページがよくまとまっていますが、2016年も恐ろしい数の論文が提出されその話題の幅も非常に幅広いものでした。是非ちらっとでも確認されてみると良いと思います。

- [ ] [2016年のディープラーニング論文100選](http://qiita.com/sakaiakira/items/9da1edda802c4884865c)
- [ ] [DeepLearning研究 2016年のまとめ](http://qiita.com/eve_yk/items/f4b274da7042cba1ba76)
- [ ] [foobarNet: ディープラーニング関連の○○Netまとめ](http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7)


## DLは本当に&quot;賢い&quot;のか？ 弱点は？

大手メディアの記事を読んでいると非常に賢いと思えてしまうDLを利用したAIですが、実際はちょっとイメージが違うと思います。

例えばDQNを用いてブロック崩しを解くAIが発表された時、幾つかのメディアは”ルールを勝手に推測して”プレイすると記事にしていました。ですが実際はルールなどは理解せずにただその状況ではどの行動をとれば良かったかを経験から学習して選んでいるだけです。（もちろんCNNを用いてゲーム中の画像から現在の状態把握ができるようになったという点は大きな進歩ではありますが）

あるいはAlphaGoの時も&quot;棋士が次に打つ手を57%の精度で予測&quot;などと書かれていました。この表現だと__まるで相手の打つ手を高確率で予測できるように聞こえます__が、実際はそうではありません。次にコンピュータが選んだ手が、過去のプロの対戦の中で使われたようなまともな手である確率が57%です。43%の確率で変な手を選んでしまうのです。

とにかく言いたかったことは、メディアの記事は鵜呑みにせず興味がある場合はできるだけ引用元を読みましょうということと、今のAIはあくまで限定的な状況での最適化がメイントピックであるということです。

__そして弱点というとまず思い浮かぶのは、どうしても学習用データが必要になることです。__教師データが作れない分野はまだまだ多く、また教師なし学習でできることは非常に限られているように思えます。


## 医療革命、生産革命、エネルギー問題、そして...?

知能の実現はまだ難しそうですが、逆に言えば非常に単純なタスクか、あるいはスペシャリストが対応するような限定された問題の両極端なところから実用化が始まっていきそうです。

現在でも既に各種の画像処理、翻訳や文章解析、音声認識などで一気に技術シーンを塗り替えていますが、これからは各種医療技術に応用されガンの予測や遺伝子解析、病気の診断などが一気に進んで行くでしょう。

PFNやnVidiaさんが勧めているIoT機器の知能化も非常に面白いです。モバイルやIoT機器にDL処理用のチップを使って知能化し、ネットに繋がなくても手元で高度なAIが動いたり、あるいはデバイスが得たデータをクラウドに送ってさらなる学習を中央で行いデバイスにアップデートして行くことが可能になります。工場や各種の生産技術はさらなる知能化や無人化が進み生産能力が大きく上がるのは間違いなさそうです。

- [ ] [手紙やフォームに入力された文章を従来の10倍の速度で認識する Captricity](https://captricity.com)
- [ ] [産業用ロボットの知能化を行う東大発ベンチャー MUJIN](https://www.ut-ec.co.jp/portfolio/mujin)

DLは計算量も多く、大量に電気を消費します。最後に残された問題はエネルギー問題でしょうか。ただしそれもAIがエネルギー消費の効率化を進めています。おそらく電力が足りなくなるのは過渡期だけで、最終的には全てが効率化されエネルギーがむしろ余りそうな気もします。需要予測や最適化はAIの得意分野ですし、余ったエネルギーは無駄なく効率よくAIが使ってくれることでしょう。

- [ ] [DeepMind AI Reduces Google Data Centre Cooling Bill by 40%](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)

- [ ] [Short-Term Price Forecasting Models Based on Artificial Neural Networks for Intraday Sessions in the Iberian Electricity Market](http://www.mdpi.com/1996-1073/9/9/721/htm)

それで、そんな風に世の中の生産効率が上がり続けて行くとどうなるか？
これはDLが話題になり始めた数年前から考えていたことなのですが、人の時間の使い方としてはより原始的な方向に戻るのではないかと思ってます。

単に自分だけや家族、友人、会社のためだけに時間や労力を割くのではなく、見ず知らずの人にさえ時間を使って貢献するような社会です。AIが世の中を管理すると言ってもそれは恐らく一部の指標に対して社会の中の変更して良い部分を最適化するという話であって、そんなに悪い社会になるとは僕には思えません。

アメリカの人々の生活を見ていると、進行する格差の拡大が大きな問題に思えます。この拡大を社会が真剣に止めようとできるかどうかが分かれ目で、結局は良い方向にAIを使う道へ進むと信じてます。



## ソフトエンジニアの在り方は変わるか？

「深層学習は何を変えるのか」なんていう立派なお題をつけてしまったわりには、実を言うと世の中がどう変わっていくかについてはそんなに良く分かりません。実はエンジニアの身の回りの変化についての考察がメインでした。

一番気になるのは、__「自分の子どもたちにエンジニアになることを勧めるべきか否か」__です。

ぱっと見ではAIを作る側にいるエンジニアは安泰なようにも思えますが...

これは究極的に言えば、&quot;AIを作るAI&quot;を作ることは現実的なのかという問題に近いと思います。


例えば現時点で既にLSTMでプログラムコードを読んで実行する”Learning to Execute”という論文があります。

```
Input:
  j=8584
  for x in range(8):
    j+=920
  b=(1500+j)
  print((b+7567))
```

上記のようなテキスト行を入力した場合に、```Target: 25011.``` という答えを予測して出力できるのです（ちなみに上記の出力結果は25011です）。これはソースコードとその出力を学習させてやるだけでそれぞれの演算子やfor文の意味をシミュレートできるようになったという面白い論文です。（実際はそこまで簡単には実現できなかったっぽいですが）

あるいは「オプティマイザのオプティマイザ」とも言うべきもので、やはりLSTMを用いてオプティマイザ自体を学習してしまう論文も発表されています。上記で説明したようなモデルを自動生成するLearning to Learnと言われる、学習方法自体を学習するアイデア・実験も沢山あります。

つまりは既にその兆候は現れています。

- [ ] [Learning to Execute](https://arxiv.org/pdf/1410.4615.pdf)

- [ ] [Learning to learn by gradient descent by gradient descent](https://arxiv.org/pdf/1606.04474v2.pdf)

今まで見てきた進化の速度から考えるに、恐らく子供達が今の僕の年になる30年後には、今のようなソフトウェア職への需要は大きく下がっているのではと感じています。ベイエリアで年収2000万円もらってひーはーやってるエンジニアの職は多分なくなりません。問題なのはIT土方なんて言われるような大部分の中産階級のエンジニアです。（僕のようなニートは真っ先に消されます :) ）

少なくともソースコードを書く部分はかなりの領域をAIが担当すると思います。人間や社会のことを理解し、ソフトの仕様を定義する&quot;人間&quot;は一定数必要になるのではと思いますが、仕様を箇条書きで書くだけなので難しい要素は全くありません。

仕様をいかにコンピュータが分かる形の知識表現に落とすか、という問題はそんなに難しくない気がします。人間の書いた仕様テキストをそのまま入力として理解できるようになるのもそう遠くないのではないでしょうか。仕様ができたら後はAIがコードを生成し、もし矛盾があれば自動で直してくれるか、優先順位やさらなる仕様を要求してもらいます。もちろんAIはこちらの性格を把握済みですから、こちらが気を悪くせずに仕事ができるよう最大限の配慮をしてくれます。もしかしたら今の上司や同僚よりも気持ちよく仕事ができたりするかもしれません。


AIのチューニングはほとんどAIがやります。人間も一部手伝うとは思いますが、今のようなプログラムコードを書くやりかたではなく、何かビジュアルなツールを用いてモデルの構造や組み合わせを変えたりするような感じになるのではと思いました。

結論としては、子供にプログラミングを教えるのはちょっと躊躇っています。

僕自身は高校時代からプログラミングを始めて凄いハマったのでちょっと寂しい気はします。もちろん論理的思考やクリエイティビティを刺激するようなものは積極的に与えたいです。かつての僕がしたように、CPUの動作やアドレスバスの動作を理解させてコンピュータの動作原理を理解させていくのも良いと思います。ただし今や車のことを分からなくても運転できるのと同じことが起こるだろうという感覚はあります。

プログラミングの形は大きく変わり、開発に必要な人数も大きく減るだろうなとは感じます。コンピュータの仕組みやサイエンスを教えずにプログラミングだけを教えるのはやはり30年後の子供達にとってリスクになるのではないでしょうか。


## 最後に

この道を行けば、どうなるものか。

危ぶむなかれ。危ぶめば道はなし。

迷わず行けよ、行けばわかるさ。

みんなでやろう、ＤL！
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Inside of Deep Learning あるいは深層学習は何を変えるのか by @Jiny2001 on @Qiita" data-url="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Inside of Deep Learning あるいは深層学習は何を変えるのか" href="http://b.hatena.ne.jp/entry/http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/jiny2001"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://avatars.githubusercontent.com/u/12959344?v=3" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/jiny2001">jiny2001</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">399</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;jiny2001&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-74d3c284-351d-4313-89fe-03bd93f00f5d"></div>
    <div id="UserFollowButton-react-component-74d3c284-351d-4313-89fe-03bd93f00f5d"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/jiny2001/items/85af7dd163a63b3a152a">Inside of Deep Learning あるいは深層学習は何を変えるのか</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/jiny2001/items/1d3f2d0370b2689d2da7">Inside of Deep Learning （ディープラーニングの性能改善手法 一覧）</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/jiny2001/items/e3961a009690af0c435c">Inside of Deep Learning （ディープラーニングの基本要素）</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/jiny2001/items/ee816b442e1ee8ed7a5c">ディープラーニングによる超解像(Deeply-Recursive Convolutional Network)のtensorflow実装</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/jiny2001/items/7af1f5dbfaa6c5e4a9f2">tensorflow 各環境でのCPU / GPUベンチマーク結果 (1.0での結果追加)</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%93%E3%81%AE%E3%83%9D%E3%82%B9%E3%83%88%E3%81%AE%E6%96%B9%E9%87%9D\&quot;\u003eこのポストの方針\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%86%85%E5%AE%B9\&quot;\u003e内容\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dl%E3%81%AF%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B\&quot;\u003eDLは何を変えたか\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#1-%E3%83%93%E3%83%83%E3%82%B0%E3%83%87%E3%83%BC%E3%82%BF%E5%AF%BE%E5%BF%9C\&quot;\u003e1. ビッグデータ対応\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#2-%E8%A4%87%E9%9B%91%E5%A4%9A%E5%B1%A4%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AE%9F%E7%8F%BE\&quot;\u003e2. 複雑／多層モデルの実現\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#3-%E6%B1%8E%E5%8C%96%E6%80%A7%E8%83%BD%E9%9D%9E%E7%B7%9A%E5%BD%A2%E3%82%B7%E3%82%B9%E3%83%86%E3%83%A0%E3%81%B8%E3%81%AE%E4%BF%A1%E9%A0%BC%E6%80%A7%E3%81%AE%E5%90%91%E4%B8%8A\&quot;\u003e3. 汎化性能（非線形システムへの信頼性）の向上\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#4-%E6%99%82%E7%B3%BB%E5%88%97%E3%83%87%E3%83%BC%E3%82%BF%E3%82%84%E5%A4%9A%E6%AC%A1%E5%85%83%E3%83%87%E3%83%BC%E3%82%BF%E3%81%B8%E3%81%AE%E6%8B%A1%E5%BC%B5\&quot;\u003e4. 時系列データや多次元データへの拡張\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dl%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B\&quot;\u003eDLとは何か？何を変えたか？\u003c/a\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%81%9F%E3%81%8B\&quot;\u003e何を変えたか？\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0\&quot;\u003e基本要素\u003c/a\u003e\n\n\n\u003ca href=\&quot;#dl%E3%81%AE%E3%83%87%E3%83%A1%E3%83%AA%E3%83%83%E3%83%88\&quot;\u003eDLのデメリット\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E9%81%8E%E5%AD%A6%E7%BF%92%E3%81%97%E3%82%84%E3%81%99%E3%81%84\&quot;\u003e過学習しやすい\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%A4%A7%E9%87%8F%E3%81%AE%E5%AD%A6%E7%BF%92%E7%94%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8C%E5%BF%85%E8%A6%81\&quot;\u003e大量の学習用データが必要\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%86%A8%E5%A4%A7%E3%81%AA%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E3%83%AA%E3%82%BD%E3%83%BC%E3%82%B9%E3%81%8C%E5%BF%85%E8%A6%81\&quot;\u003e膨大なコンピューティングリソースが必要\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%81%AE%E5%8F%8E%E6%9D%9F%E3%81%8C%E9%9B%A3%E3%81%97%E3%81%84\&quot;\u003eパラメータの収束が難しい\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%80%A7%E8%83%BD%E6%94%B9%E5%96%84%E3%81%8C%E9%9B%A3%E3%81%97%E3%81%84\&quot;\u003e性能改善が難しい\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E5%90%84%E7%A8%AE%E6%94%B9%E5%96%84%E6%89%8B%E6%B3%95%E6%9C%80%E6%96%B0%E4%BA%8B%E4%BE%8B\&quot;\u003e各種改善手法・最新事例\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E9%80%B2%E5%8C%96\&quot;\u003eモデルの進化\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E3%81%AE%E9%80%B2%E5%8C%96\&quot;\u003eフレームの進化\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#the-deeper-the-better\&quot;\u003eThe Deeper, The Better\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E6%80%A7%E8%83%BD%E6%94%B9%E5%96%84%E6%89%8B%E6%B3%95\&quot;\u003e性能改善手法\u003c/a\u003e\n\n\n\u003ca href=\&quot;#dl%E3%81%AF%E4%BD%95%E3%82%92%E5%A4%89%E3%81%88%E3%82%8B%E3%81%AE%E3%81%8B\&quot;\u003eDLは何を変えるのか?\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dl%E3%81%AF%E6%9C%AC%E5%BD%93%E3%81%AB%E8%B3%A2%E3%81%84%E3%81%AE%E3%81%8B-%E5%BC%B1%E7%82%B9%E3%81%AF\&quot;\u003eDLは本当に\&quot;賢い\&quot;のか？ 弱点は？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8C%BB%E7%99%82%E9%9D%A9%E5%91%BD%E7%94%9F%E7%94%A3%E9%9D%A9%E5%91%BD%E3%82%A8%E3%83%8D%E3%83%AB%E3%82%AE%E3%83%BC%E5%95%8F%E9%A1%8C%E3%81%9D%E3%81%97%E3%81%A6\&quot;\u003e医療革命、生産革命、エネルギー問題、そして...?\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%82%BD%E3%83%95%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%81%AE%E5%9C%A8%E3%82%8A%E6%96%B9%E3%81%AF%E5%A4%89%E3%82%8F%E3%82%8B%E3%81%8B\&quot;\u003eソフトエンジニアの在り方は変わるか？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%9C%80%E5%BE%8C%E3%81%AB\&quot;\u003e最後に\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-a3ac179c-710b-44a6-8d9d-a00a67b29849"></div>
    <div id="Toc-react-component-a3ac179c-710b-44a6-8d9d-a00a67b29849"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:280,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;85af7dd163a63b3a152a&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="matarillo"><a itemprop="url" href="/matarillo"><img alt="matarillo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/2079/profile-images/1473680702" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Leonhalt2714"><a itemprop="url" href="/Leonhalt2714"><img alt="Leonhalt2714" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/23418/profile-images/1473683834" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="toronya"><a itemprop="url" href="/toronya"><img alt="toronya" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31723/profile-images/1473685756" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="inoory"><a itemprop="url" href="/inoory"><img alt="inoory" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/61468/profile-images/1473695664" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Tsutomu-KKE@github"><a itemprop="url" href="/Tsutomu-KKE@github"><img alt="Tsutomu-KKE@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/13955/profile-images/1473683126" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="manabedaiki"><a itemprop="url" href="/manabedaiki"><img alt="manabedaiki" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/104597/profile-images/1473709346" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="TokyoMickey"><a itemprop="url" href="/TokyoMickey"><img alt="TokyoMickey" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/141216/profile-images/1483272512" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ryo_grid"><a itemprop="url" href="/ryo_grid"><img alt="ryo_grid" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/12325/profile-images/1473682364" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="GushiSnow"><a itemprop="url" href="/GushiSnow"><img alt="GushiSnow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/10496/profile-images/1473757289" /></a></div></div><div class="ArticleFooter__user"><a href="/jiny2001/items/85af7dd163a63b3a152a/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/85af7dd163a63b3a152a/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/jiny2001/items/85af7dd163a63b3a152a.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/jiny2001/items/e3961a009690af0c435c#_reference-a8d1e33169214020a9fc"><img alt="" width="18" height="18" src="https://avatars.githubusercontent.com/u/12959344?v=3" />Inside of Deep Learning （ディープラーニングの基本要素）</a><time class="references_datetime js-dateTimeView" datetime="2017-01-20T18:26:43+00:00">about 2 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/jiny2001/items/1d3f2d0370b2689d2da7#_reference-7ba02b2d0bc5b1794266"><img alt="" width="18" height="18" src="https://avatars.githubusercontent.com/u/12959344?v=3" />Inside of Deep Learning （ディープラーニングの性能改善手法 一覧）</a><time class="references_datetime js-dateTimeView" datetime="2017-01-23T06:16:27+00:00">about 2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Inside of Deep Learning あるいは深層学習は何を変えるのか by @Jiny2001 on @Qiita" data-url="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Inside of Deep Learning あるいは深層学習は何を変えるのか" href="http://b.hatena.ne.jp/entry/http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/jiny2001/items/85af7dd163a63b3a152a" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:460107,&quot;uuid&quot;:&quot;85af7dd163a63b3a152a&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;jiny2001&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:145675,&quot;url_name&quot;:&quot;jiny2001&quot;,&quot;profile_image_url&quot;:&quot;https://avatars.githubusercontent.com/u/12959344?v=3&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-7ab8c702-7950-41e1-902c-3b8b81d556a5"></div>
    <div id="CommentListContainer-react-component-7ab8c702-7950-41e1-902c-3b8b81d556a5"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="Uiq5gEMRctGB+XWWxfbubn4NfNAwf792RZgpXPCLE4tMogBuFts9s/7Of9ZmVyLwAJpds5LBUenV/22PcJkStg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/jiny2001/items/85af7dd163a63b3a152a" /><input type="hidden" name="item_uuid" id="item_uuid" value="85af7dd163a63b3a152a" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/jiny2001/items/85af7dd163a63b3a152a", "id": 460107, "uuid": "85af7dd163a63b3a152a" }</script><script class="js-user" type="application/json">{&quot;id&quot;:145675,&quot;url_name&quot;:&quot;jiny2001&quot;,&quot;profile_image_url&quot;:&quot;https://avatars.githubusercontent.com/u/12959344?v=3&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="yeN9kY8/bGOhNA5ytz9v1AtMp9g/gVANf7sdrOvLDnjXa8R/2vUjAd4DBDIUnqNKdduGu50/vpLv3Fl/a9kPRQ==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/jiny2001/items/85af7dd163a63b3a152a" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>