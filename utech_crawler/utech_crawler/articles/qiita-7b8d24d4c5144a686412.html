<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="今話題のDeep Learning(深層学習)フレームワーク、Chainerに手書き文字の判別を行うサンプルコードがあります。こちらを使って内容を少し解説する記事を書いてみたいと思います。

(本記事のコードの全文をGitHubにアップしました。[PC推奨])

とにかく、インストールがすごく簡単かつ、Pythonが書ければすぐに使うことができておすすめです！
Pythonに閉じてコードが書けるのもすごくいいですよね。

こんな感じのニューラルネットワークモデルを試して..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="Kenmatsu4" name="twitter:creator" /><meta content="【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="今話題のDeep Learning(深層学習)フレームワーク、[Chainer](http://chainer.org/)に手書き文字の判別を行うサンプルコードがあります。こちらを使って内容を少し解説する記事を書いてみたいと思います。..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="sxVHSqmiLQIoOz9bVJ9kGcJnhswwD3c8ra0PN8JnmG6kcqP9V+4GlFtDu2Fp6YmjDNVN8goMGShuEMbQzPXOKw==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"kenmatsu4","type":"items","id":"7b8d24d4c5144a686412"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;募集&quot;,&quot;content&quot;:&quot;QiitaやQiita:Teamを良くしたいエンジニア&quot;,&quot;url&quot;:&quot;http://increments.co.jp/jobs/engineers?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-93ad3ac1-ee17-4333-a7d7-4ebccd17d881"></div>
    <div id="HeaderContainer-react-component-93ad3ac1-ee17-4333-a7d7-4ebccd17d881"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/Python",        "name": "Python"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。</h1><ul class="TagList"><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span><div class="tagIcon_version">2.7</div></a></li><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="358"><a class="u-link-unstyled TagList__label" href="/tags/Chainer"><img alt="Chainer" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/755fdcf477b1d3db5946dad4f779ba11a5954c18/medium.jpg?1434432587" /><span>Chainer</span></a></li><li class="TagList__item" data-count="126"><a class="u-link-unstyled TagList__label" href="/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0"><img alt="ディープラーニング" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>ディープラーニング</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">1585</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="26 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>26</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:1585,&quot;uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="HirofumiYashima"><a itemprop="url" href="/HirofumiYashima"><img alt="HirofumiYashima" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" /></a></li><li class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></li><li class="js-hovercard" data-hovercard-target-name="yacchin1205"><a itemprop="url" href="/yacchin1205"><img alt="yacchin1205" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/28770/profile-images/1473685246" /></a></li><li class="js-hovercard" data-hovercard-target-name="_kuni88"><a itemprop="url" href="/_kuni88"><img alt="_kuni88" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/53788/profile-images/1473693161" /></a></li><li class="js-hovercard" data-hovercard-target-name="toshi19890331"><a itemprop="url" href="/toshi19890331"><img alt="toshi19890331" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/25513/profile-images/1473684349" /></a></li><li class="js-hovercard" data-hovercard-target-name="K-1"><a itemprop="url" href="/K-1"><img alt="K-1" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/37387/profile-images/1473687407" /></a></li><li class="js-hovercard" data-hovercard-target-name="sinsnk"><a itemprop="url" href="/sinsnk"><img alt="sinsnk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/82692/profile-images/1473702534" /></a></li><li class="js-hovercard" data-hovercard-target-name="user19"><a itemprop="url" href="/user19"><img alt="user19" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/69767/profile-images/1473698259" /></a></li><li class="js-hovercard" data-hovercard-target-name="shirayu"><a itemprop="url" href="/shirayu"><img alt="shirayu" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/50725/profile-images/1475554726" /></a></li><li class="js-hovercard" data-hovercard-target-name="Reds"><a itemprop="url" href="/Reds"><img alt="Reds" class="thumb thumb--xs" src="https://0.gravatar.com/avatar/e751888e30cdc35253c1a3b241977adf?d=https%3A%2F%2Fidenticons.github.com%2F0e2f15414d97b8b642022be91ff105e0.png" /></a></li><li><a href="/kenmatsu4/items/7b8d24d4c5144a686412/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/kenmatsu4"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184" alt="1473692184" /></a> <a class="u-link-unstyled" href="/kenmatsu4">kenmatsu4</a> </div><div class="ArticleAsideHeader__date"><meta content="2015-06-19T07:56:21+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2015-06-19">Edited at <time datetime="2015-09-27T00:01:30+09:00" itemprop="dateModified">2015-09-27</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/kenmatsu4/items/7b8d24d4c5144a686412/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">32</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/kenmatsu4/items/7b8d24d4c5144a686412/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(32)</span></a></li><li><a href="/kenmatsu4/items/7b8d24d4c5144a686412.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-7b8d24d4c5144a686412" itemprop="articleBody"><div class="alert alert-warning"><i class="fa fa-clock-o"></i> More than 1 year has passed since last update.</div><p>今話題のDeep Learning(深層学習)フレームワーク、<a href="http://chainer.org/" rel="nofollow noopener" target="_blank">Chainer</a>に手書き文字の判別を行うサンプルコードがあります。こちらを使って内容を少し解説する記事を書いてみたいと思います。</p>

<p><strong>(本記事のコードの全文を<a href="https://github.com/matsuken92/Qiita_Contents/blob/master/chainer-MNIST/chainer-MNIST_forPubs.ipynb" rel="nofollow noopener" target="_blank">GitHub</a>にアップしました。[PC推奨])</strong></p>

<p>とにかく、インストールがすごく簡単かつ、Pythonが書ければすぐに使うことができておすすめです！<br>
Pythonに閉じてコードが書けるのもすごくいいですよね。</p>

<p>こんな感じのニューラルネットワークモデルを試してみる、という記事です。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png" alt="nn_structure6.png"></a></p>

<p>主要な情報はこちらにあります。<br>
<a href="http://chainer.org/" rel="nofollow noopener" target="_blank">Chainerのメインサイト</a><br>
<a href="https://github.com/pfnet/chainer" rel="nofollow noopener" target="_blank">ChainerのGitHubリポジトリ</a><br>
<a href="http://docs.chainer.org/en/latest/" rel="nofollow noopener" target="_blank">Chainerのチュートリアルとリファレンス</a></p>

<h1>
<span id="1-インストール" class="fragment"></span><a href="#1-%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB"><i class="fa fa-link"></i></a>1. インストール</h1>

<p>まずは何はともあれインストールです。ChainerのGitHubに記載の"Requirements" ( <a href="https://github.com/pfnet/chainer#requirements" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/pfnet/chainer#requirements</a> )を参考に必要なソフト、ライブラリをインストールした上で</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>
pip install chainer
</pre></div></div>

<p>を実行します。<br>
これだけでインストールできちゃいます。超簡単！CaffeをMacにインストールしようとした時はかなり苦戦しましたが、嘘のようです <img alt=":smile:" class="emoji" height="20" src="https://cdn.qiita.com/emoji/twemoji/unicode/1f604.png" title=":smile:" width="20"> </p>

<p>もし、インストールに詰まったらcvl-robotさんの「<a href="http://cvl-robot.hateblo.jp/entry/2015/06/11/223928" rel="nofollow noopener" target="_blank">DeepLearningライブラリのChainerがすごい、らしい</a>」という記事が詳しく必要なライブラリ等のインストールについて記載してくれていて便利です。</p>

<h1>
<span id="2サンプルコードの入手" class="fragment"></span><a href="#2%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E5%85%A5%E6%89%8B"><i class="fa fa-link"></i></a>2.サンプルコードの入手</h1>

<p>GitHubの下記ディレクトリにおなじみMNISTの手書き文字を判別する、というサンプルがありますので、これを題材としたいと思います。これをChainerの順伝播型ニューラルネットワークでClassificationしてみる、という試みです。<br>
<a href="https://github.com/pfnet/chainer/tree/master/examples/mnist" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/pfnet/chainer/tree/master/examples/mnist</a><br>
　　┗ train_mnist.py</p>

<p>このコードにコメントを加えたり、一部途中のフローをグラフで表示してイメージをつけたりしながら見ていきたいと思います。</p>

<h1>
<span id="3サンプルコードを見ていく" class="fragment"></span><a href="#3%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E8%A6%8B%E3%81%A6%E3%81%84%E3%81%8F"><i class="fa fa-link"></i></a>3.サンプルコードを見ていく</h1>

<p>今回、手持ちのMacbook Air(OS X ver10.10.2)での動作確認をしながら書いていますので、環境によっては差分があるかもしれませんが、そこはよしなに見てもらえればと思っています。また、こういった環境のためGPUでの計算は行わずCPUのみとなりますのでGPU関連のコードは省略して記載します。</p>

<h2>
<span id="3-1準備" class="fragment"></span><a href="#3-1%E6%BA%96%E5%82%99"><i class="fa fa-link"></i></a>3-1.準備</h2>

<p>まず最初に必要なライブラリ群のインポートです。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_mldata</span>
<span class="kn">from</span> <span class="nn">chainer</span> <span class="kn">import</span> <span class="n">cuda</span><span class="p">,</span> <span class="n">Variable</span><span class="p">,</span> <span class="n">FunctionSet</span><span class="p">,</span> <span class="n">optimizers</span>
<span class="kn">import</span> <span class="nn">chainer.functions</span>  <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'ggplot'</span><span class="p">)</span>
</pre></div></div>

<p>次に各種パラメーターの定義・設定を行います。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># 確率的勾配降下法で学習させる際の１回分のバッチサイズ</span>
<span class="n">batchsize</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c"># 学習の繰り返し回数</span>
<span class="n">n_epoch</span>   <span class="o">=</span> <span class="mi">20</span>

<span class="c"># 中間層の数</span>
<span class="n">n_units</span>   <span class="o">=</span> <span class="mi">1000</span>
</pre></div></div>

<p>Scikit LearnをつかってMNISTの手書き数字データをダウンロードします。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># MNISTの手書き数字データのダウンロード</span>
<span class="c"># #HOME/scikit_learn_data/mldata/mnist-original.mat にキャッシュされる</span>
<span class="k">print</span> <span class="s">'fetch MNIST dataset'</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">fetch_mldata</span><span class="p">(</span><span class="s">'MNIST original'</span><span class="p">)</span>
<span class="c"># mnist.data : 70,000件の784次元ベクトルデータ</span>
<span class="n">mnist</span><span class="o">.</span><span class="n">data</span>   <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">mnist</span><span class="o">.</span><span class="n">data</span>  <span class="o">/=</span> <span class="mi">255</span>     <span class="c"># 0-1のデータに変換</span>

<span class="c"># mnist.target : 正解データ（教師データ）</span>
<span class="n">mnist</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
</pre></div></div>

<p>3つくらい取り出して描画してみます。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># 手書き数字データを描画する関数</span>
<span class="k">def</span> <span class="nf">draw_digit</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">28</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="nb">range</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span>   <span class="c"># convert from vector to 28x28 matrix</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>             <span class="c"># flip vertical</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">draw_digit</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
<span class="n">draw_digit</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">12345</span><span class="p">])</span>
<span class="n">draw_digit</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">33456</span><span class="p">])</span>
</pre></div></div>

<p>28x28, 784次元ベクトルのこんなデータですね。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/0e36e752-3fea-716a-c331-2bf8a15b47de.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/0e36e752-3fea-716a-c331-2bf8a15b47de.png" alt="digits-compressor.png"></a></p>

<p>データセットを学習用データ検証用データに分割します。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># 学習用データを N個、検証用データを残りの個数と設定</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">60000</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>   <span class="p">[</span><span class="n">N</span><span class="p">])</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="p">[</span><span class="n">N</span><span class="p">])</span>
<span class="n">N_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">size</span>
</pre></div></div>

<h2>
<span id="32-モデルの定義" class="fragment"></span><a href="#32-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AE%9A%E7%BE%A9"><i class="fa fa-link"></i></a>3.2 モデルの定義</h2>

<p>いよいよモデルの定義です。ここからが本番ですね。Chainerのクラスや関数を使います。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># Prepare multi-layer perceptron model</span>
<span class="c"># 多層パーセプトロンモデルの設定</span>
<span class="c"># 入力 784次元、出力 10次元</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">FunctionSet</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="n">n_units</span><span class="p">),</span>
                    <span class="n">l2</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="n">n_units</span><span class="p">),</span>
                    <span class="n">l3</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_units</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div></div>

<p>入力の手書き数字のデータが784次元ベクトルなので、入力素子は784個になります。今回中間層はn_unitsで1000と指定しています。出力は、数字を識別するので10個になります。下記がこのモデルのイメージです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png" alt="nn_structure6.png"></a></p>

<p>順伝播の構造が下記のforward()関数で定義されます。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># Neural net architecture</span>
<span class="c"># ニューラルネットの構造</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">x_data</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span>  <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">h1</span><span class="p">)),</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">y</span>  <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
    <span class="c"># 多クラス分類なので誤差関数としてソフトマックス関数の</span>
    <span class="c"># 交差エントロピー関数を用いて、誤差を導出</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div></div>

<p>ここで各関数等を説明したいと思います。<br>
Chainerのお作法で、データは配列からChainerのVariableという型（クラス）のオブジェクトに変換して使います。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="p">),</span> <span class="n">Variable</span><span class="p">(</span><span class="n">y_data</span><span class="p">)</span>
</pre></div></div>

<p>活性化関数はシグモイド関数ではなく、F.relu()関数が使われています。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div></div>

<p>このF.relu()は正規化線形関数(Rectified Linear Unit function)で</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
f(x) = \max(0, x)
</pre></div></div>

<p>つまり</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/e1cc4c94-e4ae-0010-82e5-c27956b5986c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/e1cc4c94-e4ae-0010-82e5-c27956b5986c.png" alt="relu-compressor.png"></a></p>

<p>こんな感じです。<br>
描画コードはこちら。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># F.reluテスト</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></div>

<p>シンプルな関数ですね。このため、計算量が小さく学習スピードが速くなることが利点のようです。</p>

<p>次に、このrelu()関数の出力を入力としてF.dropout()関数が使われています。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span>  <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
</pre></div></div>

<p>このドロップアウト関数F.dropout()は<a href="http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" rel="nofollow noopener" target="_blank">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>という論文で提唱されている手法で、ランダムに中間層をドロップ（ないものとする）し、そうすると過学習を防ぐことができるそうです。</p>

<p>ちょっと動かしてみましょう。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># dropout(x, ratio=0.5, train=True) テスト</span>
<span class="c"># x: 入力値</span>
<span class="c"># ratio: 0を出力する確率</span>
<span class="c"># train: Falseの場合はxをそのまま返却する</span>
<span class="c"># return: ratioの確率で0を、1−ratioの確率で,x*(1/(1-ratio))の値を返す</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">v_sum</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>
    <span class="n">dr</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ratio</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span><span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span> <span class="nb">str</span><span class="p">(</span><span class="n">dr</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">+</span> <span class="s">', '</span> <span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">""</span><span class="p">)</span>
    <span class="n">v_sum</span> <span class="o">+=</span> <span class="n">dr</span><span class="o">.</span><span class="n">data</span>

<span class="c"># outputの平均がx_dataとだいたい一致する </span>
<span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">write</span><span class="p">(</span> <span class="nb">str</span><span class="p">((</span><span class="n">v_sum</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">n</span><span class="p">)))</span> <span class="p">)</span>
</pre></div></div>

<div class="code-frame" data-lang="bash">
<div class="code-lang"><span class="bold">output</span></div>
<div class="highlight"><pre>
2.5, 5.0, 7.5, 0.0, 0.0, 0.0, 
2.5, 5.0, 7.5, 10.0, 0.0, 15.0, 
0.0, 5.0, 7.5, 10.0, 12.5, 15.0, 
　　　　　　・・・
0.0, 0.0, 7.5, 10.0, 0.0, 0.0, 
2.5, 0.0, 7.5, 10.0, 0.0, 15.0, 
<span class="o">[</span> 0.94999999  2.29999995  3.          3.5999999   7.25        5.69999981<span class="o">]</span>
</pre></div>
</div>

<p>[1,2,3,4,5,6]という配列をF.dropout()関数に渡します。いま、ratioはドロップアウト率であり、ratio=0.6を設定しているので、60%の確率でドロップアウトされ、0が出力されます。40%の確率で値が返されるのですが、その際、値を返す確率が40%に減ってしまっているので、それを補うために${1 \over 0.4}$倍=2.5倍された値が出力されます。つまり</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
(0 \times 0.6 + 2.5 \times 0.4) = 1
</pre></div></div>

<p>で、平均すると元の数字になるようになっています。上記の例だと最後の行が出力の平均ですが、50回繰り返して大体元の[1,2,3,4,5,6]に近い値になっています。</p>

<p>同じ構造がもう１層あり、出力され出力値が$y$となります。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">h1</span><span class="p">)),</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">)</span>
    <span class="n">y</span>  <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
</pre></div></div>

<p>最後の出力ですが、ソフトマックス関数と交差エントロピー関数を用いて誤差の出力。それとF.accuracy()関数で精度を返しています。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
    <span class="c"># 多クラス分類なので誤差関数としてソフトマックス関数の</span>
    <span class="c"># 交差エントロピー関数を用いて、誤差を導出</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</pre></div></div>

<p>ソフトマックス関数ですが、</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
y_k = z_k = f_{k}({\bf u})={\exp(u_{k}) \over \sum_j^K \exp(u_{j})}
</pre></div></div>

<p>のように定義される関数で、この関数を挟むことで$y_1, \cdots ,y_{10}$の10個の出力の総和が1となり、出力を確率として解釈することが可能になります。<br>
なぜ$\exp()$関数が使われているかというと、値がマイナスにならないように、ということと自分は理解しています。</p>

<p>おなじみ$\exp()$関数は<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/50670/50ff3c5d-cab5-65c7-f19a-48a383e0f957.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/50ff3c5d-cab5-65c7-f19a-48a383e0f957.png" alt="exp-compressor (1).png"></a><br>
のような形なので、マイナスの値を取りません。これにより値がマイナスにならず、かつ総和が１ということになり、確率と解釈できるということですね。<br>
さっきのソフトマックス関数の出力値$y_k$を用いて交差エントロピー関数は</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
E({\bf w}) = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k ({\bf x}_n, {\bf w})
</pre></div></div>

<p>と表現されます。</p>

<p>Chainerのコードで言うと、<br>
<a href="https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py</a><br>
にある、</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="k">def</span> <span class="nf">forward_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">()</span><span class="o">.</span><span class="n">forward_cpu</span><span class="p">((</span><span class="n">x</span><span class="p">,))</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">t</span><span class="p">)),</span> <span class="n">t</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">/</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span><span class="p">,</span>
</pre></div></div>

<p>に相当します。</p>

<p>また、<code>F.accuracy(y, t)</code>は出力と、教師データを照合して正答率を返しています。</p>

<h2>
<span id="33-optimizerの設定" class="fragment"></span><a href="#33-optimizer%E3%81%AE%E8%A8%AD%E5%AE%9A"><i class="fa fa-link"></i></a>3.3 Optimizerの設定</h2>

<p>さて、モデルが決まったので訓練に移ります。<br>
ここでは最適化手法としてAdamが使われています。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># Setup optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">()</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">collect_parameters</span><span class="p">())</span>
</pre></div></div>

<p>Adamについては<a href="http://ja.scribd.com/doc/260859670/30minutes-Adam" rel="nofollow noopener" target="_blank">30分でわかるAdam</a>で<a href="https://twitter.com/echizen_tm" rel="nofollow noopener" target="_blank">echizen_tm</a>さんが解説してくれています。</p>

<h1>
<span id="4訓練の実施と結果" class="fragment"></span><a href="#4%E8%A8%93%E7%B7%B4%E3%81%AE%E5%AE%9F%E6%96%BD%E3%81%A8%E7%B5%90%E6%9E%9C"><i class="fa fa-link"></i></a>4.訓練の実施と結果</h1>

<p>以上の準備から、ミニバッチ学習で手書き数字の判別を実施し、その精度を見ていきます。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_acc</span>  <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_acc</span>  <span class="o">=</span> <span class="p">[]</span>

<span class="n">l1_W</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">l2_W</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">l3_W</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c"># Learning loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">print</span> <span class="s">'epoch'</span><span class="p">,</span> <span class="n">epoch</span>

    <span class="c"># training</span>
    <span class="c"># N個の順番をランダムに並び替える</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">sum_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sum_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c"># 0〜Nまでのデータをバッチサイズごとに使って学習</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">perm</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batchsize</span><span class="p">]]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">perm</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batchsize</span><span class="p">]]</span>

        <span class="c"># 勾配を初期化</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grads</span><span class="p">()</span>
        <span class="c"># 順伝播させて誤差と精度を算出</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>
        <span class="c"># 誤差逆伝播で勾配を計算</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

        <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">train_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">sum_loss</span>     <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="o">*</span> <span class="n">batchsize</span>
        <span class="n">sum_accuracy</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="o">*</span> <span class="n">batchsize</span>

    <span class="c"># 訓練データの誤差と、正解精度を表示</span>
    <span class="k">print</span> <span class="s">'train mean loss={}, accuracy={}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sum_loss</span> <span class="o">/</span> <span class="n">N</span><span class="p">,</span> <span class="n">sum_accuracy</span> <span class="o">/</span> <span class="n">N</span><span class="p">)</span>

    <span class="c"># evaluation</span>
    <span class="c"># テストデータで誤差と、正解精度を算出し汎化性能を確認</span>
    <span class="n">sum_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sum_loss</span>     <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N_test</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
        <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batchsize</span><span class="p">]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batchsize</span><span class="p">]</span>

        <span class="c"># 順伝播させて誤差と精度を算出</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">test_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">test_acc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="n">sum_loss</span>     <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="o">*</span> <span class="n">batchsize</span>
        <span class="n">sum_accuracy</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">cuda</span><span class="o">.</span><span class="n">to_cpu</span><span class="p">(</span><span class="n">acc</span><span class="o">.</span><span class="n">data</span><span class="p">))</span> <span class="o">*</span> <span class="n">batchsize</span>

    <span class="c"># テストデータでの誤差と、正解精度を表示</span>
    <span class="k">print</span> <span class="s">'test  mean loss={}, accuracy={}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sum_loss</span> <span class="o">/</span> <span class="n">N_test</span><span class="p">,</span> <span class="n">sum_accuracy</span> <span class="o">/</span> <span class="n">N_test</span><span class="p">)</span>

    <span class="c"># 学習したパラメーターを保存</span>
    <span class="n">l1_W</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="n">l2_W</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l2</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
    <span class="n">l3_W</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l3</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>

<span class="c"># 精度と誤差をグラフ描画</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)),</span> <span class="n">train_acc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)),</span> <span class="n">test_acc</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s">"train_acc"</span><span class="p">,</span><span class="s">"test_acc"</span><span class="p">],</span><span class="n">loc</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Accuracy of digit recognition."</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div></div>

<p>epoch毎のサマリ結果はこちらです。20回しして98.5%くらいの高精度で判別できています。</p>

<div class="code-frame" data-lang="bash">
<div class="code-lang"><span class="bold">output</span></div>
<div class="highlight"><pre>
epoch 1
train mean <span class="nv">loss</span><span class="o">=</span>0.278375425202, <span class="nv">accuracy</span><span class="o">=</span>0.914966667456
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.11533634907, <span class="nv">accuracy</span><span class="o">=</span>0.964300005436
epoch 2
train mean <span class="nv">loss</span><span class="o">=</span>0.137060894324, <span class="nv">accuracy</span><span class="o">=</span>0.958216670454
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0765812527167, <span class="nv">accuracy</span><span class="o">=</span>0.976100009084
epoch 3
train mean <span class="nv">loss</span><span class="o">=</span>0.107826075749, <span class="nv">accuracy</span><span class="o">=</span>0.966816672881
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0749603212342, <span class="nv">accuracy</span><span class="o">=</span>0.97770000577
epoch 4
train mean <span class="nv">loss</span><span class="o">=</span>0.0939164237926, <span class="nv">accuracy</span><span class="o">=</span>0.970616674324
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0672153823725, <span class="nv">accuracy</span><span class="o">=</span>0.980000005364
epoch 5
train mean <span class="nv">loss</span><span class="o">=</span>0.0831089563683, <span class="nv">accuracy</span><span class="o">=</span>0.973950009048
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0705943618687, <span class="nv">accuracy</span><span class="o">=</span>0.980100004673
epoch 6
train mean <span class="nv">loss</span><span class="o">=</span>0.0752325405277, <span class="nv">accuracy</span><span class="o">=</span>0.976883343955
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0732760328815, <span class="nv">accuracy</span><span class="o">=</span>0.977900006771
epoch 7
train mean <span class="nv">loss</span><span class="o">=</span>0.0719517664274, <span class="nv">accuracy</span><span class="o">=</span>0.977383343875
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.063611669606, <span class="nv">accuracy</span><span class="o">=</span>0.981900005937
epoch 8
train mean <span class="nv">loss</span><span class="o">=</span>0.0683009948514, <span class="nv">accuracy</span><span class="o">=</span>0.978566677173
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0604036964733, <span class="nv">accuracy</span><span class="o">=</span>0.981400005221
epoch 9
train mean <span class="nv">loss</span><span class="o">=</span>0.0621755663728, <span class="nv">accuracy</span><span class="o">=</span>0.980550010701
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0591542539285, <span class="nv">accuracy</span><span class="o">=</span>0.982400006652
epoch 10
train mean <span class="nv">loss</span><span class="o">=</span>0.0618313539471, <span class="nv">accuracy</span><span class="o">=</span>0.981183344225
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0693172766063, <span class="nv">accuracy</span><span class="o">=</span>0.982900006175
epoch 11
train mean <span class="nv">loss</span><span class="o">=</span>0.0583098273944, <span class="nv">accuracy</span><span class="o">=</span>0.982000010014
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0668152360269, <span class="nv">accuracy</span><span class="o">=</span>0.981600006819
epoch 12
train mean <span class="nv">loss</span><span class="o">=</span>0.054178619228, <span class="nv">accuracy</span><span class="o">=</span>0.983533344865
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0614466062452, <span class="nv">accuracy</span><span class="o">=</span>0.982900005579
epoch 13
train mean <span class="nv">loss</span><span class="o">=</span>0.0532431817259, <span class="nv">accuracy</span><span class="o">=</span>0.98390001148
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.060112986485, <span class="nv">accuracy</span><span class="o">=</span>0.98400000751
epoch 14
train mean <span class="nv">loss</span><span class="o">=</span>0.0538122716064, <span class="nv">accuracy</span><span class="o">=</span>0.983266676267
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0624165921964, <span class="nv">accuracy</span><span class="o">=</span>0.983300005198
epoch 15
train mean <span class="nv">loss</span><span class="o">=</span>0.0501562882114, <span class="nv">accuracy</span><span class="o">=</span>0.983833344777
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0688113694015, <span class="nv">accuracy</span><span class="o">=</span>0.98310000658
epoch 16
train mean <span class="nv">loss</span><span class="o">=</span>0.0513108611095, <span class="nv">accuracy</span><span class="o">=</span>0.984533343514
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0724038232205, <span class="nv">accuracy</span><span class="o">=</span>0.982200007439
epoch 17
train mean <span class="nv">loss</span><span class="o">=</span>0.0471463404785, <span class="nv">accuracy</span><span class="o">=</span>0.985666677058
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0612579581685, <span class="nv">accuracy</span><span class="o">=</span>0.983600008488
epoch 18
train mean <span class="nv">loss</span><span class="o">=</span>0.0460166006556, <span class="nv">accuracy</span><span class="o">=</span>0.986050010125
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0654888718335, <span class="nv">accuracy</span><span class="o">=</span>0.984400007725
epoch 19
train mean <span class="nv">loss</span><span class="o">=</span>0.0458772557077, <span class="nv">accuracy</span><span class="o">=</span>0.986433342795
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0602016936944, <span class="nv">accuracy</span><span class="o">=</span>0.984400007129
epoch 20
train mean <span class="nv">loss</span><span class="o">=</span>0.046333729005, <span class="nv">accuracy</span><span class="o">=</span>0.986433343093
<span class="nb">test  </span>mean <span class="nv">loss</span><span class="o">=</span>0.0621869922416, <span class="nv">accuracy</span><span class="o">=</span>0.985100006461
</pre></div>
</div>

<p>各バッチ毎の判別精度と、誤差のグラフがこちらです。赤いほうがトレーニングデータ、青いほうがテストデータになります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/fc2a9774-27e7-1155-0891-69ba9a4e7510.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/fc2a9774-27e7-1155-0891-69ba9a4e7510.png" alt="nn_result-compressor.png"></a></p>

<p>以前、<a href="http://qiita.com/kenmatsu4/items/c91f5740808022decaae" id="reference-47922c7300bbac8ae8bc">【機械学習】k-nearest neighbor method(k最近傍法)を自力でpythonで書いて、手書き数字の認識をする</a>という記事で、同様に手書き数字の判別をやっていたのですが、その時の精度が97%くらいだったので、更に少し上がっていることがわかります。</p>

<p>このChinerは全てPythonコードで操作ができるので、非常にPythonistaとしては嬉しいフレームワークになっていると思います。まだ、「ディープ」ラーニングできておらず、ただのフィードフォワードニューラルネットワークなので、近々「ディープ」なやつの記事も書ければと思います。</p>

<h1>
<span id="5答え合わせ" class="fragment"></span><a href="#5%E7%AD%94%E3%81%88%E5%90%88%E3%82%8F%E3%81%9B"><i class="fa fa-link"></i></a>5.答え合わせ</h1>

<p>識別した100個の数字を表示してみます。ランダムに100個抽出したのですが、ほとんど正解です。何回か100個表示を行ってやっと間違っているところを１つ表示できたので、その例を下記に貼っています。なんだか人間の方が試されている気分です（笑）</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/cc9626ad-2314-2c7f-3e67-2f78fdd0ed5d.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/cc9626ad-2314-2c7f-3e67-2f78fdd0ed5d.png" alt="mnist_ans2-compressor.png"></a><br>
（※ 2行3列の4を9と誤識別しています）</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'fivethirtyeight'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">draw_digit3</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">ans</span><span class="p">,</span> <span class="n">recog</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">28</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span>   <span class="c"># convert from vector to 28x28 matrix</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>             <span class="c"># flip vertical</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"ans=</span><span class="si">%d</span><span class="s">, recog=</span><span class="si">%d</span><span class="s">"</span><span class="o">%</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span><span class="n">recog</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>

<span class="n">cnt</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)[:</span><span class="mi">100</span><span class="p">]:</span>

    <span class="n">xxx</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">Variable</span><span class="p">(</span><span class="n">xxx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">784</span><span class="p">)))),</span>  <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">h1</span><span class="p">)),</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">y</span>  <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
    <span class="n">cnt</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">draw_digit3</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">cnt</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span>
</pre></div></div>

<h1>
<span id="6第１層のパラメータwの可視化" class="fragment"></span><a href="#6%E7%AC%AC%EF%BC%91%E5%B1%A4%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BFw%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96"><i class="fa fa-link"></i></a>6.第１層のパラメータwの可視化</h1>

<p>入力層のパラメータ$w^{(1)}$784次元ベクトルを28x28ピクセルとしてマッピングして表示してみました。1000個のうちランダムに100個選んでいます。よくみると"2"とか"5"とか"0"に見えるものもありますね。1層目のパラメーターで特徴抽出ができていそうな雰囲気が伺えます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/02fda82c-851c-b0f4-e257-e287e6ce0741.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/02fda82c-851c-b0f4-e257-e287e6ce0741.png" alt="param_images-compressor.png"></a></p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="k">def</span> <span class="nf">draw_digit2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">28</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span>   <span class="c"># convert from vector to 28x28 matrix</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>             <span class="c"># flip vertical</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">27</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"</span><span class="si">%d</span><span class="s">"</span><span class="o">%</span><span class="n">i</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="mi">1000</span><span class="p">)[:</span><span class="mi">100</span><span class="p">]:</span>
    <span class="n">draw_digit2</span><span class="p">(</span><span class="n">l1_W</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">l1_W</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">cnt</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></div>

<h1>
<span id="7出力層のパラメータwの可視化" class="fragment"></span><a href="#7%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BFw%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96"><i class="fa fa-link"></i></a>7.出力層のパラメータwの可視化</h1>

<p>出力層は1000個のインプットを受けて、10個のアウトプットを行う層ですが、ここも可視化してみました。"0"と書いてあるところが手書き数字を"0"と判別するためのパラメーターです。</p>

<p>1000次元ベクトルなので、0を24個後ろにつけて32x32の画像に落としています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/03dfe1ac-d4b2-883a-cc78-a336e5061cbe.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/03dfe1ac-d4b2-883a-cc78-a336e5061cbe.png" alt="output_layer-compressor.png"></a></p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># レイヤー3</span>
<span class="k">def</span> <span class="nf">draw_digit2</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">data</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">24</span><span class="p">)]</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">size</span><span class="p">,</span><span class="n">size</span><span class="p">)</span>   <span class="c"># convert from vector to 28x28 matrix</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>             <span class="c"># flip vertical</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">size</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolor</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"</span><span class="si">%d</span><span class="s">"</span><span class="o">%</span><span class="n">i</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelbottom</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="s">"off"</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">cnt</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">draw_digit2</span><span class="p">(</span><span class="n">l3_W</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">l3_W</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">cnt</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">cnt</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></div>

<h1>
<span id="8おまけ" class="fragment"></span><a href="#8%E3%81%8A%E3%81%BE%E3%81%91"><i class="fa fa-link"></i></a>8.おまけ</h1>

<p>中間層の素子数を<code>[100, 500, 800, 900, 1000, 1100, 1200, 1500, 2000]</code>にしてそれぞれ判別してみた。結果のグラフが下記です。素子数500以上で概ね98%を達成しており、それ以上の素子数はあんまり変わらないみたいですね。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/c8e2fadf-362b-bb50-237d-875a57a955e0.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/c8e2fadf-362b-bb50-237d-875a57a955e0.png" alt="total_result.png"></a></p>

<h1>
<span id="9おまけ2--活性化関数" class="fragment"></span><a href="#9%E3%81%8A%E3%81%BE%E3%81%912--%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0"><i class="fa fa-link"></i></a>9.おまけ2 : 活性化関数</h1>

<p>Chainerにプリインされている主な活性化関数に</p>

<ul>
<li>ReLu function</li>
<li>tanh function</li>
<li>sigmoid function</li>
</ul>

<p>があります。図示すると下記のような形です。<br>
素子の入力と出力の間に入る関数で、入出力に関する閾値を設定するような役割を持ちます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/50670/ced9f1d5-5ec2-6086-a142-9833614e3ee6.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/50670/ced9f1d5-5ec2-6086-a142-9833614e3ee6.png" alt="activation_func.png"></a></p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="c"># 活性化関数テスト</span>
<span class="n">x_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x_data</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">311</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"ReLu function."</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">312</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"tanh function."</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">313</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"sigmoid function."</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-.</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></div>

<p>次の記事<br>
「<a href="http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8" id="reference-aa79f97045d03b55e63a">【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。</a>」<br>
ディープラーニングで特徴抽出を自動化する技術のAutoencoderを実装してみた記事です。</p>

<p>【参考書籍】<br>
　深層学習（機械学習プロフェッショナルシリーズ） 岡谷貴之<br>
　</p>

<p>【参考webサイト】<br>
　Chainerのメインサイト<br>
　　　<a href="http://chainer.org/" class="autolink" rel="nofollow noopener" target="_blank">http://chainer.org/</a><br>
　ChainerのGitHubリポジトリ<br>
　　　<a href="https://github.com/pfnet/chainer" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/pfnet/chainer</a><br>
　Chainerのチュートリアルとリファレンス<br>
　　　<a href="http://docs.chainer.org/en/latest/" class="autolink" rel="nofollow noopener" target="_blank">http://docs.chainer.org/en/latest/</a><br>
　"Dropout: A Simple Way to Prevent Neural Networks from Overfitting"<br>
　 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov <br>
　　　<a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" class="autolink" rel="nofollow noopener" target="_blank">http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a></p>
<div class="hidden"><form class="js-task-list-update" action="/kenmatsu4/items/7b8d24d4c5144a686412" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="cVt/RfNrHvejGzJg12gw2adkRFPWRs4/tMcf4ygl6ShmPJvyDSc1YdBjtlrqHt1jadaPbexFoCt3etYEJre/bQ==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1443279690" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
今話題のDeep Learning(深層学習)フレームワーク、[Chainer](http://chainer.org/)に手書き文字の判別を行うサンプルコードがあります。こちらを使って内容を少し解説する記事を書いてみたいと思います。

**(本記事のコードの全文を[GitHub](https://github.com/matsuken92/Qiita_Contents/blob/master/chainer-MNIST/chainer-MNIST_forPubs.ipynb)にアップしました。[PC推奨])**

とにかく、インストールがすごく簡単かつ、Pythonが書ければすぐに使うことができておすすめです！
Pythonに閉じてコードが書けるのもすごくいいですよね。

こんな感じのニューラルネットワークモデルを試してみる、という記事です。
![nn_structure6.png](https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png)


主要な情報はこちらにあります。
[Chainerのメインサイト](http://chainer.org/)
[ChainerのGitHubリポジトリ](https://github.com/pfnet/chainer)
[Chainerのチュートリアルとリファレンス](http://docs.chainer.org/en/latest/)


#1. インストール#

まずは何はともあれインストールです。ChainerのGitHubに記載の&quot;Requirements&quot; ( https://github.com/pfnet/chainer#requirements )を参考に必要なソフト、ライブラリをインストールした上で

```
pip install chainer
```
を実行します。
これだけでインストールできちゃいます。超簡単！CaffeをMacにインストールしようとした時はかなり苦戦しましたが、嘘のようです :smile: 

もし、インストールに詰まったらcvl-robotさんの「[DeepLearningライブラリのChainerがすごい、らしい](http://cvl-robot.hateblo.jp/entry/2015/06/11/223928)」という記事が詳しく必要なライブラリ等のインストールについて記載してくれていて便利です。

#2.サンプルコードの入手#
GitHubの下記ディレクトリにおなじみMNISTの手書き文字を判別する、というサンプルがありますので、これを題材としたいと思います。これをChainerの順伝播型ニューラルネットワークでClassificationしてみる、という試みです。
https://github.com/pfnet/chainer/tree/master/examples/mnist
　　┗ train_mnist.py

このコードにコメントを加えたり、一部途中のフローをグラフで表示してイメージをつけたりしながら見ていきたいと思います。

#3.サンプルコードを見ていく#
今回、手持ちのMacbook Air(OS X ver10.10.2)での動作確認をしながら書いていますので、環境によっては差分があるかもしれませんが、そこはよしなに見てもらえればと思っています。また、こういった環境のためGPUでの計算は行わずCPUのみとなりますのでGPU関連のコードは省略して記載します。

##3-1.準備##

まず最初に必要なライブラリ群のインポートです。

```py
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_mldata
from chainer import cuda, Variable, FunctionSet, optimizers
import chainer.functions  as F
import sys

plt.style.use(&#39;ggplot&#39;)
```

次に各種パラメーターの定義・設定を行います。

```py
# 確率的勾配降下法で学習させる際の１回分のバッチサイズ
batchsize = 100

# 学習の繰り返し回数
n_epoch   = 20

# 中間層の数
n_units   = 1000
```

Scikit LearnをつかってMNISTの手書き数字データをダウンロードします。

```py
# MNISTの手書き数字データのダウンロード
# #HOME/scikit_learn_data/mldata/mnist-original.mat にキャッシュされる
print &#39;fetch MNIST dataset&#39;
mnist = fetch_mldata(&#39;MNIST original&#39;)
# mnist.data : 70,000件の784次元ベクトルデータ
mnist.data   = mnist.data.astype(np.float32)
mnist.data  /= 255     # 0-1のデータに変換

# mnist.target : 正解データ（教師データ）
mnist.target = mnist.target.astype(np.int32)
```

3つくらい取り出して描画してみます。

```py
# 手書き数字データを描画する関数
def draw_digit(data):
    size = 28
    plt.figure(figsize=(2.5, 3))

    X, Y = np.meshgrid(range(size),range(size))
    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix
    Z = Z[::-1,:]             # flip vertical
    plt.xlim(0,27)
    plt.ylim(0,27)
    plt.pcolor(X, Y, Z)
    plt.gray()
    plt.tick_params(labelbottom=&quot;off&quot;)
    plt.tick_params(labelleft=&quot;off&quot;)

    plt.show()

draw_digit(mnist.data[5])
draw_digit(mnist.data[12345])
draw_digit(mnist.data[33456])
```

28x28, 784次元ベクトルのこんなデータですね。

![digits-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/0e36e752-3fea-716a-c331-2bf8a15b47de.png)


データセットを学習用データ検証用データに分割します。

```py
# 学習用データを N個、検証用データを残りの個数と設定
N = 60000
x_train, x_test = np.split(mnist.data,   [N])
y_train, y_test = np.split(mnist.target, [N])
N_test = y_test.size
```

##3.2 モデルの定義##

いよいよモデルの定義です。ここからが本番ですね。Chainerのクラスや関数を使います。

```py
# Prepare multi-layer perceptron model
# 多層パーセプトロンモデルの設定
# 入力 784次元、出力 10次元
model = FunctionSet(l1=F.Linear(784, n_units),
                    l2=F.Linear(n_units, n_units),
                    l3=F.Linear(n_units, 10))
```

入力の手書き数字のデータが784次元ベクトルなので、入力素子は784個になります。今回中間層はn_unitsで1000と指定しています。出力は、数字を識別するので10個になります。下記がこのモデルのイメージです。


![nn_structure6.png](https://qiita-image-store.s3.amazonaws.com/0/50670/155b9533-4b47-0748-226c-1e3082930ed9.png)

順伝播の構造が下記のforward()関数で定義されます。

```py
# Neural net architecture
# ニューラルネットの構造
def forward(x_data, y_data, train=True):
    x, t = Variable(x_data), Variable(y_data)
    h1 = F.dropout(F.relu(model.l1(x)),  train=train)
    h2 = F.dropout(F.relu(model.l2(h1)), train=train)
    y  = model.l3(h2)
    # 多クラス分類なので誤差関数としてソフトマックス関数の
    # 交差エントロピー関数を用いて、誤差を導出
    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)
```

ここで各関数等を説明したいと思います。
Chainerのお作法で、データは配列からChainerのVariableという型（クラス）のオブジェクトに変換して使います。

```py
x, t = Variable(x_data), Variable(y_data)
```

活性化関数はシグモイド関数ではなく、F.relu()関数が使われています。

```py
F.relu(model.l1(x))
```
このF.relu()は正規化線形関数(Rectified Linear Unit function)で

```math
f(x) = \max(0, x)
```
つまり

![relu-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/e1cc4c94-e4ae-0010-82e5-c27956b5986c.png)

こんな感じです。
描画コードはこちら。

```py
# F.reluテスト
x_data = np.linspace(-10, 10, 100, dtype=np.float32)
x = Variable(x_data)
y = F.relu(x)

plt.figure(figsize=(7,5))
plt.ylim(-2,10)
plt.plot(x.data, y.data)
plt.show()
```

シンプルな関数ですね。このため、計算量が小さく学習スピードが速くなることが利点のようです。

次に、このrelu()関数の出力を入力としてF.dropout()関数が使われています。

```py
F.dropout(F.relu(model.l1(x)),  train=train)
```

このドロップアウト関数F.dropout()は[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)という論文で提唱されている手法で、ランダムに中間層をドロップ（ないものとする）し、そうすると過学習を防ぐことができるそうです。

ちょっと動かしてみましょう。

```py
# dropout(x, ratio=0.5, train=True) テスト
# x: 入力値
# ratio: 0を出力する確率
# train: Falseの場合はxをそのまま返却する
# return: ratioの確率で0を、1−ratioの確率で,x*(1/(1-ratio))の値を返す

n = 50
v_sum = 0
for i in range(n):
    x_data = np.array([1,2,3,4,5,6], dtype=np.float32)
    x = Variable(x_data)
    dr = F.dropout(x, ratio=0.6,train=True)
    
    for j in range(6):
        sys.stdout.write( str(dr.data[j]) + &#39;, &#39; )
    print(&quot;&quot;)
    v_sum += dr.data

# outputの平均がx_dataとだいたい一致する 
sys.stdout.write( str((v_sum/float(n))) )
```

```bash:output
2.5, 5.0, 7.5, 0.0, 0.0, 0.0, 
2.5, 5.0, 7.5, 10.0, 0.0, 15.0, 
0.0, 5.0, 7.5, 10.0, 12.5, 15.0, 
　　　　　　・・・
0.0, 0.0, 7.5, 10.0, 0.0, 0.0, 
2.5, 0.0, 7.5, 10.0, 0.0, 15.0, 
[ 0.94999999  2.29999995  3.          3.5999999   7.25        5.69999981]
```

[1,2,3,4,5,6]という配列をF.dropout()関数に渡します。いま、ratioはドロップアウト率であり、ratio=0.6を設定しているので、60%の確率でドロップアウトされ、0が出力されます。40%の確率で値が返されるのですが、その際、値を返す確率が40%に減ってしまっているので、それを補うために${1 \over 0.4}$倍=2.5倍された値が出力されます。つまり

```math
(0 \times 0.6 + 2.5 \times 0.4) = 1
```
で、平均すると元の数字になるようになっています。上記の例だと最後の行が出力の平均ですが、50回繰り返して大体元の[1,2,3,4,5,6]に近い値になっています。

同じ構造がもう１層あり、出力され出力値が$y$となります。

```py
    h2 = F.dropout(F.relu(model.l2(h1)), train=train)
    y  = model.l3(h2)
```

最後の出力ですが、ソフトマックス関数と交差エントロピー関数を用いて誤差の出力。それとF.accuracy()関数で精度を返しています。

```py
    # 多クラス分類なので誤差関数としてソフトマックス関数の
    # 交差エントロピー関数を用いて、誤差を導出
    return F.softmax_cross_entropy(y, t), F.accuracy(y, t)
```

ソフトマックス関数ですが、

```math
y_k = z_k = f_{k}({\bf u})={\exp(u_{k}) \over \sum_j^K \exp(u_{j})}
```
のように定義される関数で、この関数を挟むことで$y_1, \cdots ,y_{10}$の10個の出力の総和が1となり、出力を確率として解釈することが可能になります。
なぜ$\exp()$関数が使われているかというと、値がマイナスにならないように、ということと自分は理解しています。

おなじみ$\exp()$関数は
![exp-compressor (1).png](https://qiita-image-store.s3.amazonaws.com/0/50670/50ff3c5d-cab5-65c7-f19a-48a383e0f957.png)
のような形なので、マイナスの値を取りません。これにより値がマイナスにならず、かつ総和が１ということになり、確率と解釈できるということですね。
さっきのソフトマックス関数の出力値$y_k$を用いて交差エントロピー関数は

```math
E({\bf w}) = -\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \log y_k ({\bf x}_n, {\bf w})
```
と表現されます。

Chainerのコードで言うと、
https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py
にある、

```py
def forward_cpu(self, inputs):
        x, t = inputs
        self.y, = Softmax().forward_cpu((x,))
        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,
```
に相当します。

また、`F.accuracy(y, t)`は出力と、教師データを照合して正答率を返しています。

##3.3 Optimizerの設定##

さて、モデルが決まったので訓練に移ります。
ここでは最適化手法としてAdamが使われています。

```py
# Setup optimizer
optimizer = optimizers.Adam()
optimizer.setup(model.collect_parameters())
```
Adamについては[30分でわかるAdam](http://ja.scribd.com/doc/260859670/30minutes-Adam)で[echizen_tm](https://twitter.com/echizen_tm)さんが解説してくれています。

#4.訓練の実施と結果#

以上の準備から、ミニバッチ学習で手書き数字の判別を実施し、その精度を見ていきます。


```py
train_loss = []
train_acc  = []
test_loss = []
test_acc  = []

l1_W = []
l2_W = []
l3_W = []

# Learning loop
for epoch in xrange(1, n_epoch+1):
    print &#39;epoch&#39;, epoch

    # training
    # N個の順番をランダムに並び替える
    perm = np.random.permutation(N)
    sum_accuracy = 0
    sum_loss = 0
    # 0〜Nまでのデータをバッチサイズごとに使って学習
    for i in xrange(0, N, batchsize):
        x_batch = x_train[perm[i:i+batchsize]]
        y_batch = y_train[perm[i:i+batchsize]]

        # 勾配を初期化
        optimizer.zero_grads()
        # 順伝播させて誤差と精度を算出
        loss, acc = forward(x_batch, y_batch)
        # 誤差逆伝播で勾配を計算
        loss.backward()
        optimizer.update()

        train_loss.append(loss.data)
        train_acc.append(acc.data)
        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize
        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize

    # 訓練データの誤差と、正解精度を表示
    print &#39;train mean loss={}, accuracy={}&#39;.format(sum_loss / N, sum_accuracy / N)

    # evaluation
    # テストデータで誤差と、正解精度を算出し汎化性能を確認
    sum_accuracy = 0
    sum_loss     = 0
    for i in xrange(0, N_test, batchsize):
        x_batch = x_test[i:i+batchsize]
        y_batch = y_test[i:i+batchsize]

        # 順伝播させて誤差と精度を算出
        loss, acc = forward(x_batch, y_batch, train=False)

        test_loss.append(loss.data)
        test_acc.append(acc.data)
        sum_loss     += float(cuda.to_cpu(loss.data)) * batchsize
        sum_accuracy += float(cuda.to_cpu(acc.data)) * batchsize

    # テストデータでの誤差と、正解精度を表示
    print &#39;test  mean loss={}, accuracy={}&#39;.format(sum_loss / N_test, sum_accuracy / N_test)

    # 学習したパラメーターを保存
    l1_W.append(model.l1.W)
    l2_W.append(model.l2.W)
    l3_W.append(model.l3.W)

# 精度と誤差をグラフ描画
plt.figure(figsize=(8,6))
plt.plot(range(len(train_acc)), train_acc)
plt.plot(range(len(test_acc)), test_acc)
plt.legend([&quot;train_acc&quot;,&quot;test_acc&quot;],loc=4)
plt.title(&quot;Accuracy of digit recognition.&quot;)
plt.plot()
```

epoch毎のサマリ結果はこちらです。20回しして98.5%くらいの高精度で判別できています。


```bash:output
epoch 1
train mean loss=0.278375425202, accuracy=0.914966667456
test  mean loss=0.11533634907, accuracy=0.964300005436
epoch 2
train mean loss=0.137060894324, accuracy=0.958216670454
test  mean loss=0.0765812527167, accuracy=0.976100009084
epoch 3
train mean loss=0.107826075749, accuracy=0.966816672881
test  mean loss=0.0749603212342, accuracy=0.97770000577
epoch 4
train mean loss=0.0939164237926, accuracy=0.970616674324
test  mean loss=0.0672153823725, accuracy=0.980000005364
epoch 5
train mean loss=0.0831089563683, accuracy=0.973950009048
test  mean loss=0.0705943618687, accuracy=0.980100004673
epoch 6
train mean loss=0.0752325405277, accuracy=0.976883343955
test  mean loss=0.0732760328815, accuracy=0.977900006771
epoch 7
train mean loss=0.0719517664274, accuracy=0.977383343875
test  mean loss=0.063611669606, accuracy=0.981900005937
epoch 8
train mean loss=0.0683009948514, accuracy=0.978566677173
test  mean loss=0.0604036964733, accuracy=0.981400005221
epoch 9
train mean loss=0.0621755663728, accuracy=0.980550010701
test  mean loss=0.0591542539285, accuracy=0.982400006652
epoch 10
train mean loss=0.0618313539471, accuracy=0.981183344225
test  mean loss=0.0693172766063, accuracy=0.982900006175
epoch 11
train mean loss=0.0583098273944, accuracy=0.982000010014
test  mean loss=0.0668152360269, accuracy=0.981600006819
epoch 12
train mean loss=0.054178619228, accuracy=0.983533344865
test  mean loss=0.0614466062452, accuracy=0.982900005579
epoch 13
train mean loss=0.0532431817259, accuracy=0.98390001148
test  mean loss=0.060112986485, accuracy=0.98400000751
epoch 14
train mean loss=0.0538122716064, accuracy=0.983266676267
test  mean loss=0.0624165921964, accuracy=0.983300005198
epoch 15
train mean loss=0.0501562882114, accuracy=0.983833344777
test  mean loss=0.0688113694015, accuracy=0.98310000658
epoch 16
train mean loss=0.0513108611095, accuracy=0.984533343514
test  mean loss=0.0724038232205, accuracy=0.982200007439
epoch 17
train mean loss=0.0471463404785, accuracy=0.985666677058
test  mean loss=0.0612579581685, accuracy=0.983600008488
epoch 18
train mean loss=0.0460166006556, accuracy=0.986050010125
test  mean loss=0.0654888718335, accuracy=0.984400007725
epoch 19
train mean loss=0.0458772557077, accuracy=0.986433342795
test  mean loss=0.0602016936944, accuracy=0.984400007129
epoch 20
train mean loss=0.046333729005, accuracy=0.986433343093
test  mean loss=0.0621869922416, accuracy=0.985100006461
```

各バッチ毎の判別精度と、誤差のグラフがこちらです。赤いほうがトレーニングデータ、青いほうがテストデータになります。

![nn_result-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/fc2a9774-27e7-1155-0891-69ba9a4e7510.png)

以前、[【機械学習】k-nearest neighbor method(k最近傍法)を自力でpythonで書いて、手書き数字の認識をする](http://qiita.com/kenmatsu4/items/c91f5740808022decaae)という記事で、同様に手書き数字の判別をやっていたのですが、その時の精度が97%くらいだったので、更に少し上がっていることがわかります。

このChinerは全てPythonコードで操作ができるので、非常にPythonistaとしては嬉しいフレームワークになっていると思います。まだ、「ディープ」ラーニングできておらず、ただのフィードフォワードニューラルネットワークなので、近々「ディープ」なやつの記事も書ければと思います。

#5.答え合わせ#

識別した100個の数字を表示してみます。ランダムに100個抽出したのですが、ほとんど正解です。何回か100個表示を行ってやっと間違っているところを１つ表示できたので、その例を下記に貼っています。なんだか人間の方が試されている気分です（笑）

![mnist_ans2-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/cc9626ad-2314-2c7f-3e67-2f78fdd0ed5d.png)
（※ 2行3列の4を9と誤識別しています）

```py 
plt.style.use(&#39;fivethirtyeight&#39;)
def draw_digit3(data, n, ans, recog):
    size = 28
    plt.subplot(10, 10, n)
    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix
    Z = Z[::-1,:]             # flip vertical
    plt.xlim(0,27)
    plt.ylim(0,27)
    plt.pcolor(Z)
    plt.title(&quot;ans=%d, recog=%d&quot;%(ans,recog), size=8)
    plt.gray()
    plt.tick_params(labelbottom=&quot;off&quot;)
    plt.tick_params(labelleft=&quot;off&quot;)
    

plt.figure(figsize=(15,15))

cnt = 0
for idx in np.random.permutation(N)[:100]:
    
    xxx = x_train[idx].astype(np.float32)
    h1 = F.dropout(F.relu(model.l1(Variable(xxx.reshape(1,784)))),  train=False)
    h2 = F.dropout(F.relu(model.l2(h1)), train=False)
    y  = model.l3(h2)
    cnt+=1
    draw_digit3(x_train[idx], cnt, y_train[idx], np.argmax(y.data))
plt.show
```


#6.第１層のパラメータwの可視化#

入力層のパラメータ$w^{(1)}$784次元ベクトルを28x28ピクセルとしてマッピングして表示してみました。1000個のうちランダムに100個選んでいます。よくみると&quot;2&quot;とか&quot;5&quot;とか&quot;0&quot;に見えるものもありますね。1層目のパラメーターで特徴抽出ができていそうな雰囲気が伺えます。

![param_images-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/02fda82c-851c-b0f4-e257-e287e6ce0741.png)

```py 
def draw_digit2(data, n, i):
    size = 28
    plt.subplot(10, 10, n)
    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix
    Z = Z[::-1,:]             # flip vertical
    plt.xlim(0,27)
    plt.ylim(0,27)
    plt.pcolor(Z)
    plt.title(&quot;%d&quot;%i, size=9)
    plt.gray()
    plt.tick_params(labelbottom=&quot;off&quot;)
    plt.tick_params(labelleft=&quot;off&quot;)

plt.figure(figsize=(10,10))
cnt = 1
for i in np.random.permutation(1000)[:100]:
    draw_digit2(l1_W[len(l1_W)-1][i], cnt, i)
    cnt += 1
    
plt.show()
```

#7.出力層のパラメータwの可視化#

出力層は1000個のインプットを受けて、10個のアウトプットを行う層ですが、ここも可視化してみました。&quot;0&quot;と書いてあるところが手書き数字を&quot;0&quot;と判別するためのパラメーターです。

1000次元ベクトルなので、0を24個後ろにつけて32x32の画像に落としています。

![output_layer-compressor.png](https://qiita-image-store.s3.amazonaws.com/0/50670/03dfe1ac-d4b2-883a-cc78-a336e5061cbe.png)

```py 
# レイヤー3
def draw_digit2(data, n, i):
    size = 32
    plt.subplot(4, 4, n)
    data = np.r_[data,np.zeros(24)]
    Z = data.reshape(size,size)   # convert from vector to 28x28 matrix
    Z = Z[::-1,:]             # flip vertical
    plt.xlim(0,size-1)
    plt.ylim(0,size-1)
    plt.pcolor(Z)
    plt.title(&quot;%d&quot;%i, size=9)
    plt.gray()
    plt.tick_params(labelbottom=&quot;off&quot;)
    plt.tick_params(labelleft=&quot;off&quot;)

plt.figure(figsize=(10,10))
cnt = 1
for i in range(10):
    draw_digit2(l3_W[len(l3_W)-1][i], cnt, i)
    cnt += 1
    
plt.show()
```

#8.おまけ#
中間層の素子数を`[100, 500, 800, 900, 1000, 1100, 1200, 1500, 2000]`にしてそれぞれ判別してみた。結果のグラフが下記です。素子数500以上で概ね98%を達成しており、それ以上の素子数はあんまり変わらないみたいですね。

![total_result.png](https://qiita-image-store.s3.amazonaws.com/0/50670/c8e2fadf-362b-bb50-237d-875a57a955e0.png)


#9.おまけ2 : 活性化関数#

Chainerにプリインされている主な活性化関数に

* ReLu function
* tanh function
* sigmoid function

があります。図示すると下記のような形です。
素子の入力と出力の間に入る関数で、入出力に関する閾値を設定するような役割を持ちます。

![activation_func.png](https://qiita-image-store.s3.amazonaws.com/0/50670/ced9f1d5-5ec2-6086-a142-9833614e3ee6.png)

```py 
# 活性化関数テスト
x_data = np.linspace(-10, 10, 100, dtype=np.float32)
x = Variable(x_data)

y = F.relu(x)
plt.figure(figsize=(8,15))
plt.subplot(311)
plt.title(&quot;ReLu function.&quot;)
plt.ylim(-2,10)
plt.xlim(-6,6)
plt.plot(x.data, y.data)

y = F.tanh(x)
plt.subplot(312)
plt.title(&quot;tanh function.&quot;)
plt.ylim(-1.5,1.5)
plt.xlim(-6,6)
plt.plot(x.data, y.data)

y = F.sigmoid(x)
plt.subplot(313)
plt.title(&quot;sigmoid function.&quot;)
plt.ylim(-.2,1.2)
plt.xlim(-6,6)
plt.plot(x.data, y.data)
plt.show()
```

次の記事
「[【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。](http://qiita.com/kenmatsu4/items/99d4a54d5a57405ecaf8)」
ディープラーニングで特徴抽出を自動化する技術のAutoencoderを実装してみた記事です。

【参考書籍】
　深層学習（機械学習プロフェッショナルシリーズ） 岡谷貴之
　

【参考webサイト】
　Chainerのメインサイト
　　　http://chainer.org/
　ChainerのGitHubリポジトリ
　　　https://github.com/pfnet/chainer
　Chainerのチュートリアルとリファレンス
　　　http://docs.chainer.org/en/latest/
　&quot;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&quot;
　 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov 
　　　http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。 by @Kenmatsu4 on @Qiita" data-url="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。" href="http://b.hatena.ne.jp/entry/http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/kenmatsu4"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/kenmatsu4">kenmatsu4</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">8840</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;kenmatsu4&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-f29a6b1d-726d-4750-8988-5ad666afdc5f"></div>
    <div id="UserFollowButton-react-component-f29a6b1d-726d-4750-8988-5ad666afdc5f"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">Popular Posts</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kenmatsu4/items/7b8d24d4c5144a686412">【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kenmatsu4/items/e6c6acb289c02609e619">【統計学】初めての「標準偏差」（統計学に挫折しないために）</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kenmatsu4/items/2a8573e3c878fc2da306">【数学】固有値・固有ベクトルとは何かを可視化してみる</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kenmatsu4/items/b28d1b3b3d291d0cc698">【統計学】尤度って何？をグラフィカルに説明してみる。</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kenmatsu4/items/23768cbe32fe381d54a2">スタバのTwitterデータをpythonで大量に取得し、データ分析を試みる その１</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#1-%E3%82%A4%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BC%E3%83%AB\&quot;\u003e1. インストール\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#2%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%AE%E5%85%A5%E6%89%8B\&quot;\u003e2.サンプルコードの入手\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#3%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E8%A6%8B%E3%81%A6%E3%81%84%E3%81%8F\&quot;\u003e3.サンプルコードを見ていく\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#3-1%E6%BA%96%E5%82%99\&quot;\u003e3-1.準備\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#32-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%AE%9A%E7%BE%A9\&quot;\u003e3.2 モデルの定義\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#33-optimizer%E3%81%AE%E8%A8%AD%E5%AE%9A\&quot;\u003e3.3 Optimizerの設定\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#4%E8%A8%93%E7%B7%B4%E3%81%AE%E5%AE%9F%E6%96%BD%E3%81%A8%E7%B5%90%E6%9E%9C\&quot;\u003e4.訓練の実施と結果\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#5%E7%AD%94%E3%81%88%E5%90%88%E3%82%8F%E3%81%9B\&quot;\u003e5.答え合わせ\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#6%E7%AC%AC%EF%BC%91%E5%B1%A4%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BFw%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96\&quot;\u003e6.第１層のパラメータwの可視化\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#7%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BFw%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96\&quot;\u003e7.出力層のパラメータwの可視化\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#8%E3%81%8A%E3%81%BE%E3%81%91\&quot;\u003e8.おまけ\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#9%E3%81%8A%E3%81%BE%E3%81%912--%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0\&quot;\u003e9.おまけ2 : 活性化関数\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-07439a0e-1118-45c9-9d56-eb13e0be0128"></div>
    <div id="Toc-react-component-07439a0e-1118-45c9-9d56-eb13e0be0128"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:1585,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="HirofumiYashima"><a itemprop="url" href="/HirofumiYashima"><img alt="HirofumiYashima" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="yacchin1205"><a itemprop="url" href="/yacchin1205"><img alt="yacchin1205" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/28770/profile-images/1473685246" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="_kuni88"><a itemprop="url" href="/_kuni88"><img alt="_kuni88" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/53788/profile-images/1473693161" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="toshi19890331"><a itemprop="url" href="/toshi19890331"><img alt="toshi19890331" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/25513/profile-images/1473684349" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="K-1"><a itemprop="url" href="/K-1"><img alt="K-1" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/37387/profile-images/1473687407" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="sinsnk"><a itemprop="url" href="/sinsnk"><img alt="sinsnk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/82692/profile-images/1473702534" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="user19"><a itemprop="url" href="/user19"><img alt="user19" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/69767/profile-images/1473698259" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="shirayu"><a itemprop="url" href="/shirayu"><img alt="shirayu" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/50725/profile-images/1475554726" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Reds"><a itemprop="url" href="/Reds"><img alt="Reds" class="thumb thumb--xs" src="https://0.gravatar.com/avatar/e751888e30cdc35253c1a3b241977adf?d=https%3A%2F%2Fidenticons.github.com%2F0e2f15414d97b8b642022be91ff105e0.png" /></a></div></div><div class="ArticleFooter__user"><a href="/kenmatsu4/items/7b8d24d4c5144a686412/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/7b8d24d4c5144a686412/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/kenmatsu4/items/7b8d24d4c5144a686412.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><a class="references_toggleOldReferences js-toggleOldReferences" href="#"><i class="fa fa-expand js-toggleOldReferencesIcon"></i><span class="js-toggleOldReferencesText">Show old 6 links</span></a><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/kenmatsu4/items/99d4a54d5a57405ecaf8#_reference-11658a87c00006139190"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184" />【ディープラーニング】ChainerでAutoencoderを試して結果を可視化してみる。</a><time class="references_datetime js-dateTimeView" datetime="2015-07-05T00:40:33+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/mokemokechicken/items/0c515a60a447d341d1d9#_reference-0d05f7fad1fcfc713b33"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/11124/profile-images/1473681925" />Chainerで機械学習と戯れる： ２進法を学習できるか？</a><time class="references_datetime js-dateTimeView" datetime="2015-07-11T06:58:37+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/bbr_bbq/items/1864577f8280bca15ac1#_reference-77d56e885abcc80df98d"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/76953/profile-images/1473700666" />機械学習×Webアプリ診断：多層パーセプトロンでCAPTCHAを認識する（Chainer編）</a><time class="references_datetime js-dateTimeView" datetime="2015-07-15T23:04:43+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/kenmatsu4/items/623514c61166e34283bb#_reference-1d88dc8788bb8679558a"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184" />【Qiita API】[統計学•機械学習] 今までの投稿記事のまとめと分析やってみた。</a><time class="references_datetime js-dateTimeView" datetime="2015-08-10T14:36:39+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/shinji1105/items/3c54feb6dd5e7ea63a62#_reference-afcfbdfe8f178fcfa675"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/72107/profile-images/1473699047" />【機械学習超入門】chainerでサンプルコードを動かすまで</a><time class="references_datetime js-dateTimeView" datetime="2015-08-18T12:43:53+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/ichiroex/items/e0486a6dea1f14c2cfc2#_reference-961110444b7a8b210e1e"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/92685/profile-images/1473705706" />深層学習フレームワークChainerの勉強に役立つページのまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-04-05T15:03:21+00:00">12 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/kenmatsu4/items/183020c058feac6a779b#_reference-737583890bc8ccd2dd97"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184" />今までの投稿記事のまとめ（統計学/機械学習/数学 etc）</a><time class="references_datetime js-dateTimeView" datetime="2016-07-09T02:34:14+00:00">8 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/7of9/items/ae3138ceb21bc8e0ce46#_reference-82e2999a8cd7c8942f74"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996" />TensorFlow &gt; link &gt; ADAM optimizer &gt; echizen_tmさんの解説</a><time class="references_datetime js-dateTimeView" datetime="2016-10-09T07:01:37+00:00">5 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/yoshizaki_kkgk/items/bfe559d1bdd434be03ed#_reference-c6e98e2c95f495a612ba"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/130181/profile-images/1484809889" />Deep Learning 入門（２） - Chainerで自作の非線形回帰を試そう -</a><time class="references_datetime js-dateTimeView" datetime="2016-11-08T09:39:55+00:00">4 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/HirofumiYashima/items/a4641393b9064d414cca#_reference-ea1635b75fdb6854eb10"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" />【 Python 】Chaier  基本ニューラル・ネットワーク コード書き方 備忘録 （１）irisデータ 多値分類（編）</a><time class="references_datetime js-dateTimeView" datetime="2016-11-21T22:58:39+00:00">4 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/t_nakayama0714/items/776724410b2a119af088#_reference-b4b59bc3550a68b9e905"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/45253/profile-images/1473690211" />Qiitaレジェンド達の偉大さをシェル芸で眺めて2016年を振り返る</a><time class="references_datetime js-dateTimeView" datetime="2016-12-28T14:21:49+00:00">3 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。 by @Kenmatsu4 on @Qiita" data-url="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。" href="http://b.hatena.ne.jp/entry/http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;text\&quot;\u003e\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\nmodel = FunctionSet(l1=F.Linear(784, n_units),\n                    l2=F.Linear(n_units, n_units),\n                    l3=F.Linear(n_units, 10))\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eについてで質問です。\u003cbr\u003e\nl1,l2,l3ごとに重みとバイアスをパラメータとして持っているので、モデルのイメージ図の層が1層少ない気がするのですがどうなんでしょう\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-06-19T22:57:19+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:267455,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;```\nmodel = FunctionSet(l1=F.Linear(784, n_units),\n                    l2=F.Linear(n_units, n_units),\n                    l3=F.Linear(n_units, 10))\n```\nについてで質問です。\nl1,l2,l3ごとに重みとバイアスをパラメータとして持っているので、モデルのイメージ図の層が1層少ない気がするのですがどうなんでしょう\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-3e8d84fc0ee5c4560d30&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2013-04-07T14:37:23+09:00&quot;,&quot;id&quot;:19435,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://secure.gravatar.com/avatar/7bd7ebb064f8cfb99b1d02da327669be?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;moudameda&quot;},&quot;uuid&quot;:&quot;3e8d84fc0ee5c4560d30&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/moudameda\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;moudameda\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;moudameda\&quot;\u003e@moudameda\u003c/a\u003e さん\u003cbr\u003e\nご指摘ありがとうございます！\u003cbr\u003e\n図を書き間違えておりました・・・\u003cbr\u003e\n修正しましたので見ていただければと思います。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-06-20T03:14:57+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:267514,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;@moudameda さん\nご指摘ありがとうございます！\n図を書き間違えておりました・・・\n修正しましたので見ていただければと思います。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-6024ca0cd86cd4d587cc&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;6024ca0cd86cd4d587cc&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eRectify は正規化じゃなくて整流ですね。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-06-26T06:20:48+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:272513,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Rectify は正規化じゃなくて整流ですね。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-808c32a6952bd45185aa&quot;,&quot;user&quot;:{&quot;contribution&quot;:117,&quot;created_at&quot;:&quot;2012-06-15T17:35:19+09:00&quot;,&quot;id&quot;:6637,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/6637/profile-images/1473683049&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;mrkn&quot;},&quot;uuid&quot;:&quot;808c32a6952bd45185aa&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/mrkn\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;mrkn\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;mrkn\&quot;\u003e@mrkn\u003c/a\u003e さん\u003cbr\u003e\nコメントありがとうございます。\u003cbr\u003e\n参考書籍に上げさせていただいた「深層学習」では日本語訳がついていたので\u003cbr\u003e\nそれを使っていたのですが、ググってみるとあんまり一般的でないのですね。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-06-26T10:09:09+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:272616,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@mrkn さん\nコメントありがとうございます。\n参考書籍に上げさせていただいた「深層学習」では日本語訳がついていたので\nそれを使っていたのですが、ググってみるとあんまり一般的でないのですね。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-1cce85e84a4b49f9467a&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;1cce85e84a4b49f9467a&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003efor i in np.random.permutation(1000)[:100]:\u003cbr\u003e\n    draw_digit2(l1_W[9][i], cnt, i)\u003cbr\u003e\n    cnt += 1\u003c/p\u003e\n\n\u003cp\u003eパラメータWの可視化でl1_Wの定義がされていないと思うのですが、詳細お願いします。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-01T08:40:05+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:324549,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;for i in np.random.permutation(1000)[:100]:\n    draw_digit2(l1_W[9][i], cnt, i)\n    cnt += 1\n\nパラメータWの可視化でl1_Wの定義がされていないと思うのですが、詳細お願いします。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-21f3f9cb9b26cdd81df9&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-09-01T08:38:35+09:00&quot;,&quot;id&quot;:91776,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;atto617&quot;},&quot;uuid&quot;:&quot;21f3f9cb9b26cdd81df9&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/atto617\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;atto617\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;atto617\&quot;\u003e@atto617\u003c/a\u003e さん\u003cbr\u003e\nご指摘ありがとうございます！\u003cbr\u003e\n本文に反映漏れていましたので先ほど追加しました。\u003cbr\u003e\nご確認のほどよろしくお願いいたします。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-01T10:20:15+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:324651,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@atto617 さん\nご指摘ありがとうございます！\n本文に反映漏れていましたので先ほど追加しました。\nご確認のほどよろしくお願いいたします。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-e61fe84f14fdfe8ff1c8&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;e61fe84f14fdfe8ff1c8&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e追加ありがとうございます！\u003cbr\u003e\n差し支えなければ、今回扱ったフルのソースを見てみたいのですが、、\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-01T10:34:47+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:324679,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;追加ありがとうございます！\n差し支えなければ、今回扱ったフルのソースを見てみたいのですが、、\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-79f6eb9c863ec69e28f8&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-09-01T08:38:35+09:00&quot;,&quot;id&quot;:91776,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;atto617&quot;},&quot;uuid&quot;:&quot;79f6eb9c863ec69e28f8&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/atto617\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;atto617\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;atto617\&quot;\u003e@atto617\u003c/a\u003e さん\u003cbr\u003e\nコードをアップしていると思っていましたが、できていなかったのですね・・・。\u003cbr\u003e\n近日中に整理してアップしますので少々お待ちください！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-01T12:04:46+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:325051,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@atto617 さん\nコードをアップしていると思っていましたが、できていなかったのですね・・・。\n近日中に整理してアップしますので少々お待ちください！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-4114ca42d56da4c95080&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;4114ca42d56da4c95080&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eよろしくお願いいたします！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-01T13:35:27+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:325347,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;よろしくお願いいたします！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-81d600ae7419ba40a4bf&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-09-01T08:38:35+09:00&quot;,&quot;id&quot;:91776,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;atto617&quot;},&quot;uuid&quot;:&quot;81d600ae7419ba40a4bf&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eもう一つ質問させてください。\u003cbr\u003e\nl1_W[9][i]　\u003cbr\u003e\nこれは、入力の9個目のユニットから中間層l2の各ユニットiに対する重要度を表しているという認識でよろしいでしょうか？\u003c/p\u003e\n\n\u003cp\u003eその場合、中間層を可視化ではなく、数値として重要度を測ることは可能でしょうか？　\u003cbr\u003e\n宜しければ、お答えいただきたいです。　よろしくお願いいたします。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-02T11:35:10+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:326164,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;もう一つ質問させてください。\nl1_W[9][i]　\nこれは、入力の9個目のユニットから中間層l2の各ユニットiに対する重要度を表しているという認識でよろしいでしょうか？\n\nその場合、中間層を可視化ではなく、数値として重要度を測ることは可能でしょうか？　\n宜しければ、お答えいただきたいです。　よろしくお願いいたします。\n\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-a16ce6a46fe5b81334f3&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-09-01T08:38:35+09:00&quot;,&quot;id&quot;:91776,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;atto617&quot;},&quot;uuid&quot;:&quot;a16ce6a46fe5b81334f3&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/atto617\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;atto617\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;atto617\&quot;\u003e@atto617\u003c/a\u003e さん\u003cbr\u003e\n遅くなりましたが、コードをGitHubにアップしました。記事の冒頭にリンクがありますのでご確認ください。\u003c/p\u003e\n\n\u003cp\u003eまた、質問いただいている件ですが、9はepoch数です。10回まわしていたときのコードになっていたので\u003cbr\u003e\nこの記事はepoch数20にしていたので本当は19が正しかったです。\u003cbr\u003e\nわかりやすいように下記のように配列の最後の値を示すように少し変えました。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode\u003el1_W[len(l1_W)-1][i]\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eよろしくお願いします。\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eその場合、中間層を可視化ではなく、数値として重要度を測ることは可能でしょうか？　\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e重要度という意味では可視化している各点、$w_{ji}$が重要度と言えると思いますが、いかがでしょうか？\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-09-26T16:26:45+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:343280,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@atto617 さん\n遅くなりましたが、コードをGitHubにアップしました。記事の冒頭にリンクがありますのでご確認ください。\n\nまた、質問いただいている件ですが、9はepoch数です。10回まわしていたときのコードになっていたので\nこの記事はepoch数20にしていたので本当は19が正しかったです。\nわかりやすいように下記のように配列の最後の値を示すように少し変えました。\n\n`l1_W[len(l1_W)-1][i]`\n\nよろしくお願いします。\n\n\u003e その場合、中間層を可視化ではなく、数値として重要度を測ることは可能でしょうか？　\n\n重要度という意味では可視化している各点、$w_{ji}$が重要度と言えると思いますが、いかがでしょうか？\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-b6e7f2f35c4e7a0b7df7&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;b6e7f2f35c4e7a0b7df7&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e遅くなってしまいすみません。ご回答ありがとうございます。\u003cbr\u003e\nアップありがとうございます。助かります。\u003cbr\u003e\nやはりWこそが重要度と言えますよね、ありがとうございます。\u003c/p\u003e\n\n\u003cp\u003e重みWが表示できるように、# evaluationの時に、中間層のノードがどうなるのかを確認したいのですが、可能でしょうか？\u003cbr\u003e\nトレーニングが完了したモデルに、１つのデータを入力した時の中間層の動きを見たいのです。\u003cbr\u003e\nなにかアドバイスを頂けるとありがたいです。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-10-19T03:14:50+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:360331,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;遅くなってしまいすみません。ご回答ありがとうございます。\nアップありがとうございます。助かります。\nやはりWこそが重要度と言えますよね、ありがとうございます。\n\n重みWが表示できるように、# evaluationの時に、中間層のノードがどうなるのかを確認したいのですが、可能でしょうか？\nトレーニングが完了したモデルに、１つのデータを入力した時の中間層の動きを見たいのです。\nなにかアドバイスを頂けるとありがたいです。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-6207f198a3d847dc284d&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-09-01T08:38:35+09:00&quot;,&quot;id&quot;:91776,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;atto617&quot;},&quot;uuid&quot;:&quot;6207f198a3d847dc284d&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eわかりやすい説明ありがとうございます。\u003cbr\u003e\n質問なのですが、正解データと生成データの正答率を求めていますが、プログラムのどの部分でファインチューニングを行っているのでしょうか？\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-11-04T13:31:43+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:374363,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;わかりやすい説明ありがとうございます。\n質問なのですが、正解データと生成データの正答率を求めていますが、プログラムのどの部分でファインチューニングを行っているのでしょうか？\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-1d30d986f8a45f0a0c20&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-11-03T16:44:07+09:00&quot;,&quot;id&quot;:99042,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/99042/profile-images/1473707681&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;sonnnakotoittatte&quot;},&quot;uuid&quot;:&quot;1d30d986f8a45f0a0c20&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/sonnnakotoittatte\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;sonnnakotoittatte\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;sonnnakotoittatte\&quot;\u003e@sonnnakotoittatte\u003c/a\u003e さん\u003cbr\u003e\n記事をご覧いただきありがとうございます。ちなみに、ここで言う「ファインチューニング」\u003cbr\u003e\nとは、どのようなチューニングのことを指していますでしょうか？ここでのMNISTの例では\u003cbr\u003e\n事前学習は行っていませんでしたので。。。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-11-05T02:08:16+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:375033,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;@sonnnakotoittatte さん\n記事をご覧いただきありがとうございます。ちなみに、ここで言う「ファインチューニング」\nとは、どのようなチューニングのことを指していますでしょうか？ここでのMNISTの例では\n事前学習は行っていませんでしたので。。。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-3b48d75725900a9e61c1&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;3b48d75725900a9e61c1&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/atto617\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;atto617\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;atto617\&quot;\u003e@atto617\u003c/a\u003e さん\u003cbr\u003e\nお返事が遅れてすみません、コメントいただいていたことに気づけていませんでしたm(_ _)m\u003cbr\u003e\n「6.第１層のパラメータwの可視化」でやっていることを他の層で\u003cbr\u003e\n行えば中間層が見えると思いますので、そちらでいかがでしょうか？\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-11-05T02:10:10+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:375035,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@atto617 さん\nお返事が遅れてすみません、コメントいただいていたことに気づけていませんでしたm(_ _)m\n「6.第１層のパラメータwの可視化」でやっていることを他の層で\n行えば中間層が見えると思いますので、そちらでいかがでしょうか？\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-3a034ba5f8d91ce712d1&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;3a034ba5f8d91ce712d1&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/kenmatsu4\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;kenmatsu4\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;kenmatsu4\&quot;\u003e@kenmatsu4\u003c/a\u003eさん\u003cbr\u003e\nご返信ありがとうございます。\u003cbr\u003e\nF.softmax_cross_entropy(y, t) のtの中身はただの整数（0~9)だと思うのですが、なぜ0～9に学習できているのかが分からなくて。。\u003cbr\u003e\ntの中に教師データがあってニューラルネットの出力との誤差を求めているのなら理解が出来るのですが、、、。\u003cbr\u003e\n初歩的な質問ですみません。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-11-05T02:49:08+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:375047,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@kenmatsu4さん\nご返信ありがとうございます。\nF.softmax_cross_entropy(y, t) のtの中身はただの整数（0~9)だと思うのですが、なぜ0～9に学習できているのかが分からなくて。。\ntの中に教師データがあってニューラルネットの出力との誤差を求めているのなら理解が出来るのですが、、、。\n初歩的な質問ですみません。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-2a33c0abba48f4f3b6f2&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-11-03T16:44:07+09:00&quot;,&quot;id&quot;:99042,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/99042/profile-images/1473707681&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;sonnnakotoittatte&quot;},&quot;uuid&quot;:&quot;2a33c0abba48f4f3b6f2&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e少々分からないところがあり御教示いただけると助かります。\u003cbr\u003e\n出力層の１から１０個目のノードに０～９の確率が出力されるというところで、\u003cbr\u003e\nなぜ一つ目に０の確率、二つ目に１の確率、、というように出力されるのかが分かりません。\u003cbr\u003e\n例えば一つ目に５の確率、二つ目に９の確率、、というようにならないのは、どこのどんなパラメータが利いてそのようになるのでしょうか？\u003cbr\u003e\n例えば数字ではなくアルファベットで同じようなものをつくろうとするとき、どのように出力されるのでしょうか。\u003cbr\u003e\n（一つ目にＡ、二つ目にＢとなる？？）\u003c/p\u003e\n\n\u003cp\u003e初心者のような質問で申し訳ないのですがよろしくお願いいたします。。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-12-21T12:31:46+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:417263,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;少々分からないところがあり御教示いただけると助かります。\n出力層の１から１０個目のノードに０～９の確率が出力されるというところで、\nなぜ一つ目に０の確率、二つ目に１の確率、、というように出力されるのかが分かりません。\n例えば一つ目に５の確率、二つ目に９の確率、、というようにならないのは、どこのどんなパラメータが利いてそのようになるのでしょうか？\n例えば数字ではなくアルファベットで同じようなものをつくろうとするとき、どのように出力されるのでしょうか。\n（一つ目にＡ、二つ目にＢとなる？？）\n\n初心者のような質問で申し訳ないのですがよろしくお願いいたします。。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-48294d49d5ca368e2071&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-12-21T12:27:04+09:00&quot;,&quot;id&quot;:106458,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/106458/profile-images/1473709935&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;love_forza&quot;},&quot;uuid&quot;:&quot;48294d49d5ca368e2071&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/sonnnakotoittatte\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;sonnnakotoittatte\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;sonnnakotoittatte\&quot;\u003e@sonnnakotoittatte\u003c/a\u003e さん、\u003ca href=\&quot;/love_forza\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;love_forza\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;love_forza\&quot;\u003e@love_forza\u003c/a\u003e さん\u003c/p\u003e\n\n\u003cp\u003eお二人の質問はほぼ同じものと思いますので同時に回答させていただきます。\u003cbr\u003e\n（お返事遅れてすみません・・・）\u003c/p\u003e\n\n\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;py\&quot;\u003e\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\n\u003cspan class=\&quot;k\&quot;\u003edef\u003c/span\u003e \u003cspan class=\&quot;nf\&quot;\u003eforward\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003ex_data\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003ey_data\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003etrain\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e\u003cspan class=\&quot;bp\&quot;\u003eTrue\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e):\u003c/span\u003e\n    \u003cspan class=\&quot;n\&quot;\u003ex\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003eVariable\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003ex_data\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e),\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003eVariable\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003ey_data\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e)\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eにあるように、tは教師データ、正解データをあらわしているので、\u003ca href=\&quot;/sonnnakotoittatte\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;sonnnakotoittatte\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;sonnnakotoittatte\&quot;\u003e@sonnnakotoittatte\u003c/a\u003e さんの推測通りの誤差が求められているので学習が成功するのです。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;/love_forza\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;love_forza\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;love_forza\&quot;\u003e@love_forza\u003c/a\u003e さんの質問にお答えすると、上記のように、28x28, 784次元ベクトル画像データ${\\bf x}$と、正解データtがあって、\u003cbr\u003e\n${\\bf x}$が０の画像に対して\u003cbr\u003e\n\u003ccode\u003et=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\u003c/code\u003e\u003cbr\u003e\n${\\bf x}$が１の画像に対して\u003cbr\u003e\n\u003ccode\u003et=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\u003c/code\u003e\u003cbr\u003e\nのように作ってあるからそうなるだけなのです。なので教師データの並び順を変えればそのように学習されることになります。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-12-22T15:38:14+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:418660,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@sonnnakotoittatte さん、@love_forza さん\n\nお二人の質問はほぼ同じものと思いますので同時に回答させていただきます。\n（お返事遅れてすみません・・・）\n\n```py\ndef forward(x_data, y_data, train=True):\n    x, t = Variable(x_data), Variable(y_data)\n```\n\nにあるように、tは教師データ、正解データをあらわしているので、@sonnnakotoittatte さんの推測通りの誤差が求められているので学習が成功するのです。\n\n@love_forza さんの質問にお答えすると、上記のように、28x28, 784次元ベクトル画像データ${\\bf x}$と、正解データtがあって、\n${\\bf x}$が０の画像に対して\n`t=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]`\n${\\bf x}$が１の画像に対して\n`t=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`\nのように作ってあるからそうなるだけなのです。なので教師データの並び順を変えればそのように学習されることになります。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-bfc85cba35e4e596050d&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;bfc85cba35e4e596050d&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e始めまして，とても理解しやすかったのですが，\u003cbr\u003e\n使用データについて少しわからなかったので質問させてください．\u003c/p\u003e\n\n\u003cp\u003e自ら作成した、数字データを学習させたいのですが、教師の取り方がいまいちわかりません。\u003cbr\u003e\n上の回答では[0,0,0,0,0,0,0,0,0,0]のうちのどれかが1になるとありましたが、\u003cbr\u003e\nsoftmax_cross_entropyやaccuracyの入力の際にもそのように入力を行っているのでしょうか。\u003cbr\u003e\nそれとも、出力層のうち、どの層が一番大きな出力となるか、という整数(0~9)を入力とするのでしょうか。\u003cbr\u003e\n初歩的な質問で申し訳ないですが、どうかよろしくお願いいたします。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-01-29T18:58:30+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:446515,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;始めまして，とても理解しやすかったのですが，\n使用データについて少しわからなかったので質問させてください．\n\n自ら作成した、数字データを学習させたいのですが、教師の取り方がいまいちわかりません。\n上の回答では[0,0,0,0,0,0,0,0,0,0]のうちのどれかが1になるとありましたが、\nsoftmax_cross_entropyやaccuracyの入力の際にもそのように入力を行っているのでしょうか。\nそれとも、出力層のうち、どの層が一番大きな出力となるか、という整数(0~9)を入力とするのでしょうか。\n初歩的な質問で申し訳ないですが、どうかよろしくお願いいたします。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-18f0deaf91b941970cef&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2016-01-29T18:22:48+09:00&quot;,&quot;id&quot;:111344,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/111344/profile-images/1473711534&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;tigjavalike&quot;},&quot;uuid&quot;:&quot;18f0deaf91b941970cef&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/tigjavalike\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;tigjavalike\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;tigjavalike\&quot;\u003e@tigjavalike\u003c/a\u003e さん\u003c/p\u003e\n\n\u003cp\u003eコメントありがとうございます（・∀・）\u003c/p\u003e\n\n\u003cp\u003e今回使っているMNISTというデータセットは、手書き数字の画像データと、正解データがセットになった\u003cbr\u003e\nものとなっています。なのでたとえば”２”の画像には対応した教師データにはで2という数字が入っています。\u003c/p\u003e\n\n\u003cp\u003e私の手元にあるchainerの環境が1.3.2で古いもので恐縮ですが、このなかの\u003c/p\u003e\n\n\u003cp\u003echainer/function/にある下記の箇所\u003c/p\u003e\n\n\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;py\&quot;\u003e\n\u003cdiv class=\&quot;code-lang\&quot;\u003e\u003cspan class=\&quot;bold\&quot;\u003esoftmax_cross_entropy.py\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\n    \u003cspan class=\&quot;k\&quot;\u003edef\u003c/span\u003e \u003cspan class=\&quot;nf\&quot;\u003ebackward_cpu\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;bp\&quot;\u003eself\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003einputs\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egrad_outputs\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e):\u003c/span\u003e\n        \u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egloss\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003einputs\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;mi\&quot;\u003e1\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e],\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egrad_outputs\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e]\u003c/span\u003e\n        \u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;bp\&quot;\u003eself\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e.\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003ey\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e.\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003ecopy\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e()\u003c/span\u003e\n        \u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;nb\&quot;\u003exrange\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;nb\&quot;\u003elen\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e)),\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e]\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e-=\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e1\u003c/span\u003e\n        \u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e*=\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egloss\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e]\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e/\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e.\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003esize\u003c/span\u003e\n        \u003cspan class=\&quot;k\&quot;\u003ereturn\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;bp\&quot;\u003eNone\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003e特に\u003ccode\u003egx[xrange(len(t)), t]\u003c/code\u003eの箇所がキモかなと思います。\u003cbr\u003e\nここの動作がどのようになるか、お試しコードを書いてみます。\u003c/p\u003e\n\n\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;py\&quot;\u003e\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\n\u003cspan class=\&quot;kn\&quot;\u003eimport\u003c/span\u003e \u003cspan class=\&quot;nn\&quot;\u003enumpy\u003c/span\u003e \u003cspan class=\&quot;kn\&quot;\u003eas\u003c/span\u003e \u003cspan class=\&quot;nn\&quot;\u003enp\u003c/span\u003e\n\u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003enp\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e.\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003earray\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e([\u003c/span\u003e\u003cspan class=\&quot;mi\&quot;\u003e4\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e1\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e5\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e4\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e])\u003c/span\u003e\n\u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003enp\u003c/span\u003e\u003cspan class=\&quot;o\&quot;\u003e.\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003earray\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e([\u003c/span\u003e\n \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.14517167e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.18063912e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e5.44141121e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.48448044e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e4.44299877e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.82369277e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.48409256e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.02085918e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e6.66329563e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e3.74313742e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,],\u003c/span\u003e\n \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.73833370e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e2.35062107e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.32443100e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e3.10195535e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n   \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e6.91536367e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.54255414e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e8.66706073e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e3.16072673e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e5.01736701e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.50058925e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,],\u003c/span\u003e\n \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e  \u003cspan class=\&quot;mf\&quot;\u003e5.44062495e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e6.17116868e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e7.13338703e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e3.75412285e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e1.23380005e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e4.39898908e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.81496853e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e4.11424935e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e2.51770467e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.22962028e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,],\u003c/span\u003e\n \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e  \u003cspan class=\&quot;mf\&quot;\u003e2.28096262e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e2.53549933e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.43667588e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e3.80125463e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e1.40988261e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e2.05328599e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e7.77791440e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.11141615e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n   \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.66694045e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e7.70356357e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,],\u003c/span\u003e\n \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e4.30959463e-03\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e1.64866388e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e4.46578115e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e4.23524320e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e1.36142075e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e9.26714316e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e8.04089069e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e  \u003cspan class=\&quot;o\&quot;\u003e-\u003c/span\u003e\u003cspan class=\&quot;mf\&quot;\u003e7.60478899e-02\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e\n    \u003cspan class=\&quot;mf\&quot;\u003e1.01261869e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e   \u003cspan class=\&quot;mf\&quot;\u003e1.27016604e-01\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,]])\u003c/span\u003e\n\u003cspan class=\&quot;k\&quot;\u003eprint\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003egx\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;nb\&quot;\u003exrange\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;nb\&quot;\u003elen\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e(\u003c/span\u003e\u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e)),\u003c/span\u003e \u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e]\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e出力結果は下記です。\u003c/p\u003e\n\n\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;\&quot;\u003e\n\u003cdiv class=\&quot;code-lang\&quot;\u003e\u003cspan class=\&quot;bold\&quot;\u003eout\u003c/span\u003e\u003c/div\u003e\n\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\n[ 0.44429988 -0.23506211 -0.43989891  0.14098826 -0.00430959]\n\u003c/pre\u003e\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cp\u003eつまり、誤差を計算する際に、tで指定された箇所の誤差を小さくするような処理になっているのですね。\u003c/p\u003e\n\n\u003cp\u003eなので、明示的に\u003c/p\u003e\n\n\u003cdiv class=\&quot;code-frame\&quot; data-lang=\&quot;py\&quot;\u003e\u003cdiv class=\&quot;highlight\&quot;\u003e\u003cpre\u003e\n\u003cspan class=\&quot;n\&quot;\u003et\u003c/span\u003e \u003cspan class=\&quot;o\&quot;\u003e=\u003c/span\u003e \u003cspan class=\&quot;p\&quot;\u003e[\u003c/span\u003e\u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e1\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e,\u003c/span\u003e \u003cspan class=\&quot;mi\&quot;\u003e0\u003c/span\u003e\u003cspan class=\&quot;p\&quot;\u003e]\u003c/span\u003e\n\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eのようにOne-hot vectorに変換しているわけではないものの、\u003cbr\u003e\nそれと同じような働きをするようなコードになっています。\u003cbr\u003e\nなので、今回は教師データのOne-hot vectorが頭から0, 1, 2 ..., 9の\u003cbr\u003e\nラベルに相当するという前提で計算がされています。\u003cbr\u003e\nもしこの割り当てを変えるようにコードを組めばそのように各ベクトルの要素と\u003cbr\u003e\nそれが指し示す数字のラベルの対応付けを変えることもやれなくはないですね。\u003c/p\u003e\n\n\u003cp\u003eこんな感じで回答になりましたでしょうか。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-01-30T01:24:46+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:446808,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@tigjavalike さん\n\nコメントありがとうございます（・∀・）\n\n今回使っているMNISTというデータセットは、手書き数字の画像データと、正解データがセットになった\nものとなっています。なのでたとえば”２”の画像には対応した教師データにはで2という数字が入っています。\n\n\n\n私の手元にあるchainerの環境が1.3.2で古いもので恐縮ですが、このなかの\n\nchainer/function/にある下記の箇所\n\n```py:softmax_cross_entropy.py\n    def backward_cpu(self, inputs, grad_outputs):\n        t, gloss = inputs[1], grad_outputs[0]\n        gx = self.y.copy()\n        gx[xrange(len(t)), t] -= 1\n        gx *= gloss[0] / t.size\n        return gx, None\n```\n特に`gx[xrange(len(t)), t]`の箇所がキモかなと思います。\nここの動作がどのようになるか、お試しコードを書いてみます。\n\n```py\nimport numpy as np\nt = np.array([4, 1, 5, 4, 0])\ngx = np.array([\n [ -1.14517167e-01,  -1.18063912e-01,   5.44141121e-02,   2.48448044e-01,\n    4.44299877e-01,   1.82369277e-01,   2.48409256e-01,   1.02085918e-01,\n    6.66329563e-02,   3.74313742e-01,],\n [ -1.73833370e-02,  -2.35062107e-01,   1.32443100e-01,   3.10195535e-01,\n   -6.91536367e-02,   2.54255414e-01,   8.66706073e-02,  -3.16072673e-01,\n    5.01736701e-02,  -1.50058925e-01,],\n [  5.44062495e-01,  -6.17116868e-01,  -7.13338703e-02,   3.75412285e-01,\n    1.23380005e-01,  -4.39898908e-01,   2.81496853e-01,   4.11424935e-02,\n    2.51770467e-02,   1.22962028e-01,],\n [  2.28096262e-01,  -2.53549933e-01,   2.43667588e-01,   3.80125463e-01,\n    1.40988261e-01,   2.05328599e-01,  -7.77791440e-02,   1.11141615e-01,\n   -1.66694045e-01,  -7.70356357e-02,],\n [ -4.30959463e-03,  -1.64866388e-01,   4.46578115e-01,   4.23524320e-01,\n    1.36142075e-01,   9.26714316e-02,   8.04089069e-01,  -7.60478899e-02,\n    1.01261869e-01,   1.27016604e-01,]])\nprint gx[xrange(len(t)), t]\n```\n\n出力結果は下記です。\n\n```:out\n[ 0.44429988 -0.23506211 -0.43989891  0.14098826 -0.00430959]\n```\n\nつまり、誤差を計算する際に、tで指定された箇所の誤差を小さくするような処理になっているのですね。\n\nなので、明示的に\n\n```py\nt = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n```\n\nのようにOne-hot vectorに変換しているわけではないものの、\nそれと同じような働きをするようなコードになっています。\nなので、今回は教師データのOne-hot vectorが頭から0, 1, 2 ..., 9の\nラベルに相当するという前提で計算がされています。\nもしこの割り当てを変えるようにコードを組めばそのように各ベクトルの要素と\nそれが指し示す数字のラベルの対応付けを変えることもやれなくはないですね。\n\nこんな感じで回答になりましたでしょうか。\n\n\n\n\n\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-5cc9fbff4ab320858941&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;5cc9fbff4ab320858941&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e初歩的な質問で申し訳ありません。draw_digit3の中で返り値としてnp.argmax(y.data))を使っているのですが、argmaxを使う理由がわかっておりません。y.dataの最大値を取る理由は何故でしょうか。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-01T21:55:06+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:580678,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;初歩的な質問で申し訳ありません。draw_digit3の中で返り値としてnp.argmax(y.data))を使っているのですが、argmaxを使う理由がわかっておりません。y.dataの最大値を取る理由は何故でしょうか。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-eca8dc44004aae0a02c9&quot;,&quot;user&quot;:{&quot;contribution&quot;:223,&quot;created_at&quot;:&quot;2014-12-28T14:51:45+09:00&quot;,&quot;id&quot;:64334,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/64334/profile-images/1473696550&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;miyamotok0105&quot;},&quot;uuid&quot;:&quot;eca8dc44004aae0a02c9&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/miyamotok0105\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;miyamotok0105\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;miyamotok0105\&quot;\u003e@miyamotok0105\u003c/a\u003e さん\u003c/p\u003e\n\n\u003cp\u003e当記事を読んでくださりありがとうございます！\u003cbr\u003e\nnp.argmaxは最大値をとるのとは少し違い、引数のベクトルで最大の値を持っている\u003cbr\u003e\nindexの番号を返します。つまりy.dataがもし\u003cbr\u003e\ny.data = [0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\u003cbr\u003e\nだったとすると\u003cbr\u003e\nnp.argmax(y.data)\u003cbr\u003e\nは \&quot;3\&quot; を返します。\u003c/p\u003e\n\n\u003cp\u003ey.dataはもともと、各要素が数字の0〜9に対応していて、インプットデータががその数字である\u003cbr\u003e\nことのある種の確信度を表していると言えます。(足して1になるとか、各要素が0から1の間の値\u003cbr\u003e\nをとるなど正規化されていれば確率と解釈もできます。そのために学習時はsoftmax関数を入れて\u003cbr\u003e\nその条件が満たされるようにしています)\u003cbr\u003e\n上記の例だと、入力された画像は \&quot;3\&quot; であるとこのモデルは判断しているわけです。\u003cbr\u003e\nこの予測結果を表示するためにdraw_digit3関数に渡しています。\u003c/p\u003e\n\n\u003cp\u003e回答になっていましたでしょうか？\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-02T01:24:41+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:580766,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@miyamotok0105 さん\n\n当記事を読んでくださりありがとうございます！\nnp.argmaxは最大値をとるのとは少し違い、引数のベクトルで最大の値を持っている\nindexの番号を返します。つまりy.dataがもし\ny.data = [0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\nだったとすると\nnp.argmax(y.data)\nは \&quot;3\&quot; を返します。\n\ny.dataはもともと、各要素が数字の0〜9に対応していて、インプットデータががその数字である\nことのある種の確信度を表していると言えます。(足して1になるとか、各要素が0から1の間の値\nをとるなど正規化されていれば確率と解釈もできます。そのために学習時はsoftmax関数を入れて\nその条件が満たされるようにしています)\n上記の例だと、入力された画像は \&quot;3\&quot; であるとこのモデルは判断しているわけです。\nこの予測結果を表示するためにdraw_digit3関数に渡しています。\n\n回答になっていましたでしょうか？\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-baad88faa252bc511bdf&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;baad88faa252bc511bdf&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eすごくわかりやすい回答をありがとうございます。\u003cbr\u003e\n自分の実装で出力層ユニットを2個にしてるにもかかわらず、\u003cbr\u003e\n実際に出力数をみると１０個とか増えて出力されているのですが\u003cbr\u003e\n定義したユニット数が変わってしまうようなことって今までにありましたでしょうか。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-02T08:16:14+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:580801,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;すごくわかりやすい回答をありがとうございます。\n自分の実装で出力層ユニットを2個にしてるにもかかわらず、\n実際に出力数をみると１０個とか増えて出力されているのですが\n定義したユニット数が変わってしまうようなことって今までにありましたでしょうか。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-3cb7241ad6a17ec96f7c&quot;,&quot;user&quot;:{&quot;contribution&quot;:223,&quot;created_at&quot;:&quot;2014-12-28T14:51:45+09:00&quot;,&quot;id&quot;:64334,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/64334/profile-images/1473696550&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;miyamotok0105&quot;},&quot;uuid&quot;:&quot;3cb7241ad6a17ec96f7c&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/miyamotok0105\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;miyamotok0105\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;miyamotok0105\&quot;\u003e@miyamotok0105\u003c/a\u003e さん\u003c/p\u003e\n\n\u003cp\u003eよかったです！\u003c/p\u003e\n\n\u003cp\u003e出力の次元が変わってしまうということに遭遇したことはないのですが、\u003cbr\u003e\nどこかにそのコードが上がっていたりしますか？ちょっと見てみないと\u003cbr\u003e\nなんとも回答できなさそうです・・・\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-02T10:25:48+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:580820,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@miyamotok0105 さん\n\nよかったです！\n\n出力の次元が変わってしまうということに遭遇したことはないのですが、\nどこかにそのコードが上がっていたりしますか？ちょっと見てみないと\nなんとも回答できなさそうです・・・\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-2afc970a3071db076a97&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;2afc970a3071db076a97&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eすいません。ミニバッチの内容が出ているだけでした。お騒がせいたしました。\u003cbr\u003e\nご親切にご回答ありがとうございました。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-02T23:38:27+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:580983,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;すいません。ミニバッチの内容が出ているだけでした。お騒がせいたしました。\nご親切にご回答ありがとうございました。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-b24ee37ccbe94889ec43&quot;,&quot;user&quot;:{&quot;contribution&quot;:223,&quot;created_at&quot;:&quot;2014-12-28T14:51:45+09:00&quot;,&quot;id&quot;:64334,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/64334/profile-images/1473696550&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;miyamotok0105&quot;},&quot;uuid&quot;:&quot;b24ee37ccbe94889ec43&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e\u003ca href=\&quot;/miyamotok0105\&quot; class=\&quot;user-mention js-hovercard\&quot; title=\&quot;miyamotok0105\&quot; data-hovercard-target-type=\&quot;user\&quot; data-hovercard-target-name=\&quot;miyamotok0105\&quot;\u003e@miyamotok0105\u003c/a\u003e \u003cbr\u003e\n了解です！良かったです(・∀・)\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-04T22:22:21+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:582039,&quot;is_team&quot;:false,&quot;item_id&quot;:310402,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;@miyamotok0105 \n了解です！良かったです(・∀・)\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412#comment-2e337d74d86b54f687a5&quot;,&quot;user&quot;:{&quot;contribution&quot;:8840,&quot;created_at&quot;:&quot;2014-08-01T17:56:53+09:00&quot;,&quot;id&quot;:50670,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;uuid&quot;:&quot;2e337d74d86b54f687a5&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:310402,&quot;uuid&quot;:&quot;7b8d24d4c5144a686412&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;kenmatsu4&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:50670,&quot;url_name&quot;:&quot;kenmatsu4&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;},{&quot;id&quot;:19435,&quot;url_name&quot;:&quot;moudameda&quot;,&quot;profile_image_url&quot;:&quot;https://secure.gravatar.com/avatar/7bd7ebb064f8cfb99b1d02da327669be?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&quot;},{&quot;id&quot;:6637,&quot;url_name&quot;:&quot;mrkn&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/6637/profile-images/1473683049&quot;},{&quot;id&quot;:91776,&quot;url_name&quot;:&quot;atto617&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91776/profile-images/1473705416&quot;},{&quot;id&quot;:99042,&quot;url_name&quot;:&quot;sonnnakotoittatte&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/99042/profile-images/1473707681&quot;},{&quot;id&quot;:106458,&quot;url_name&quot;:&quot;love_forza&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/106458/profile-images/1473709935&quot;},{&quot;id&quot;:111344,&quot;url_name&quot;:&quot;tigjavalike&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/111344/profile-images/1473711534&quot;},{&quot;id&quot;:64334,&quot;url_name&quot;:&quot;miyamotok0105&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/64334/profile-images/1473696550&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-5b8681f6-a10a-445c-9fb0-3dddb6d33030"></div>
    <div id="CommentListContainer-react-component-5b8681f6-a10a-445c-9fb0-3dddb6d33030"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="k26GV5qi/XHv/LcwtvxKgGfvSIjlGgTqSmyfkqbQFjKECWLgZO7W55yEMwqLiqc6qV2Dtt8Zav6J0VZ1qEJAdw==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kenmatsu4/items/7b8d24d4c5144a686412" /><input type="hidden" name="item_uuid" id="item_uuid" value="7b8d24d4c5144a686412" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412", "id": 310402, "uuid": "7b8d24d4c5144a686412" }</script><script class="js-user" type="application/json">{&quot;id&quot;:50670,&quot;url_name&quot;:&quot;kenmatsu4&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/50670/profile-images/1473692184&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="5kjKMTSjjJgwv4K/yay/Pf5A5p10xwaKElIisrD2v0/xLy6Gyu+nDkPHBoX02lKHMPIto07EaJ7R7+tVvmTpCg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kenmatsu4/items/7b8d24d4c5144a686412" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
          ga('create', 'UA-60123434-1', { name: 'user' });
          ga('user.send', 'pageview');
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>