<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>2016年のディープラーニング論文100選 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="これはFujitsu Advent Calendar 2016の11日目の記事です。
掲載内容は個人の意見・見解であり、富士通グループを代表するものではありません。なお、内容の正確性には注意を払っていますが無保証です。


はじめに

この記事では先月今年発表されたディープラーニング論文（ArXivでの発表時期、発表された国際会議が2016年開催またはジャーナル掲載が2016年のもの）から私が個人的に重要だと思った論文を収集しています。また、2015年末ごろの論文も重要..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="2016年のディープラーニング論文100選 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="これは[Fujitsu Advent Calendar 2016](http://qiita.com/advent-calendar/2016/fujitsu)の11日目の記事です。
掲載内容は個人の意見・見解であり、富士通グループを..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="86xPeSt+54DjolOOZMGTmHg0h8LSv3UzRRhvQiNLhXEGl6tmFbQZWH9MFvokqNyzSRSP4mEUaCXh6gtuTtsKQw==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"sakaiakira","type":"items","id":"9da1edda802c4884865c"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;募集&quot;,&quot;content&quot;:&quot;QiitaやQiita:Teamを良くしたいエンジニア&quot;,&quot;url&quot;:&quot;http://increments.co.jp/jobs/engineers?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-7a1c6b41-5f8d-4fc5-8e65-60938459cb08"></div>
    <div id="HeaderContainer-react-component-7a1c6b41-5f8d-4fc5-8e65-60938459cb08"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0",        "name": "ディープラーニング"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader ArticleMainHeader--adcalItem"><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><div class="adventCalendarRibbon"><span><a class="adventCalendarRibbon_title" href="/advent-calendar/2016/fujitsu">Fujitsu Advent Calendar 2016</a> Day 11</span></div><h1 class="ArticleMainHeader__title" itemprop="headline">2016年のディープラーニング論文100選</h1><ul class="TagList"><li class="TagList__item" data-count="126"><a class="u-link-unstyled TagList__label" href="/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0"><img alt="ディープラーニング" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>ディープラーニング</span></a></li><li class="TagList__item" data-count="229"><a class="u-link-unstyled TagList__label" href="/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92"><img alt="深層学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/9594cfcb56d9180f74c468e56c69ce9f69cbe6ee/medium.jpg?1480640899" /><span>深層学習</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="254"><a class="u-link-unstyled TagList__label" href="/tags/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD"><img alt="人工知能" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/2e7c05efd215b716c8372e4bf0388a3084c98f53/medium.jpg?1433343003" /><span>人工知能</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">570</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="2 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>2</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:570,&quot;uuid&quot;:&quot;9da1edda802c4884865c&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="YasunoriGoto1"><a itemprop="url" href="/YasunoriGoto1"><img alt="YasunoriGoto1" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/101351/profile-images/1473708368" /></a></li><li class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></li><li class="js-hovercard" data-hovercard-target-name="GushiSnow"><a itemprop="url" href="/GushiSnow"><img alt="GushiSnow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/10496/profile-images/1473757289" /></a></li><li class="js-hovercard" data-hovercard-target-name="syoutahirama"><a itemprop="url" href="/syoutahirama"><img alt="syoutahirama" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63717/profile-images/1473696353" /></a></li><li class="js-hovercard" data-hovercard-target-name="satohi_onoue"><a itemprop="url" href="/satohi_onoue"><img alt="satohi_onoue" class="thumb thumb--xs" src="https://secure.gravatar.com/avatar/4c39cf6af5f08ec724746baeab2ee53c" /></a></li><li class="js-hovercard" data-hovercard-target-name="KaYuichi"><a itemprop="url" href="/KaYuichi"><img alt="KaYuichi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/80264/profile-images/1473762283" /></a></li><li class="js-hovercard" data-hovercard-target-name="Implem"><a itemprop="url" href="/Implem"><img alt="Implem" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/78230/profile-images/1477886941" /></a></li><li class="js-hovercard" data-hovercard-target-name="Quasi-quant2010"><a itemprop="url" href="/Quasi-quant2010"><img alt="Quasi-quant2010" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/96204/profile-images/1473706809" /></a></li><li class="js-hovercard" data-hovercard-target-name="KINGDOMKOHE"><a itemprop="url" href="/KINGDOMKOHE"><img alt="KINGDOMKOHE" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64672/profile-images/1473696656" /></a></li><li class="js-hovercard" data-hovercard-target-name="orita"><a itemprop="url" href="/orita"><img alt="orita" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/40623/profile-images/1479274837" /></a></li><li><a href="/sakaiakira/items/9da1edda802c4884865c/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/sakaiakira"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532" alt="1481880532" /></a> <a class="u-link-unstyled" href="/sakaiakira">sakaiakira</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-12-11T17:19:10+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-12-11">Edited at <time datetime="2017-01-15T19:53:08+09:00" itemprop="dateModified">2017-01-15</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/sakaiakira/items/9da1edda802c4884865c/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">32</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/sakaiakira/items/9da1edda802c4884865c/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(32)</span></a></li><li><a href="/sakaiakira/items/9da1edda802c4884865c.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-9da1edda802c4884865c" itemprop="articleBody"><p>これは<a href="http://qiita.com/advent-calendar/2016/fujitsu">Fujitsu Advent Calendar 2016</a>の11日目の記事です。<br>
掲載内容は個人の意見・見解であり、富士通グループを代表するものではありません。なお、内容の正確性には注意を払っていますが無保証です。</p>

<h1>
<span id="はじめに" class="fragment"></span><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><i class="fa fa-link"></i></a>はじめに</h1>

<p>この記事では<del>先月</del>今年発表されたディープラーニング論文（ArXivでの発表時期、発表された国際会議が2016年開催またはジャーナル掲載が2016年のもの）から私が個人的に重要だと思った論文を収集しています。また、2015年末ごろの論文も重要なものは採用しています。</p>

<p>以下の投稿も合わせてご覧ください。<br>
<a href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" id="reference-78a89c1645733c2b58b7">DeepLearning研究 2016年のまとめ</a><br>
<a href="http://qiita.com/aiskoaskosd/items/59c49f2e2a6d76d62798" id="reference-600b7bf5521f8c5dc3b8">2016年の深層学習を用いた画像認識モデル</a> <br>
<a href="http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7" id="reference-64f6ef91f92cb8b4d1ec">foobarNet: ディープラーニング関連の○○Netまとめ</a><br>
<a href="https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/" rel="nofollow noopener" target="_blank">NIPS2016実装集</a></p>

<h1>
<span id="ディープラーニングにとっての2016年" class="fragment"></span><a href="#%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AB%E3%81%A8%E3%81%A3%E3%81%A6%E3%81%AE2016%E5%B9%B4"><i class="fa fa-link"></i></a>ディープラーニングにとっての2016年</h1>

<p>2016年において、ディープラーニング業界は2015年を大きく上回る成果をあげました。<br>
Alphagoが囲碁のプロ棋士に勝ち越した出来事は各界に衝撃を与えましたが、これは2016年におけるディープラーニング革命のほんの始まりに過ぎませんでした。　</p>

<p>2015年に人の認識率を超えた画像認識技術は画像認識にとどまらず、様々な分野に応用され始めています。<br>
例えば、写真にキャプションを付けたり、キャプションから写真を生成したり(!)、ラフ画から詳細な絵を自動生成したり(!!)、近未来を予想したり(!!!)。</p>

<p>一年前は遠い先のように思えた言語処理の分野も実用化が視野に入りつつあります。言語翻訳の分野では一部の印欧言語間において、文レベルの翻訳に関しては人間レベルをほぼ達成しました。会話レベルの音声認識技術も今や人の水準を超えつつあります。</p>

<p>ディープラーニングの基幹技術においても革新的な進歩がありました。<br>
1つないし、少数のデータからの学習を可能にするone-shot学習関連技術やネットワーク規模を約50分の1に大幅に圧縮する技術が登場しました。<br>
かつてはブラックボックスと言われたディープラーニングの数学的背景についても理解が進み始めました。<br>
強化学習技術の進歩も著しく、人間にかなり近いレベルの自律学習能力が機械に備わるのはそう遠い将来では無いでしょう。</p>

<p>今年の初めにはディープラーニング技術者である私自身ですら、これほどの進歩は予想していませんでした。未来予想図が毎日のように書き変わっていく、ここはそういう世界です。そして、人類の未来もまた非線形に変化して行くことでしょう。</p>

<p>未来が全ての皆さまにとって素晴らしいものになるように、私たちディープラーニング技術者はこれまで以上に努力していきます。ぜひ、応援してください。<br>
(あくまで、個人的見解です。)</p>

<p>2017年もまた素晴らしい年になりますように。。。</p>

<h1>
<span id="2016年のディープラーニング論文100選" class="fragment"></span><a href="#2016%E5%B9%B4%E3%81%AE%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E8%AB%96%E6%96%87100%E9%81%B8"><i class="fa fa-link"></i></a>2016年のディープラーニング論文100選</h1>

<h2>
<span id="マイルストーン" class="fragment"></span><a href="#%E3%83%9E%E3%82%A4%E3%83%AB%E3%82%B9%E3%83%88%E3%83%BC%E3%83%B3"><i class="fa fa-link"></i></a>マイルストーン</h2>

<ul>
<li>現役囲碁トップ棋士に勝利<br>
Mastering the game of Go with deep neural networks and tree search [<a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>会話レベルの音声認識で人間と同レベルを達成<br>
Achieving Human Parity in Conversational Speech Recognition [<a href="https://arxiv.org/abs/1610.05256" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>一部の印欧言語間の文レベルでの翻訳でほぼ人間レベルを達成<br>
Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [<a href="https://arxiv.org/abs/1609.08144" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>読唇術で人間レベルを達成<br>
Lip Reading Sentences in the Wild [<a href="https://arxiv.org/abs/1611.05358" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="画像処理" class="fragment"></span><a href="#%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86"><i class="fa fa-link"></i></a>画像処理</h2>

<ul>
<li>エッジ抽出<br>
Unsupervised Learning of Edges [<a href="https://arxiv.org/abs/1511.04166" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>キャプション生成<br>
Rich Image Captioning in the Wild [<a href="https://arxiv.org/abs/1603.09016" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>キャプションから画像生成<br>
Generating Images from Captions with Attention [<a href="https://arxiv.org/abs/1511.02793" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>近未来予測(数秒程度の未来を予想)<br>
Generating Videos with Scene Dynamics [<a href="http://papers.nips.cc/paper/6194-generating-videos-with-scene-dynamics" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Anticipating the future by watching unlabeled video [<a href="https://arxiv.org/abs/1504.08023" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning [<a href="https://arxiv.org/abs/1605.08104" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Deep multi-scale video prediction beyond mean square error [<a href="https://arxiv.org/abs/1511.05440" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>物体追跡<br>
Modeling and Propagating CNNs in a Tree Structure for Visual Tracking [<a href="https://arxiv.org/abs/1608.07242" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Fully-Convolutional Siamese Networks for Object Tracking [<a href="https://arxiv.org/abs/1606.09549" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>画像の着色<br>
Colorful Image Colorization [<a href="https://arxiv.org/abs/1603.08511" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>画風の模倣<br>
Perceptual Losses for Real-Time Style Transfer and Super-Resolution [<a href="https://arxiv.org/abs/1603.08155" rel="nofollow noopener" target="_blank">pdf</a>]<br>
A Learned Representation For Artistic Style [<a href="https://arxiv.org/abs/1610.07629" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>抽象的な原画(ベタ塗りの絵)から具体的な絵を生成<br>
Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks [<a href="https://arxiv.org/abs/1603.01768" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="言語音声処理" class="fragment"></span><a href="#%E8%A8%80%E8%AA%9E%E9%9F%B3%E5%A3%B0%E5%87%A6%E7%90%86"><i class="fa fa-link"></i></a>言語・音声処理</h2>

<ul>
<li>音声認識/音声合成<br>
Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin [<a href="https://arxiv.org/abs/1512.02595" rel="nofollow noopener" target="_blank">pdf</a>]<br>
WaveNet: A Generative Model for Raw Audio [<a href="https://arxiv.org/abs/1609.03499" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>word2vec(単語の分散表現)<br>
Enriching word vectors with subword information [<a href="https://arxiv.org/abs/1607.04606" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>文章分類<br>
Very Deep Convolutional Networks for Natural Language Processing [<a href="https://arxiv.org/abs/1606.01781" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Bag of Tricks for Efficient Text Classification [<a href="https://arxiv.org/abs/1607.01759" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>改良技術<br>
Pointing the Unknown Words [<a href="https://arxiv.org/abs/1603.08148" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>言語翻訳<br>
Fully Character-Level Neural Machine Translation without Explicit Segmentation [<a href="https://arxiv.org/abs/1610.03017" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>詩や物語等の自動生成<br>
Generating Sentences From a Continuous Spaces [<a href="https://arxiv.org/abs/1511.06349" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>楽曲生成<br>
Text-based LSTM networks for Automatic Music Composition [<a href="https://arxiv.org/abs/1604.05358" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="その他応用" class="fragment"></span><a href="#%E3%81%9D%E3%81%AE%E4%BB%96%E5%BF%9C%E7%94%A8"><i class="fa fa-link"></i></a>その他応用</h2>

<ul>
<li>自動プログラミング関連<br>
Neural Programmer-Interpreters...ICLR 2016 - Best Paper Award [<a href="https://arxiv.org/abs/1511.06279" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>数学の定理の自動証明<br>
DeepMath - Deep Sequence Models for Premise Selection [<a href="https://arxiv.org/abs/1606.04442" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>通信の自動暗号化<br>
Learning to Protect Communications with Adversarial Neural Cryptography [<a href="https://arxiv.org/abs/1610.06918" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>太陽フレアの予想<br>
A Deep-Learning Approach for Operation of an Automated Realtime Flare Forecast [<a href="https://arxiv.org/abs/1606.01587" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="cnn技術" class="fragment"></span><a href="#cnn%E6%8A%80%E8%A1%93"><i class="fa fa-link"></i></a>CNN技術</h2>

<ul>
<li>ResNet関連<br>
Identity Mappings in Deep Residual Networks [<a href="https://arxiv.org/abs/1603.05027" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Deep residual learning for image recognition [<a href="https://arxiv.org/abs/1512.03385" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>理論解析<br>
Understanding convolutional neural networks　 [<a href="https://arxiv.org/abs/1609.04112" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Residual Networks Behave Like Ensembles of Relatively Shallow Networks [<a href="https://arxiv.org/abs/1605.06431" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>蒸留技術改良<br>
Multi-Scale Context Aggregation by Dilated Convolutions [<a href="https://arxiv.org/abs/1511.07122" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>高速化/省メモリ技術<br>
Training CNNs with Low-Rank Filters for Efficient Image Classification [<a href="https://arxiv.org/abs/1511.06744" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>RNNの問題を解けるようにCNN技術を改良<br>
Quasi-Recurrent Neural Networks  [<a href="https://arxiv.org/abs/1611.01576" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="rnn技術" class="fragment"></span><a href="#rnn%E6%8A%80%E8%A1%93"><i class="fa fa-link"></i></a>RNN技術</h2>

<ul>
<li>LSTMユニットの拡張<br>
Grid Long Short-Term Memory [<a href="https://arxiv.org/abs/1507.01526" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Associative Long Short-Term Memory [<a href="https://arxiv.org/abs/1602.03032" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Recurrent Highway Networks [<a href="https://arxiv.org/abs/1607.03474" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>attention機構<br>
Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems [<a href="https://arxiv.org/abs/1512.08756" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism [<a href="https://arxiv.org/abs/1601.01073" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>メモリーネットワーク関連<br>
Neural Random-Access Machines [<a href="https://arxiv.org/abs/1511.06392" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Control of Memory, Active Perception, and Action in Minecraft [<a href="https://arxiv.org/abs/1605.09128" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Hierarchical Memory Networks [<a href="https://arxiv.org/abs/1605.07427" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Using Fast Weights to Attend to the Recent Past  [<a href="https://arxiv.org/abs/1610.06258" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ニューラルチューリングマシン関連<br>
Hybrid computing using a neural network with dynamic external memory [<a href="http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes [<a href="https://arxiv.org/abs/1607.00036" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Neural GPUs Learn Algorithms [<a href="https://arxiv.org/abs/1511.08228" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>可視化/理論解析<br>
Architectural Complexity Measures of Recurrent Neural Networks [<a href="https://arxiv.org/abs/1602.08210" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Visualizing and Understanding Recurrent Networks [<a href="https://arxiv.org/abs/1506.02078" rel="nofollow noopener" target="_blank">pdf</a>]</li>
<li>高速化/省メモリ化<br>
Persistent RNNs: Stashing Weights on Chip [<a href="http://jmlr.org/proceedings/papers/v48/diamos16.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Adaptive Computation Time for Recurrent Neural Networks [<a href="https://arxiv.org/abs/1603.08983" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Recurrent Neural Networks With Limited Numerical Precision [<a href="https://arxiv.org/abs/1608.06902" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>強化学習応用<br>
An Actor-Critic Algorithm for Sequence Prediction [<a href="https://arxiv.org/abs/1607.07086" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>学習アルゴリズム<br>
Professor Forcing: A New Algorithm for Training Recurrent Networks [<a href="https://arxiv.org/abs/1610.09038" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>画像処理応用<br>
Pixel Recurrent Neural Networks...ICML 2016 - Best Paper Award [<a href="https://arxiv.org/abs/1601.06759" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>バッチ正規化<br>
Batch normalized recurrent neural networks [<a href="https://arxiv.org/abs/1510.01378" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>LSTM等でない素のRNNの学習手法<br>
Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations [<a href="https://arxiv.org/abs/1605.07154" rel="nofollow noopener" target="_blank">pdf</a>]<br>
GAN技術の適用<br>
SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [<a href="https://arxiv.org/abs/1609.05473" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="強化学習" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>強化学習</h2>

<ul>
<li>基礎技術<br>
Safe and Efficient Off-Policy Reinforcement Learning [<a href="https://arxiv.org/abs/1606.02647" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Learning to Reinforcement Learn [<a href="https://arxiv.org/abs/1611.05763" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Successor Features for Transfer in Reinforcement Learning [<a href="https://arxiv.org/abs/1606.05312" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Model-Free Episodic Control [<a href="https://arxiv.org/abs/1606.04460" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Dueling Network Architecture for Deep Reinforcement Learning...ICML 2016 - Best Paper Award [<a href="https://arxiv.org/abs/1511.06581" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Continuous Deep Q-Learning with Model-based Acceleration [<a href="https://arxiv.org/abs/1603.00748" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Prioritized Experience Replay [<a href="https://arxiv.org/abs/1511.05952" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Continuous control with deep reinforcement learning [<a href="https://arxiv.org/abs/1509.02971" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Increasing the Action Gap: New Operators for Reinforcement Learning [<a href="https://arxiv.org/abs/1512.04860" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Learning to Communicate with Deep Multi-Agent Reinforcement Learning [<a href="https://arxiv.org/abs/1605.06676" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Safely Interruptible Agents [<a href="https://intelligence.org/files/Interruptibility.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Value Iteration Networks...NIPS 2016 - Best Paper Award [<a href="https://arxiv.org/abs/1602.02867" rel="nofollow noopener" target="_blank">pdf</a>] </li>
<li>補助学習<br>
Reinforcement Learning with Unsupervised Auxiliary Tasks [<a href="https://arxiv.org/abs/1611.05397" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>分散学習/マルチエージェント<br>
Asynchronous Methods for Deep Reinforcement Learning [<a href="https://arxiv.org/abs/1602.01783" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Learning to Communicate with Deep Multi-Agent Reinforcement Learning [<a href="https://arxiv.org/abs/1605.06676" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ロボット<br>
Deep Reinforcement Learning for Robotic Manipulation [<a href="https://arxiv.org/abs/1610.00633" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Learning to Perform Physics Experiments via Deep Reinforcement Learning [<a href="https://arxiv.org/abs/1611.01843" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search [<a href="https://arxiv.org/abs/1610.00673" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>言語系強化学習<br>
Deep Reinforcement Learning with a Natural Language Action Space [<a href="https://arxiv.org/abs/1511.04636" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<h2>
<span id="dl技術全般" class="fragment"></span><a href="#dl%E6%8A%80%E8%A1%93%E5%85%A8%E8%88%AC"><i class="fa fa-link"></i></a>DL技術全般</h2>

<ul>
<li>ライブラリ関係<br>
Theano: A Python framework for fast computation of mathematical expressions [<a href="https://arxiv.org/abs/1605.02688" rel="nofollow noopener" target="_blank">pdf</a>]<br>
TensorFlow: Large-scale machine learning on heterogeneous distributed systems [<a href="https://arxiv.org/abs/1603.04467" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ネットワーク簡略化・圧縮<br>
Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding...ICLR 2016 - Best Paper Award [<a href="https://arxiv.org/abs/1510.00149" rel="nofollow noopener" target="_blank">pdf</a>]<br>
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size [<a href="https://arxiv.org/abs/1602.07360" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ネットワーク操作・転送<br>
Network Morphism [<a href="https://arxiv.org/abs/1603.01670" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Net2Net: Accelerating Learning via Knowledge Transfer [<a href="https://arxiv.org/abs/1511.05641" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ネットワーク分割<br>
Decoupled Neural Interfaces using Synthetic Gradients [<a href="https://arxiv.org/abs/1608.05343" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ノイズファンクション<br>
Noisy Activation Functions [<a href="https://arxiv.org/abs/1603.00391" rel="nofollow noopener" target="_blank">pdf</a>]<br>
プレコンディショニング<br>
Feedforward Initialization for Fast Inference of Deep Generative Networks is Biologically Plausible [<a href="https://arxiv.org/abs/1606.01651" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>最適化技術<br>
Learning to Learn by Gradient Descent by Gradient Descent [<a href="https://arxiv.org/abs/1606.04474" rel="nofollow noopener" target="_blank">pdf</a>]<br>
MuProp: Unbiased Backpropagation For Stochastic Neural Networks [<a href="https://arxiv.org/abs/1511.05176" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation [<a href="https://arxiv.org/abs/1602.05179" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Learning values across many orders of magnitude [<a href="https://arxiv.org/abs/1602.07714" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ヘッセ行列の固有値分布解析<br>
Singularity of the Hessian in Deep Learning [<a href="https://arxiv.org/abs/1611.07476" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>可視化手法<br>
Understanding intermediate layers using linear classifier probes [<a href="https://arxiv.org/abs/1610.01644" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>バイナリ重み(±1重み)ネットワーク<br>
Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 [<a href="https://arxiv.org/abs/1602.02830" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>ドロップアウト技術の改良<br>
Dropout Distillation [<a href="http://jmlr.org/proceedings/papers/v48/bulo16.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>正規化関連<br>
Layer Normalization [<a href="https://arxiv.org/abs/1607.06450" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks [<a href="https://arxiv.org/abs/1602.07868" rel="nofollow noopener" target="_blank">pdf</a>]　<br>
</li>
<li>分散学習技術<br>
Revisiting Distributed Synchronous SGD [<a href="https://arxiv.org/abs/1604.00981" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>one-shot学習関連技術<br>
Matching Networks for One Shot Learning [<a href="https://arxiv.org/abs/1606.04080" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Low-shot Visual Recognition by Shrinking and Hallucinating Features [<a href="https://arxiv.org/abs/1606.02819" rel="nofollow noopener" target="_blank">pdf</a>]<br>
One-shot Learning with Memory-Augmented Neural Networks [<a href="https://arxiv.org/abs/1605.06065" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Zero-Shot Learning of Intent Embeddings for Expansion by Convolutional Deep Structured Semantic Models [<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/ICASSP16_ZeroShot.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>学習によって重みが変化することによる忘却の抑止技術<br>
Progressive Neural Networks [<a href="https://arxiv.org/abs/1606.04671" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>極値や鞍点の解析（ある条件下では極小値はすべて大域的最小値、ただし、鞍点の性質は悪い。）<br>
Deep Learning without Poor Local Minima [<a href="https://arxiv.org/pdf/1605.07110v2.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
<li>GAN技術<br>
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks  [<a href="https://arxiv.org/abs/1511.06434" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Connecting Generative Adversarial Networks and Actor-Critic Method [<a href="https://arxiv.org/abs/1610.01945" rel="nofollow noopener" target="_blank">pdf</a>]　　</li>
<li>StackGAN技術<br>
StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks [<a href="https://arxiv.org/pdf/1612.03242v1.pdf" rel="nofollow noopener" target="_blank">pdf</a>]<br>
Stacked Generative Adversarial Networks [<a href="https://arxiv.org/abs/1612.04357" rel="nofollow noopener" target="_blank">pdf</a>]<br>
</li>
</ul>

<p>以上</p>
<div class="hidden"><form class="js-task-list-update" action="/sakaiakira/items/9da1edda802c4884865c" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="HiEVp4bShKM6ddK2ffygC5SSIrQORAi83jHiqoLQ6bnrGvG4uBh6e6abl8I9le8gpbIqlL3vFap6w4aG70Bmiw==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1484477588" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
これは[Fujitsu Advent Calendar 2016](http://qiita.com/advent-calendar/2016/fujitsu)の11日目の記事です。
掲載内容は個人の意見・見解であり、富士通グループを代表するものではありません。なお、内容の正確性には注意を払っていますが無保証です。

# はじめに
この記事では~~先月~~今年発表されたディープラーニング論文（ArXivでの発表時期、発表された国際会議が2016年開催またはジャーナル掲載が2016年のもの）から私が個人的に重要だと思った論文を収集しています。また、2015年末ごろの論文も重要なものは採用しています。

以下の投稿も合わせてご覧ください。
[DeepLearning研究 2016年のまとめ](http://qiita.com/eve_yk/items/f4b274da7042cba1ba76)
[2016年の深層学習を用いた画像認識モデル](http://qiita.com/aiskoaskosd/items/59c49f2e2a6d76d62798) 
[foobarNet: ディープラーニング関連の○○Netまとめ](http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7)
[NIPS2016実装集](https://www.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)
# ディープラーニングにとっての2016年
2016年において、ディープラーニング業界は2015年を大きく上回る成果をあげました。
Alphagoが囲碁のプロ棋士に勝ち越した出来事は各界に衝撃を与えましたが、これは2016年におけるディープラーニング革命のほんの始まりに過ぎませんでした。　

2015年に人の認識率を超えた画像認識技術は画像認識にとどまらず、様々な分野に応用され始めています。
例えば、写真にキャプションを付けたり、キャプションから写真を生成したり(!)、ラフ画から詳細な絵を自動生成したり(!!)、近未来を予想したり(!!!)。

一年前は遠い先のように思えた言語処理の分野も実用化が視野に入りつつあります。言語翻訳の分野では一部の印欧言語間において、文レベルの翻訳に関しては人間レベルをほぼ達成しました。会話レベルの音声認識技術も今や人の水準を超えつつあります。

ディープラーニングの基幹技術においても革新的な進歩がありました。
1つないし、少数のデータからの学習を可能にするone-shot学習関連技術やネットワーク規模を約50分の1に大幅に圧縮する技術が登場しました。
かつてはブラックボックスと言われたディープラーニングの数学的背景についても理解が進み始めました。
強化学習技術の進歩も著しく、人間にかなり近いレベルの自律学習能力が機械に備わるのはそう遠い将来では無いでしょう。

今年の初めにはディープラーニング技術者である私自身ですら、これほどの進歩は予想していませんでした。未来予想図が毎日のように書き変わっていく、ここはそういう世界です。そして、人類の未来もまた非線形に変化して行くことでしょう。

未来が全ての皆さまにとって素晴らしいものになるように、私たちディープラーニング技術者はこれまで以上に努力していきます。ぜひ、応援してください。
(あくまで、個人的見解です。)

2017年もまた素晴らしい年になりますように。。。

# 2016年のディープラーニング論文100選
## マイルストーン
- 現役囲碁トップ棋士に勝利  
Mastering the game of Go with deep neural networks and tree search [[pdf](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)]
- 会話レベルの音声認識で人間と同レベルを達成   
Achieving Human Parity in Conversational Speech Recognition [[pdf](https://arxiv.org/abs/1610.05256)]
- 一部の印欧言語間の文レベルでの翻訳でほぼ人間レベルを達成  
Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation [[pdf](https://arxiv.org/abs/1609.08144)]
- 読唇術で人間レベルを達成  
Lip Reading Sentences in the Wild [[pdf](https://arxiv.org/abs/1611.05358)]  

## 画像処理   
- エッジ抽出   
Unsupervised Learning of Edges [[pdf](https://arxiv.org/abs/1511.04166)]  
- キャプション生成  
Rich Image Captioning in the Wild [[pdf](https://arxiv.org/abs/1603.09016)]
- キャプションから画像生成  
Generating Images from Captions with Attention [[pdf](https://arxiv.org/abs/1511.02793)]
- 近未来予測(数秒程度の未来を予想)  
Generating Videos with Scene Dynamics [[pdf](http://papers.nips.cc/paper/6194-generating-videos-with-scene-dynamics)]  
Anticipating the future by watching unlabeled video [[pdf](https://arxiv.org/abs/1504.08023)]  
Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning [[pdf](https://arxiv.org/abs/1605.08104)]  
Deep multi-scale video prediction beyond mean square error [[pdf](https://arxiv.org/abs/1511.05440)]  
- 物体追跡  
Modeling and Propagating CNNs in a Tree Structure for Visual Tracking [[pdf](https://arxiv.org/abs/1608.07242)]  
 Fully-Convolutional Siamese Networks for Object Tracking [[pdf](https://arxiv.org/abs/1606.09549)]  
- 画像の着色  
Colorful Image Colorization [[pdf](https://arxiv.org/abs/1603.08511)]  
- 画風の模倣  
Perceptual Losses for Real-Time Style Transfer and Super-Resolution [[pdf](https://arxiv.org/abs/1603.08155)]  
A Learned Representation For Artistic Style [[pdf](https://arxiv.org/abs/1610.07629)]  
- 抽象的な原画(ベタ塗りの絵)から具体的な絵を生成  
Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks [[pdf](https://arxiv.org/abs/1603.01768)]  

## 言語・音声処理  
- 音声認識/音声合成  
  Deep Speech 2 : End-to-End Speech Recognition in English and Mandarin [[pdf](https://arxiv.org/abs/1512.02595)]   
  WaveNet: A Generative Model for Raw Audio [[pdf](https://arxiv.org/abs/1609.03499)]
- word2vec(単語の分散表現)  
  Enriching word vectors with subword information [[pdf](https://arxiv.org/abs/1607.04606)]  
- 文章分類  
  Very Deep Convolutional Networks for Natural Language Processing [[pdf](https://arxiv.org/abs/1606.01781)]  
  Bag of Tricks for Efficient Text Classification [[pdf](https://arxiv.org/abs/1607.01759)]   
- 改良技術  
  Pointing the Unknown Words [[pdf](https://arxiv.org/abs/1603.08148)]  
- 言語翻訳  
  Fully Character-Level Neural Machine Translation without Explicit Segmentation [[pdf](https://arxiv.org/abs/1610.03017)]
- 詩や物語等の自動生成  
  Generating Sentences From a Continuous Spaces [[pdf](https://arxiv.org/abs/1511.06349)]  
- 楽曲生成  
  Text-based LSTM networks for Automatic Music Composition [[pdf](https://arxiv.org/abs/1604.05358)]  

## その他応用  
- 自動プログラミング関連  
Neural Programmer-Interpreters...ICLR 2016 - Best Paper Award [[pdf](https://arxiv.org/abs/1511.06279)]   
- 数学の定理の自動証明  
DeepMath - Deep Sequence Models for Premise Selection [[pdf](https://arxiv.org/abs/1606.04442)]  
- 通信の自動暗号化  
Learning to Protect Communications with Adversarial Neural Cryptography [[pdf](https://arxiv.org/abs/1610.06918)] 	
- 太陽フレアの予想  
A Deep-Learning Approach for Operation of an Automated Realtime Flare Forecast [[pdf](https://arxiv.org/abs/1606.01587)]  

## CNN技術  
- ResNet関連     
Identity Mappings in Deep Residual Networks [[pdf](https://arxiv.org/abs/1603.05027)]  
Deep residual learning for image recognition [[pdf](https://arxiv.org/abs/1512.03385)]   
- 理論解析  
Understanding convolutional neural networks　 [[pdf](https://arxiv.org/abs/1609.04112)]  
Residual Networks Behave Like Ensembles of Relatively Shallow Networks [[pdf](https://arxiv.org/abs/1605.06431)]  
- 蒸留技術改良   
Multi-Scale Context Aggregation by Dilated Convolutions [[pdf](https://arxiv.org/abs/1511.07122)]
- 高速化/省メモリ技術  
Training CNNs with Low-Rank Filters for Efficient Image Classification [[pdf](https://arxiv.org/abs/1511.06744)]  
- RNNの問題を解けるようにCNN技術を改良  
Quasi-Recurrent Neural Networks  [[pdf](https://arxiv.org/abs/1611.01576)]  

##  RNN技術  
- LSTMユニットの拡張  
Grid Long Short-Term Memory [[pdf](https://arxiv.org/abs/1507.01526)]  
Associative Long Short-Term Memory [[pdf](https://arxiv.org/abs/1602.03032)]  
Recurrent Highway Networks [[pdf](https://arxiv.org/abs/1607.03474)]  
- attention機構  
Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems [[pdf](https://arxiv.org/abs/1512.08756)]  
Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism [[pdf](https://arxiv.org/abs/1601.01073)]  
- メモリーネットワーク関連     
Neural Random-Access Machines [[pdf](https://arxiv.org/abs/1511.06392)]  
Control of Memory, Active Perception, and Action in Minecraft [[pdf](https://arxiv.org/abs/1605.09128)]    
Hierarchical Memory Networks [[pdf](https://arxiv.org/abs/1605.07427)]   
Using Fast Weights to Attend to the Recent Past  [[pdf](https://arxiv.org/abs/1610.06258)]  
- ニューラルチューリングマシン関連  
Hybrid computing using a neural network with dynamic external memory [[pdf](http://www.nature.com/nature/journal/v538/n7626/full/nature20101.html)]   
Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes [[pdf](https://arxiv.org/abs/1607.00036)]   
Neural GPUs Learn Algorithms [[pdf](https://arxiv.org/abs/1511.08228)]  
- 可視化/理論解析  
Architectural Complexity Measures of Recurrent Neural Networks [[pdf](https://arxiv.org/abs/1602.08210)]  
Visualizing and Understanding Recurrent Networks [[pdf](https://arxiv.org/abs/1506.02078)]
- 高速化/省メモリ化  
Persistent RNNs: Stashing Weights on Chip [[pdf](http://jmlr.org/proceedings/papers/v48/diamos16.pdf)]   
Adaptive Computation Time for Recurrent Neural Networks [[pdf](https://arxiv.org/abs/1603.08983)]   
Recurrent Neural Networks With Limited Numerical Precision [[pdf](https://arxiv.org/abs/1608.06902)]   
- 強化学習応用  
An Actor-Critic Algorithm for Sequence Prediction [[pdf](https://arxiv.org/abs/1607.07086)]   
- 学習アルゴリズム  
Professor Forcing: A New Algorithm for Training Recurrent Networks [[pdf](https://arxiv.org/abs/1610.09038)]   
- 画像処理応用  
Pixel Recurrent Neural Networks...ICML 2016 - Best Paper Award [[pdf](https://arxiv.org/abs/1601.06759)]   
- バッチ正規化  
Batch normalized recurrent neural networks [[pdf](https://arxiv.org/abs/1510.01378)]   
- LSTM等でない素のRNNの学習手法  
Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations [[pdf](https://arxiv.org/abs/1605.07154)]   
GAN技術の適用  
SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient [[pdf](https://arxiv.org/abs/1609.05473)]   

## 強化学習  
- 基礎技術  
Safe and Efficient Off-Policy Reinforcement Learning [[pdf](https://arxiv.org/abs/1606.02647)]   
Learning to Reinforcement Learn [[pdf](https://arxiv.org/abs/1611.05763)]   
Successor Features for Transfer in Reinforcement Learning [[pdf](https://arxiv.org/abs/1606.05312)]   
Model-Free Episodic Control [[pdf](https://arxiv.org/abs/1606.04460)]   
Dueling Network Architecture for Deep Reinforcement Learning...ICML 2016 - Best Paper Award [[pdf](https://arxiv.org/abs/1511.06581)]   
Continuous Deep Q-Learning with Model-based Acceleration [[pdf](https://arxiv.org/abs/1603.00748)]   
Prioritized Experience Replay [[pdf](https://arxiv.org/abs/1511.05952)]   
Continuous control with deep reinforcement learning [[pdf](https://arxiv.org/abs/1509.02971)]   
Increasing the Action Gap: New Operators for Reinforcement Learning [[pdf](https://arxiv.org/abs/1512.04860)]      
Learning to Communicate with Deep Multi-Agent Reinforcement Learning [[pdf](https://arxiv.org/abs/1605.06676)]   
Safely Interruptible Agents [[pdf](https://intelligence.org/files/Interruptibility.pdf)]  
Value Iteration Networks...NIPS 2016 - Best Paper Award [[pdf](https://arxiv.org/abs/1602.02867)] 
- 補助学習  
  Reinforcement Learning with Unsupervised Auxiliary Tasks [[pdf](https://arxiv.org/abs/1611.05397)]   
- 分散学習/マルチエージェント  
  Asynchronous Methods for Deep Reinforcement Learning [[pdf](https://arxiv.org/abs/1602.01783)]   
  Learning to Communicate with Deep Multi-Agent Reinforcement Learning [[pdf](https://arxiv.org/abs/1605.06676)]    
- ロボット  
  Deep Reinforcement Learning for Robotic Manipulation [[pdf](https://arxiv.org/abs/1610.00633)]   
  Learning to Perform Physics Experiments via Deep Reinforcement Learning [[pdf](https://arxiv.org/abs/1611.01843)]     
  Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search [[pdf](https://arxiv.org/abs/1610.00673)]   
- 言語系強化学習  
Deep Reinforcement Learning with a Natural Language Action Space [[pdf](https://arxiv.org/abs/1511.04636)]   

## DL技術全般
- ライブラリ関係  
 Theano: A Python framework for fast computation of mathematical expressions [[pdf](https://arxiv.org/abs/1605.02688)]   
TensorFlow: Large-scale machine learning on heterogeneous distributed systems [[pdf](https://arxiv.org/abs/1603.04467)]   
- ネットワーク簡略化・圧縮   
Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding...ICLR 2016 - Best Paper Award [[pdf](https://arxiv.org/abs/1510.00149)]    
SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 1MB model size [[pdf](https://arxiv.org/abs/1602.07360)]   
- ネットワーク操作・転送    
Network Morphism [[pdf](https://arxiv.org/abs/1603.01670)]   
Net2Net: Accelerating Learning via Knowledge Transfer [[pdf](https://arxiv.org/abs/1511.05641)]   
- ネットワーク分割  
Decoupled Neural Interfaces using Synthetic Gradients [[pdf](https://arxiv.org/abs/1608.05343)]   
- ノイズファンクション  
Noisy Activation Functions [[pdf](https://arxiv.org/abs/1603.00391)]   
プレコンディショニング    
Feedforward Initialization for Fast Inference of Deep Generative Networks is Biologically Plausible [[pdf](https://arxiv.org/abs/1606.01651)]   
- 最適化技術  
Learning to Learn by Gradient Descent by Gradient Descent [[pdf](https://arxiv.org/abs/1606.04474)]   
MuProp: Unbiased Backpropagation For Stochastic Neural Networks [[pdf](https://arxiv.org/abs/1511.05176)]   
Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation [[pdf](https://arxiv.org/abs/1602.05179)]   
Learning values across many orders of magnitude [[pdf](https://arxiv.org/abs/1602.07714)]   
- ヘッセ行列の固有値分布解析  
Singularity of the Hessian in Deep Learning [[pdf](https://arxiv.org/abs/1611.07476)]   
- 可視化手法  
Understanding intermediate layers using linear classifier probes [[pdf](https://arxiv.org/abs/1610.01644)]   
- バイナリ重み(±1重み)ネットワーク  
Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 [[pdf](https://arxiv.org/abs/1602.02830)]   
- ドロップアウト技術の改良  
Dropout Distillation [[pdf](http://jmlr.org/proceedings/papers/v48/bulo16.pdf)]   
- 正規化関連   
Layer Normalization [[pdf](https://arxiv.org/abs/1607.06450)]   
Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks [[pdf](https://arxiv.org/abs/1602.07868)]　  
- 分散学習技術  
Revisiting Distributed Synchronous SGD [[pdf](https://arxiv.org/abs/1604.00981)]  
- one-shot学習関連技術  
Matching Networks for One Shot Learning [[pdf](https://arxiv.org/abs/1606.04080)]   
Low-shot Visual Recognition by Shrinking and Hallucinating Features [[pdf](https://arxiv.org/abs/1606.02819)]   
One-shot Learning with Memory-Augmented Neural Networks [[pdf](https://arxiv.org/abs/1605.06065)]   
Zero-Shot Learning of Intent Embeddings for Expansion by Convolutional Deep Structured Semantic Models [[pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/ICASSP16_ZeroShot.pdf)]       
- 学習によって重みが変化することによる忘却の抑止技術  
Progressive Neural Networks [[pdf](https://arxiv.org/abs/1606.04671)]   
- 極値や鞍点の解析（ある条件下では極小値はすべて大域的最小値、ただし、鞍点の性質は悪い。）          
 Deep Learning without Poor Local Minima [[pdf](https://arxiv.org/pdf/1605.07110v2.pdf)]  
- GAN技術  
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks  [[pdf](https://arxiv.org/abs/1511.06434)]   
Connecting Generative Adversarial Networks and Actor-Critic Method [[pdf](https://arxiv.org/abs/1610.01945)]　　
- StackGAN技術        
StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks [[pdf](https://arxiv.org/pdf/1612.03242v1.pdf)]   
Stacked Generative Adversarial Networks [[pdf](https://arxiv.org/abs/1612.04357)]  

以上
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="2016年のディープラーニング論文100選 on @Qiita" data-url="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="2016年のディープラーニング論文100選" href="http://b.hatena.ne.jp/entry/http://qiita.com/sakaiakira/items/9da1edda802c4884865c" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/sakaiakira"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/sakaiakira">sakaiakira</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">584</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;sakaiakira&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-c440f535-13e6-4acb-907b-3f381409f460"></div>
    <div id="UserFollowButton-react-component-c440f535-13e6-4acb-907b-3f381409f460"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/sakaiakira/items/9da1edda802c4884865c">2016年のディープラーニング論文100選</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\&quot;\u003eはじめに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AB%E3%81%A8%E3%81%A3%E3%81%A6%E3%81%AE2016%E5%B9%B4\&quot;\u003eディープラーニングにとっての2016年\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#2016%E5%B9%B4%E3%81%AE%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E8%AB%96%E6%96%87100%E9%81%B8\&quot;\u003e2016年のディープラーニング論文100選\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%9E%E3%82%A4%E3%83%AB%E3%82%B9%E3%83%88%E3%83%BC%E3%83%B3\&quot;\u003eマイルストーン\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86\&quot;\u003e画像処理\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%80%E8%AA%9E%E9%9F%B3%E5%A3%B0%E5%87%A6%E7%90%86\&quot;\u003e言語・音声処理\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%9D%E3%81%AE%E4%BB%96%E5%BF%9C%E7%94%A8\&quot;\u003eその他応用\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#cnn%E6%8A%80%E8%A1%93\&quot;\u003eCNN技術\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#rnn%E6%8A%80%E8%A1%93\&quot;\u003eRNN技術\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92\&quot;\u003e強化学習\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dl%E6%8A%80%E8%A1%93%E5%85%A8%E8%88%AC\&quot;\u003eDL技術全般\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-d442a718-2d63-458a-aa60-3b6a7b398be1"></div>
    <div id="Toc-react-component-d442a718-2d63-458a-aa60-3b6a7b398be1"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:570,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;9da1edda802c4884865c&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="YasunoriGoto1"><a itemprop="url" href="/YasunoriGoto1"><img alt="YasunoriGoto1" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/101351/profile-images/1473708368" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="GushiSnow"><a itemprop="url" href="/GushiSnow"><img alt="GushiSnow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/10496/profile-images/1473757289" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="syoutahirama"><a itemprop="url" href="/syoutahirama"><img alt="syoutahirama" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63717/profile-images/1473696353" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="satohi_onoue"><a itemprop="url" href="/satohi_onoue"><img alt="satohi_onoue" class="thumb thumb--xs" src="https://secure.gravatar.com/avatar/4c39cf6af5f08ec724746baeab2ee53c" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="KaYuichi"><a itemprop="url" href="/KaYuichi"><img alt="KaYuichi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/80264/profile-images/1473762283" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Implem"><a itemprop="url" href="/Implem"><img alt="Implem" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/78230/profile-images/1477886941" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Quasi-quant2010"><a itemprop="url" href="/Quasi-quant2010"><img alt="Quasi-quant2010" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/96204/profile-images/1473706809" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="KINGDOMKOHE"><a itemprop="url" href="/KINGDOMKOHE"><img alt="KINGDOMKOHE" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64672/profile-images/1473696656" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="orita"><a itemprop="url" href="/orita"><img alt="orita" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/40623/profile-images/1479274837" /></a></div></div><div class="ArticleFooter__user"><a href="/sakaiakira/items/9da1edda802c4884865c/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/9da1edda802c4884865c/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/sakaiakira/items/9da1edda802c4884865c.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><div class="itemsShowBody_adventCalendar"><div class="itemsShowBody_adventCalendar_header"><i class="fa fa-fw fa-calendar"></i> This post is the <span class="date">No.11</span> article of <a class="title" href="/advent-calendar/2016/fujitsu">Fujitsu Advent Calendar 2016</a></div><ul class="itemsShowBody_adventCalendar_nav list-unstyled"><li class="itemsShowBody_adventCalendar_neighborItem itemsShowBody_adventCalendar_neighborItem-prev"><span class="itemsShowBody_adventCalendar_date"><i class="fa fa-fw fa-arrow-circle-left"></i> Day 10:</span><span class="itemsShowBody_adventCalendar_title"><img alt="kut-arika" class="itemsShowBody_adventCalendar_icon" src="https://qiita-image-store.s3.amazonaws.com/0/48738/profile-images/1473691459" width="18" height="18" /> <a class="itemsShowBody_adventCalendar_link" href="/kut-arika/items/81b10312a9b8be86185e">職業プログラマだった私がチームにAnsibleを導入した話</a></span></li><li class="itemsShowBody_adventCalendar_neighborItem itemsShowBody_adventCalendar_neighborItem-next"><span class="itemsShowBody_adventCalendar_date"><i class="fa fa-fw fa-arrow-circle-right"></i> Day 12:</span><span class="itemsShowBody_adventCalendar_title"><img alt="gho4d76g" class="itemsShowBody_adventCalendar_icon" src="https://qiita-image-store.s3.amazonaws.com/0/36235/profile-images/1473760806" width="18" height="18" /> <a class="itemsShowBody_adventCalendar_link" href="/gho4d76g/items/302e1ff91754b9b50f34">asciidoctor-pdfで社内ドキュメントを書こう</a></span></li></ul></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/eve_yk/items/f4b274da7042cba1ba76#_reference-1cc2857fd0e64f59a6cf"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" />DeepLearning研究 2016年のまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-12-16T16:29:55+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7"><img alt="" width="18" height="18" src="https://pbs.twimg.com/profile_images/716138305872486400/CrQCQQo7_normal.png" />foobarNet: ディープラーニング関連の○○Netまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-12-17T14:25:07+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/ryota_m1020/items/79d3f97af27bf8712a64#_reference-17f700d8c5d424846a92"><img alt="" width="18" height="18" src="https://pbs.twimg.com/profile_images/761435671928016896/YGIl1nCE_normal.jpg" />人工知能技術やデータサイエンスを専門にしようと考える人たちへ</a><time class="references_datetime js-dateTimeView" datetime="2016-12-21T10:20:22+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/jiny2001/items/85af7dd163a63b3a152a#_reference-9549ce0a2d1d429599ba"><img alt="" width="18" height="18" src="https://avatars.githubusercontent.com/u/12959344?v=3" />Inside of Deep Learning あるいは深層学習は何を変えるのか</a><time class="references_datetime js-dateTimeView" datetime="2017-01-20T18:26:14+00:00">about 2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="2016年のディープラーニング論文100選 on @Qiita" data-url="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="2016年のディープラーニング論文100選" href="http://b.hatena.ne.jp/entry/http://qiita.com/sakaiakira/items/9da1edda802c4884865c" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e掲載論文のほとんどがarxiv.orgのものなのですね。いくつかはJournal出版されていると記載があるようですが、arxiv.org単体の掲載の場合は注意が必要ですね。\u003c/p\u003e\n\n\u003cp\u003e気づいた注意点:\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eバージョン更新されていき、参照した時点の情報の記録が必要\u003c/li\u003e\n\u003cli\u003epeer reviewがないので、同分野の研究者によるチェック機構がスルーされている\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e以前、Finite Difference Time Domain法の論文をもとにコード作成をしていて、論文記載の数式間違いに気づくのに2週間かかったことがあります。著者に「この数式は間違っていて、論文をもとに実装する人が失敗するので修正して欲しい」と伝えました。回答は「すでに論文出版済みなので無理」でした。「あなたの論文をもとに失敗を多くの人が繰り返すことでコミュニティの活性化が遅れる」など伝えなおしたら、ようやく修正された(Errataが出た)、という経緯を思い出しました。\u003c/p\u003e\n\n\u003cp\u003earxiv.orgの論文(Journalの論文もそうかもしれませんが）には間違いが含まれていて、過信しすぎない方がよいかもしれません。（各アイデアは面白いのでアイデアの種としては参照するのはいいでしょうね）。\u003cbr\u003e\nあとは、バージョンアップ(修正含む)がないかは随時確認した方がいいでしょうね。\u003c/p\u003e\n\n\u003cp\u003e数式に関しては、１つの情報だけでなく、複数の情報を比較して正しいことを確認するのがいいのでしょうね。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-12-16T08:30:49+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:701043,&quot;is_team&quot;:false,&quot;item_id&quot;:448023,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;9da1edda802c4884865c&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;掲載論文のほとんどがarxiv.orgのものなのですね。いくつかはJournal出版されていると記載があるようですが、arxiv.org単体の掲載の場合は注意が必要ですね。\n\n気づいた注意点:\n\n1. バージョン更新されていき、参照した時点の情報の記録が必要\n2. peer reviewがないので、同分野の研究者によるチェック機構がスルーされている\n\n以前、Finite Difference Time Domain法の論文をもとにコード作成をしていて、論文記載の数式間違いに気づくのに2週間かかったことがあります。著者に「この数式は間違っていて、論文をもとに実装する人が失敗するので修正して欲しい」と伝えました。回答は「すでに論文出版済みなので無理」でした。「あなたの論文をもとに失敗を多くの人が繰り返すことでコミュニティの活性化が遅れる」など伝えなおしたら、ようやく修正された(Errataが出た)、という経緯を思い出しました。\n\narxiv.orgの論文(Journalの論文もそうかもしれませんが）には間違いが含まれていて、過信しすぎない方がよいかもしれません。（各アイデアは面白いのでアイデアの種としては参照するのはいいでしょうね）。\nあとは、バージョンアップ(修正含む)がないかは随時確認した方がいいでしょうね。\n\n数式に関しては、１つの情報だけでなく、複数の情報を比較して正しいことを確認するのがいいのでしょうね。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/sakaiakira/items/9da1edda802c4884865c#comment-df5a8983db56805c38db&quot;,&quot;user&quot;:{&quot;contribution&quot;:3305,&quot;created_at&quot;:&quot;2013-11-24T20:17:16+09:00&quot;,&quot;id&quot;:32870,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;7of9&quot;},&quot;uuid&quot;:&quot;df5a8983db56805c38db&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eコメントありがとうございます。\u003cbr\u003e\u003cbr\u003e\narxiv.orgの論文が多いのは、DL業界では、arxivで先行掲載し、そのあとに国際学会に発表のパターンが多いからですね。\u003cbr\u003e\n間違えに関しては、まさにおっしゃる通りだと思います。\u003cbr\u003e\n特にDL業界の論文は雑なことが多いので注意が必要です。\u003cbr\u003e\n有名な論文はソースコードが入手可能なことが多いので、可能な限り、ソースコードとつき合わせて読むべきだと思います。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-12-16T12:29:41+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:701375,&quot;is_team&quot;:false,&quot;item_id&quot;:448023,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;9da1edda802c4884865c&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;コメントありがとうございます。    \narxiv.orgの論文が多いのは、DL業界では、arxivで先行掲載し、そのあとに国際学会に発表のパターンが多いからですね。\n間違えに関しては、まさにおっしゃる通りだと思います。\n特にDL業界の論文は雑なことが多いので注意が必要です。\n有名な論文はソースコードが入手可能なことが多いので、可能な限り、ソースコードとつき合わせて読むべきだと思います。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/sakaiakira/items/9da1edda802c4884865c#comment-d6acd317039d1ae35978&quot;,&quot;user&quot;:{&quot;contribution&quot;:584,&quot;created_at&quot;:&quot;2016-12-02T14:35:02+09:00&quot;,&quot;id&quot;:152471,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;sakaiakira&quot;},&quot;uuid&quot;:&quot;d6acd317039d1ae35978&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:448023,&quot;uuid&quot;:&quot;9da1edda802c4884865c&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;sakaiakira&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:152471,&quot;url_name&quot;:&quot;sakaiakira&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532&quot;},{&quot;id&quot;:32870,&quot;url_name&quot;:&quot;7of9&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-576ee0b5-536d-4496-8543-a01163483c1b"></div>
    <div id="CommentListContainer-react-component-576ee0b5-536d-4496-8543-a01163483c1b"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="fHf7/iUTHZwYt718uVElR6wpCTI/77uXkFx4fEEv6UaJTB/hG9njRIRZ+Aj5OGpsnQkBEoxEpoE0rhxQLL9mdA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/sakaiakira/items/9da1edda802c4884865c" /><input type="hidden" name="item_uuid" id="item_uuid" value="9da1edda802c4884865c" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/sakaiakira/items/9da1edda802c4884865c", "id": 448023, "uuid": "9da1edda802c4884865c" }</script><script class="js-user" type="application/json">{&quot;id&quot;:152471,&quot;url_name&quot;:&quot;sakaiakira&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="u1o2OFMymEAzqwUFkud4oGxjS+saW/c4UANWtlV3HddOYdInbfhmmK9FQHHSjjeLXUNDy6nw6i708TKaOOeS5Q==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/sakaiakira/items/9da1edda802c4884865c" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>