<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>再帰型ニューラルネットワーク: RNN入門 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="再帰型ニューラルネットワーク(RNN)は自然言語処理の分野で高い成果をあげ、現在最も注目されているアルゴリズムの一つです。しかしながら、その人気が先走りして実際にRNNがどのように動くのか、構築するのかを解説する書籍は限られているように思います。この投稿はその部分にフォーカスを当てて友人のDenny（WildMLブログの著者）と一緒に書きました。

さてRNNベースの言語モデルを解説したいと思います。言語モデルの用途は2つあります。1つ目は文章が実際にどのくらいの確率で..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="再帰型ニューラルネットワーク: RNN入門 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="再帰型ニューラルネットワーク(RNN)は自然言語処理の分野で高い成果をあげ、現在最も注目されているアルゴリズムの一つです。しかしながら、その人気が先走りして実際にRNNがどのように動くのか、構築するのかを解説する書籍は限られているよう..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="dWawZz5nG+Qal7bgc3bH08hgr2SCK1kGr/lmLvqhgFCwOE4v0r1pE4u5+bKP8Xjm5xKVk6nHgnI+2A7Q0xLlIA==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"kiminaka","type":"items","id":"87afd4a433dc655d8cfd"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-f7e35523-67cc-4265-9c19-cdfe3aa359c2"></div>
    <div id="HeaderContainer-react-component-f7e35523-67cc-4265-9c19-cdfe3aa359c2"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/Python",        "name": "Python"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">再帰型ニューラルネットワーク: RNN入門</h1><ul class="TagList"><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="126"><a class="u-link-unstyled TagList__label" href="/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0"><img alt="ディープラーニング" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>ディープラーニング</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="254"><a class="u-link-unstyled TagList__label" href="/tags/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD"><img alt="人工知能" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/2e7c05efd215b716c8372e4bf0388a3084c98f53/medium.jpg?1433343003" /><span>人工知能</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">324</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="4 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>4</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:324,&quot;uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></li><li class="js-hovercard" data-hovercard-target-name="antimon2"><a itemprop="url" href="/antimon2"><img alt="antimon2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/30400/profile-images/1473685489" /></a></li><li class="js-hovercard" data-hovercard-target-name="unokun"><a itemprop="url" href="/unokun"><img alt="unokun" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5247/profile-images/1473681853" /></a></li><li class="js-hovercard" data-hovercard-target-name="Rompei"><a itemprop="url" href="/Rompei"><img alt="Rompei" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64107/profile-images/1473696477" /></a></li><li class="js-hovercard" data-hovercard-target-name="japboy"><a itemprop="url" href="/japboy"><img alt="japboy" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5051/profile-images/1473681267" /></a></li><li class="js-hovercard" data-hovercard-target-name="luminor"><a itemprop="url" href="/luminor"><img alt="luminor" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/109257/profile-images/1473710819" /></a></li><li class="js-hovercard" data-hovercard-target-name="shogiai"><a itemprop="url" href="/shogiai"><img alt="shogiai" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/86977/profile-images/1473703926" /></a></li><li class="js-hovercard" data-hovercard-target-name="quoth"><a itemprop="url" href="/quoth"><img alt="quoth" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/21190/profile-images/1473683258" /></a></li><li class="js-hovercard" data-hovercard-target-name="dkt"><a itemprop="url" href="/dkt"><img alt="dkt" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/89459/profile-images/1473704760" /></a></li><li class="js-hovercard" data-hovercard-target-name="letusfly85"><a itemprop="url" href="/letusfly85"><img alt="letusfly85" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/49547/profile-images/1473691769" /></a></li><li><a href="/kiminaka/items/87afd4a433dc655d8cfd/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/kiminaka"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/107310/profile-images/1477882714" alt="1477882714" /></a> <a class="u-link-unstyled" href="/kiminaka">kiminaka</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-01-14T11:53:46+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-01-14">Edited at <time datetime="2017-02-12T14:02:52+09:00" itemprop="dateModified">2017-02-12</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/kiminaka/items/87afd4a433dc655d8cfd/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">9</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__editRequest"><span class="fa fa-inbox"></span><a href="/kiminaka/items/87afd4a433dc655d8cfd/patches">Edit Request</a><span class="ArticleAsideHeader__editRequestCount">4</span></div></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/kiminaka/items/87afd4a433dc655d8cfd/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(9)</span></a></li><li><a href="/kiminaka/items/87afd4a433dc655d8cfd.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li class="dropdown__item--mobile"><a href="/kiminaka/items/87afd4a433dc655d8cfd/patches"><span class="fa fa-fw fa-inbox"></span> Edit Request<span>(4)</span></a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-87afd4a433dc655d8cfd" itemprop="articleBody"><p>再帰型ニューラルネットワーク(RNN)は自然言語処理の分野で高い成果をあげ、現在最も注目されているアルゴリズムの一つです。しかしながら、その人気が先走りして実際にRNNがどのように動くのか、構築するのかを解説する書籍は限られているように思います。この投稿はその部分にフォーカスを当てて友人のDenny（<a href="http://www.wildml.com/" rel="nofollow noopener" target="_blank">WildMLブログ</a>の著者）と一緒に書きました。</p>

<p>さてRNNベースの言語モデルを解説したいと思います。言語モデルの用途は2つあります。1つ目は文章が実際にどのくらいの確率で現れるのかのスコアリングをすること。このスコアは文法的に、セマンティクス的に正しいかどうかの判断基準となります。このようなモデルは例えば機械翻訳などに使われています。次に2つ目ですが、言語モデルは新たなテキストを生成することができる点 (ちなみに個人的にこちらの方がよりCoolな用途だと思っています)。また、英語ですがAndrej Karpathyの<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow noopener" target="_blank">ブログ</a>では単語レベルのRNN言語モデルを解説・開発していますので時間のある方はぜひ一度読んでみてください。</p>

<p>読者はニューラルネットワークについて初歩的な事はおさえていると仮定します。もしそうでない場合、<a href="http://blog.moji.ai/2015/12/python%e3%81%a7%e3%83%a9%e3%82%a4%e3%83%96%e3%83%a9%e3%83%aa%e3%83%bc%e3%82%92%e4%bd%bf%e3%82%8f%e3%81%9a%e3%81%ab%e3%80%81%e3%83%8b%e3%83%a5%e3%83%bc%e3%83%a9%e3%83%ab%e3%83%8d%e3%83%83%e3%83%88/" rel="nofollow noopener" target="_blank">ライブラリーを使わずにPythonでニューラルネットワークを構築してみよう</a>、を読んでみてください。こちらの投稿では非再帰型のネットワークモデルの解説・構築をしています。</p>

<h1>
<span id="rnn-recurrent-neural-networkとは" class="fragment"></span><a href="#rnn-recurrent-neural-network%E3%81%A8%E3%81%AF"><i class="fa fa-link"></i></a>RNN (Recurrent Neural Network)とは？</h1>

<p>RNNの利点は文章など連続的な情報を利用できる点です。従来のニューラルネットワークの考え方はそうではなく、インプットデータ（またアウトプットデータも）は互いに独立の状態にある、と仮定します。しかしこの仮定は多くの場合、適切ではありません。例えば次の言葉を予測したい場合、その前の言葉が何だったのかを知っておくべきですよね？RNNのRはReccurent（再帰）という意味で、直前の計算に左右されずに、連続的な要素ごとに同じ作業を行わせることができます。言い方を変えると、RNNは以前に計算された情報を覚えるための記憶力を持っています。理論的にはRNNはとても長い文章の情報を利用することが可能です。ただ実際に実装してみると2,3ステップくらい前の情報しか覚えられません（以下で更にこの件を掘り下げます）。さて、一般的なRNNを下記図表で見てみましょう。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/107310/1907ad99-0e89-cf14-0d77-d38e4ee7d7b7.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/107310/1907ad99-0e89-cf14-0d77-d38e4ee7d7b7.jpeg" alt="rnn.jpg"></a><br>
<em>A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature</em></p>

<p>上記の図表はRNNの内部を展開しています。展開する、という意味は単に順序付けされたネットワークを書く、ということです。例えば、5ワードからなる文章があるとすれば、展開されたネットワークは1層1ワードで5層のニューラルネットワークになります。RNNを計算する際の公式は下記のようになります。</p>

<p>$x_t$は$t$ステップ時のインプットです。例えば、$x_1$は次の言葉に紐付いたベクトルです。<br>
$s_t$は$t$ステップ時の隠れ要素です。これがネットワークのMemory（記憶力）となります。$s_t$は直前の隠れ要素をベースに計算されます。そしてこのステップでのインプットが$s_t = f (U x_x + W s_{t-1})$となります。$f$関数は<a href="https://reference.wolfram.com/language/ref/Tanh.html" rel="nofollow noopener" target="_blank">tanh</a>や<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="nofollow noopener" target="_blank">ReLU</a>などの非線形型が一般的です。最初の隠れ要素を計算するのに必要な$s_{-1}$は普通0から始めます。<br>
$o_t$は$t$ステップ時のアウトプットです。例えば、次の言葉を予測したい場合、$o_t$は予測確率のベクトルになります ($o_t = softmax(V s_t)$)。</p>

<h1>
<span id="rnnができること" class="fragment"></span><a href="#rnn%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8"><i class="fa fa-link"></i></a>RNNができること</h1>

<p>RNNは自然言語処理分野で、すでに色々な成功事例があります。RNNが少しわかってきたところで、最も使われているRNNの一つである<a href="https://en.wikipedia.org/wiki/Long_short_term_memory" rel="nofollow noopener" target="_blank">LSTM</a>を紹介します。LSTMはRNNよりも距離のあるステップの関係性を学習するのに優れています。ただLSTMも基本的には今回構築するRNNと同じアルゴリズム構造なので心配しないでください。違いは隠れ要素を計算する方法が違うだけです。LSTMについては今後カバーする予定ですので、興味のある方はメルマガ登録してみてください。</p>

<h1>
<span id="言語モデルと文章生成" class="fragment"></span><a href="#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E6%96%87%E7%AB%A0%E7%94%9F%E6%88%90"><i class="fa fa-link"></i></a>言語モデルと文章生成</h1>

<p>言語モデルは、連続した言葉の中で、直前の言葉を利用して次の言葉の出現確率を予測することができます。どのくらいの頻度で文章が現れるのかを測ることができるため、機械翻訳に活用されています。次の言葉を予測できることのもう一つ良いことは、アウトプットの確率からサンプリングすることで新しい文章を生成できるGenerativeモデルを得られる点です。そのため、学習用データ次第で<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="nofollow noopener" target="_blank">様々なもの</a>を生成することができます。言語モデルでは、インプットデータは連続的な言葉の列です。そして、アウトプットは予測された言葉の列になります。ネットワークを学習させる時、$t$ステップのアウトプットを次の言葉にしたいため、$o_t = x_{t+1} $とします。</p>

<p>英語になりますが、言語モデルとテキスト生成に関する論文で参考になるものを下記に記載します。</p>

<ul>
<li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf" rel="nofollow noopener" target="_blank">Recurrent neural network based language model</a></li>
<li><a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf" rel="nofollow noopener" target="_blank">Extensions of Recurrent neural network based language model</a></li>
<li><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf" rel="nofollow noopener" target="_blank">Generating Text with Recurrent Neural Networks</a></li>
</ul>

<h1>
<span id="機械翻訳" class="fragment"></span><a href="#%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3"><i class="fa fa-link"></i></a>機械翻訳</h1>

<p>機械翻訳は、ソース言語（例えば日本語）の文章をインプットとする点で、言語モデルと似ています。そしてアウトプットは例えば英語の文章です。言語モデルとの違いは、アウトプットデータは完全なインプットデータを読み込んだ後に処理を開始するという点です。そのため、翻訳された文章の最初の言葉は完全なインプット文章の情報が必要になります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/107310/a9f0167c-72dc-bca9-f503-2a68a2fa6f54.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/107310/a9f0167c-72dc-bca9-f503-2a68a2fa6f54.png" alt="Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png"></a><br>
<em>RNN for Machine Translation. Image Source: <a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf" class="autolink" rel="nofollow noopener" target="_blank">http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf</a></em></p>

<p>英語になりますが、機械翻訳に関する論文で参考になるものを下記に記載します。</p>

<ul>
<li><a href="http://www.aclweb.org/anthology/P14-1140.pdf" rel="nofollow noopener" target="_blank">A Recursive Recurrent Neural Network for Statistical Machine Translation</a></li>
<li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="nofollow noopener" target="_blank">Sequence to Sequence Learning with Neural Networks</a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf" rel="nofollow noopener" target="_blank">Joint Language and Translation Modeling with Recurrent Neural Networks</a></li>
</ul>

<h1>
<span id="スピーチ認識" class="fragment"></span><a href="#%E3%82%B9%E3%83%94%E3%83%BC%E3%83%81%E8%AA%8D%E8%AD%98"><i class="fa fa-link"></i></a>スピーチ認識</h1>

<p>音波からの連続的な音響信号をインプットとして、連続的な音声セグメントを確率付けして予測します。</p>

<p>英語になりますが、スピーチ認識に関する論文で参考になるものを下記に記載します。</p>

<ul>
<li><a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" rel="nofollow noopener" target="_blank">Towards End-to-End Speech Recognition with Recurrent Neural Networks</a></li>
</ul>

<h1>
<span id="画像の概要生成" class="fragment"></span><a href="#%E7%94%BB%E5%83%8F%E3%81%AE%E6%A6%82%E8%A6%81%E7%94%9F%E6%88%90"><i class="fa fa-link"></i></a>画像の概要生成</h1>

<p>Convolutional Neural NetworksとRNNを使って、ラベルがついていない画像の概要生成ができます。下記の画像からわかるようにかなり高い確率で概要生成することが可能です。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/107310/266aa27f-36ac-33e9-cb61-e006115a79c3.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/107310/266aa27f-36ac-33e9-cb61-e006115a79c3.png" alt="Screen-Shot-2015-09-17-at-11.44.24-AM-1024x349.png"></a></p>

<p><em>Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: <a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" class="autolink" rel="nofollow noopener" target="_blank">http://cs.stanford.edu/people/karpathy/deepimagesent/</a></em></p>

<h1>
<span id="rnnを学習させる" class="fragment"></span><a href="#rnn%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%95%E3%81%9B%E3%82%8B"><i class="fa fa-link"></i></a>RNNを学習させる</h1>

<p>RNNの学習は従来のニューラルネットワークを学習させるのと似ていますが、RNNの場合、backpropagationアルゴリズムを少し変えて使います。RNNのパラメーターはネットワーク上の全ステップで使われているため、ステップ毎の勾配は現在のステップの計算以外にも前のステップの計算も使います。例えば、$t = 4$の勾配を計算するために、3ステップ後ろに戻って勾配を足し合わせる必要があります。これをBackpropagation Through Time (BPTT)と呼びます。もし意味がわからなくなってきても心配しないでください。後の投稿で詳細を書く予定です。今のところは、BPTTを使って学習させたRNNは遠ければ遠いステップほど学習させるのが難しいと覚えておいてください。この問題を解決するためにLSTMのようなアルゴリズム (RNNの一種)が開発されています。</p>

<h1>
<span id="rnnの応用" class="fragment"></span><a href="#rnn%E3%81%AE%E5%BF%9C%E7%94%A8"><i class="fa fa-link"></i></a>RNNの応用</h1>

<p>近年の研究者の開発努力によって従来のRNNの欠点を解消できる、より洗練されたRNNモデルが登場しています。それは今後の投稿で解説したいと思いますが、この投稿では、簡単に紹介したいと思います。</p>

<h2>
<span id="bidirectional-rnn" class="fragment"></span><a href="#bidirectional-rnn"><i class="fa fa-link"></i></a>Bidirectional RNN</h2>

<p>Bidirectional RNNでは、$t$のアウトプットは直前の要素だけをベースに計算するのではなく、後の要素も計算に含めます。例えば、前の部分に出てこない言葉を予測する場合、後の言葉も含め確率を算出するべきです。そのため、Bidirectional RNNは、2つのRNNが重なりあっているもの、と考えてみてください。アウトプットは2つの隠れ要素から計算されます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/107310/cb73be61-f4a3-a55c-d1ec-0e8c03ff4697.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/107310/cb73be61-f4a3-a55c-d1ec-0e8c03ff4697.png" alt="bidirectional-rnn-300x196.png"></a></p>

<h2>
<span id="deep-bidirectional-rnn" class="fragment"></span><a href="#deep-bidirectional-rnn"><i class="fa fa-link"></i></a>Deep (Bidirectional) RNN</h2>

<p>Deep RNNはBidirectional RNNに似ていますが、ステップごとに複数の層を持つ、という点が異なっています。実装してみると、より高い学習能力を得られるでしょう (まだ多くの学習データが必要ではありますが)。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/107310/d2c11318-bdc9-b0ee-21ba-9d1102bcd58c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/107310/d2c11318-bdc9-b0ee-21ba-9d1102bcd58c.png" alt="Screen-Shot-2015-09-16-at-2.21.51-PM-272x300.png"></a></p>

<h2>
<span id="lstmネットワーク" class="fragment"></span><a href="#lstm%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF"><i class="fa fa-link"></i></a>LSTMネットワーク</h2>

<p>少し触れましたが、LSTMネットワークは最近人気が出てきたRNNの一つです。LSTMはRNNと基本的に同じ構造ですが、隠れ要素を計算するのに異なる関数を率いています。LSTMのMemoryはCellと呼ばれており、直前の要素$h_{t-1}$と現在の要素$x_t$をインプットとしたブラックボックスだと考えてください。ブラックボックス内部ではMemoryにストアするCellを選びます。そして、直前の要素、現在の要素、インプットを組み合わせます。その結果、遠く離れた言葉の関係性をうまく抽出することが可能となります。LSTMは少し理解が難しいですが、興味があれば英語ですが、<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow noopener" target="_blank">こちら</a>の解説がわかりやすいので参考にしてみてください。</p>
<div class="hidden"><form class="js-task-list-update" action="/kiminaka/items/87afd4a433dc655d8cfd" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="SYxYblTXvDZrtpAb5W6l3s+HgntRXGgcuQJh5+tPnaaM0qYmuA3OwfqY30kZ6Rrr4PW4jHqws2goIwkZwvz41g==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1486875772" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
再帰型ニューラルネットワーク(RNN)は自然言語処理の分野で高い成果をあげ、現在最も注目されているアルゴリズムの一つです。しかしながら、その人気が先走りして実際にRNNがどのように動くのか、構築するのかを解説する書籍は限られているように思います。この投稿はその部分にフォーカスを当てて友人のDenny（[WildMLブログ](http://www.wildml.com/)の著者）と一緒に書きました。

さてRNNベースの言語モデルを解説したいと思います。言語モデルの用途は2つあります。1つ目は文章が実際にどのくらいの確率で現れるのかのスコアリングをすること。このスコアは文法的に、セマンティクス的に正しいかどうかの判断基準となります。このようなモデルは例えば機械翻訳などに使われています。次に2つ目ですが、言語モデルは新たなテキストを生成することができる点 (ちなみに個人的にこちらの方がよりCoolな用途だと思っています)。また、英語ですがAndrej Karpathyの[ブログ](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)では単語レベルのRNN言語モデルを解説・開発していますので時間のある方はぜひ一度読んでみてください。

読者はニューラルネットワークについて初歩的な事はおさえていると仮定します。もしそうでない場合、[ライブラリーを使わずにPythonでニューラルネットワークを構築してみよう](http://blog.moji.ai/2015/12/python%e3%81%a7%e3%83%a9%e3%82%a4%e3%83%96%e3%83%a9%e3%83%aa%e3%83%bc%e3%82%92%e4%bd%bf%e3%82%8f%e3%81%9a%e3%81%ab%e3%80%81%e3%83%8b%e3%83%a5%e3%83%bc%e3%83%a9%e3%83%ab%e3%83%8d%e3%83%83%e3%83%88/)、を読んでみてください。こちらの投稿では非再帰型のネットワークモデルの解説・構築をしています。

# RNN (Recurrent Neural Network)とは？
RNNの利点は文章など連続的な情報を利用できる点です。従来のニューラルネットワークの考え方はそうではなく、インプットデータ（またアウトプットデータも）は互いに独立の状態にある、と仮定します。しかしこの仮定は多くの場合、適切ではありません。例えば次の言葉を予測したい場合、その前の言葉が何だったのかを知っておくべきですよね？RNNのRはReccurent（再帰）という意味で、直前の計算に左右されずに、連続的な要素ごとに同じ作業を行わせることができます。言い方を変えると、RNNは以前に計算された情報を覚えるための記憶力を持っています。理論的にはRNNはとても長い文章の情報を利用することが可能です。ただ実際に実装してみると2,3ステップくらい前の情報しか覚えられません（以下で更にこの件を掘り下げます）。さて、一般的なRNNを下記図表で見てみましょう。

![rnn.jpg](https://qiita-image-store.s3.amazonaws.com/0/107310/1907ad99-0e89-cf14-0d77-d38e4ee7d7b7.jpeg)
*A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature*

上記の図表はRNNの内部を展開しています。展開する、という意味は単に順序付けされたネットワークを書く、ということです。例えば、5ワードからなる文章があるとすれば、展開されたネットワークは1層1ワードで5層のニューラルネットワークになります。RNNを計算する際の公式は下記のようになります。

$x_t$は$t$ステップ時のインプットです。例えば、$x_1$は次の言葉に紐付いたベクトルです。
$s_t$は$t$ステップ時の隠れ要素です。これがネットワークのMemory（記憶力）となります。$s_t$は直前の隠れ要素をベースに計算されます。そしてこのステップでのインプットが$s_t = f (U x_x + W s_{t-1})$となります。$f$関数は[tanh](https://reference.wolfram.com/language/ref/Tanh.html)や[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))などの非線形型が一般的です。最初の隠れ要素を計算するのに必要な$s_{-1}$は普通0から始めます。
$o_t$は$t$ステップ時のアウトプットです。例えば、次の言葉を予測したい場合、$o_t$は予測確率のベクトルになります ($o_t = softmax(V s_t)$)。

# RNNができること
RNNは自然言語処理分野で、すでに色々な成功事例があります。RNNが少しわかってきたところで、最も使われているRNNの一つである[LSTM](https://en.wikipedia.org/wiki/Long_short_term_memory)を紹介します。LSTMはRNNよりも距離のあるステップの関係性を学習するのに優れています。ただLSTMも基本的には今回構築するRNNと同じアルゴリズム構造なので心配しないでください。違いは隠れ要素を計算する方法が違うだけです。LSTMについては今後カバーする予定ですので、興味のある方はメルマガ登録してみてください。

# 言語モデルと文章生成
言語モデルは、連続した言葉の中で、直前の言葉を利用して次の言葉の出現確率を予測することができます。どのくらいの頻度で文章が現れるのかを測ることができるため、機械翻訳に活用されています。次の言葉を予測できることのもう一つ良いことは、アウトプットの確率からサンプリングすることで新しい文章を生成できるGenerativeモデルを得られる点です。そのため、学習用データ次第で[様々なもの](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)を生成することができます。言語モデルでは、インプットデータは連続的な言葉の列です。そして、アウトプットは予測された言葉の列になります。ネットワークを学習させる時、$t$ステップのアウトプットを次の言葉にしたいため、$o_t = x_{t+1} $とします。

英語になりますが、言語モデルとテキスト生成に関する論文で参考になるものを下記に記載します。

- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
- [Extensions of Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)
- [Generating Text with Recurrent Neural Networks](http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf)

# 機械翻訳
機械翻訳は、ソース言語（例えば日本語）の文章をインプットとする点で、言語モデルと似ています。そしてアウトプットは例えば英語の文章です。言語モデルとの違いは、アウトプットデータは完全なインプットデータを読み込んだ後に処理を開始するという点です。そのため、翻訳された文章の最初の言葉は完全なインプット文章の情報が必要になります。

![Screen-Shot-2015-09-17-at-10.39.06-AM-1024x557.png](https://qiita-image-store.s3.amazonaws.com/0/107310/a9f0167c-72dc-bca9-f503-2a68a2fa6f54.png)
*RNN for Machine Translation. Image Source: http://cs224d.stanford.edu/lectures/CS224d-Lecture8.pdf*

英語になりますが、機械翻訳に関する論文で参考になるものを下記に記載します。

- [A Recursive Recurrent Neural Network for Statistical Machine Translation](http://www.aclweb.org/anthology/P14-1140.pdf)
- [Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- [Joint Language and Translation Modeling with Recurrent Neural Networks](http://research.microsoft.com/en-us/um/people/gzweig/Pubs/EMNLP2013RNNMT.pdf)

# スピーチ認識
音波からの連続的な音響信号をインプットとして、連続的な音声セグメントを確率付けして予測します。

英語になりますが、スピーチ認識に関する論文で参考になるものを下記に記載します。

- [Towards End-to-End Speech Recognition with Recurrent Neural Networks](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf)

# 画像の概要生成
Convolutional Neural NetworksとRNNを使って、ラベルがついていない画像の概要生成ができます。下記の画像からわかるようにかなり高い確率で概要生成することが可能です。
![Screen-Shot-2015-09-17-at-11.44.24-AM-1024x349.png](https://qiita-image-store.s3.amazonaws.com/0/107310/266aa27f-36ac-33e9-cb61-e006115a79c3.png)

*Deep Visual-Semantic Alignments for Generating Image Descriptions. Source: http://cs.stanford.edu/people/karpathy/deepimagesent/*

# RNNを学習させる
RNNの学習は従来のニューラルネットワークを学習させるのと似ていますが、RNNの場合、backpropagationアルゴリズムを少し変えて使います。RNNのパラメーターはネットワーク上の全ステップで使われているため、ステップ毎の勾配は現在のステップの計算以外にも前のステップの計算も使います。例えば、$t = 4$の勾配を計算するために、3ステップ後ろに戻って勾配を足し合わせる必要があります。これをBackpropagation Through Time (BPTT)と呼びます。もし意味がわからなくなってきても心配しないでください。後の投稿で詳細を書く予定です。今のところは、BPTTを使って学習させたRNNは遠ければ遠いステップほど学習させるのが難しいと覚えておいてください。この問題を解決するためにLSTMのようなアルゴリズム (RNNの一種)が開発されています。

# RNNの応用
近年の研究者の開発努力によって従来のRNNの欠点を解消できる、より洗練されたRNNモデルが登場しています。それは今後の投稿で解説したいと思いますが、この投稿では、簡単に紹介したいと思います。

## Bidirectional RNN
Bidirectional RNNでは、$t$のアウトプットは直前の要素だけをベースに計算するのではなく、後の要素も計算に含めます。例えば、前の部分に出てこない言葉を予測する場合、後の言葉も含め確率を算出するべきです。そのため、Bidirectional RNNは、2つのRNNが重なりあっているもの、と考えてみてください。アウトプットは2つの隠れ要素から計算されます。

![bidirectional-rnn-300x196.png](https://qiita-image-store.s3.amazonaws.com/0/107310/cb73be61-f4a3-a55c-d1ec-0e8c03ff4697.png)

## Deep (Bidirectional) RNN
Deep RNNはBidirectional RNNに似ていますが、ステップごとに複数の層を持つ、という点が異なっています。実装してみると、より高い学習能力を得られるでしょう (まだ多くの学習データが必要ではありますが)。

![Screen-Shot-2015-09-16-at-2.21.51-PM-272x300.png](https://qiita-image-store.s3.amazonaws.com/0/107310/d2c11318-bdc9-b0ee-21ba-9d1102bcd58c.png)

## LSTMネットワーク
少し触れましたが、LSTMネットワークは最近人気が出てきたRNNの一つです。LSTMはRNNと基本的に同じ構造ですが、隠れ要素を計算するのに異なる関数を率いています。LSTMのMemoryはCellと呼ばれており、直前の要素$h_{t-1}$と現在の要素$x_t$をインプットとしたブラックボックスだと考えてください。ブラックボックス内部ではMemoryにストアするCellを選びます。そして、直前の要素、現在の要素、インプットを組み合わせます。その結果、遠く離れた言葉の関係性をうまく抽出することが可能となります。LSTMは少し理解が難しいですが、興味があれば英語ですが、[こちら](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)の解説がわかりやすいので参考にしてみてください。

</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="再帰型ニューラルネットワーク: RNN入門 on @Qiita" data-url="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="再帰型ニューラルネットワーク: RNN入門" href="http://b.hatena.ne.jp/entry/http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/kiminaka"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/107310/profile-images/1477882714" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/kiminaka">kiminaka</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">698</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;kiminaka&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-9a910929-4939-4bb8-91cd-29dfe17beef8"></div>
    <div id="UserFollowButton-react-component-9a910929-4939-4bb8-91cd-29dfe17beef8"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kiminaka/items/9ae195739093277490fe">ライブラリーを使わずにPythonでニューラルネットワークを構築してみる</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kiminaka/items/87afd4a433dc655d8cfd">再帰型ニューラルネットワーク: RNN入門</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kiminaka/items/cf860ee59128b3f83bd9">なぜ2015年はAI技術がアツかったのか考えてみる (+2016年のトレンド予測)</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#rnn-recurrent-neural-network%E3%81%A8%E3%81%AF\&quot;\u003eRNN (Recurrent Neural Network)とは？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#rnn%E3%81%8C%E3%81%A7%E3%81%8D%E3%82%8B%E3%81%93%E3%81%A8\&quot;\u003eRNNができること\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A8%E6%96%87%E7%AB%A0%E7%94%9F%E6%88%90\&quot;\u003e言語モデルと文章生成\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3\&quot;\u003e機械翻訳\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%82%B9%E3%83%94%E3%83%BC%E3%83%81%E8%AA%8D%E8%AD%98\&quot;\u003eスピーチ認識\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%94%BB%E5%83%8F%E3%81%AE%E6%A6%82%E8%A6%81%E7%94%9F%E6%88%90\&quot;\u003e画像の概要生成\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#rnn%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%95%E3%81%9B%E3%82%8B\&quot;\u003eRNNを学習させる\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#rnn%E3%81%AE%E5%BF%9C%E7%94%A8\&quot;\u003eRNNの応用\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#bidirectional-rnn\&quot;\u003eBidirectional RNN\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-bidirectional-rnn\&quot;\u003eDeep (Bidirectional) RNN\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#lstm%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF\&quot;\u003eLSTMネットワーク\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-fb953bac-cdad-47d9-9018-ed20f5a26689"></div>
    <div id="Toc-react-component-fb953bac-cdad-47d9-9018-ed20f5a26689"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:324,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="antimon2"><a itemprop="url" href="/antimon2"><img alt="antimon2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/30400/profile-images/1473685489" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="unokun"><a itemprop="url" href="/unokun"><img alt="unokun" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5247/profile-images/1473681853" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Rompei"><a itemprop="url" href="/Rompei"><img alt="Rompei" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64107/profile-images/1473696477" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="japboy"><a itemprop="url" href="/japboy"><img alt="japboy" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5051/profile-images/1473681267" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="luminor"><a itemprop="url" href="/luminor"><img alt="luminor" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/109257/profile-images/1473710819" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="shogiai"><a itemprop="url" href="/shogiai"><img alt="shogiai" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/86977/profile-images/1473703926" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="quoth"><a itemprop="url" href="/quoth"><img alt="quoth" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/21190/profile-images/1473683258" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="dkt"><a itemprop="url" href="/dkt"><img alt="dkt" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/89459/profile-images/1473704760" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="letusfly85"><a itemprop="url" href="/letusfly85"><img alt="letusfly85" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/49547/profile-images/1473691769" /></a></div></div><div class="ArticleFooter__user"><a href="/kiminaka/items/87afd4a433dc655d8cfd/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/87afd4a433dc655d8cfd/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/kiminaka/items/87afd4a433dc655d8cfd.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/7of9/items/ee4f7e913e6cd599510b#_reference-f9fa0266bd3bff286280"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996" />RNN &gt; RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 – INTRODUCTION TO RNNS &gt; 翻訳版にはない注記</a><time class="references_datetime js-dateTimeView" datetime="2016-11-05T12:13:18+00:00">4 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/yamano357/items/27bb3d39dc8047c46dba#_reference-49e9fbf3beed99efc3f0"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/99957/profile-images/1473707949" />RでKerasを使う（短歌手習い編）</a><time class="references_datetime js-dateTimeView" datetime="2017-01-03T15:39:27+00:00">2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="再帰型ニューラルネットワーク: RNN入門 on @Qiita" data-url="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="再帰型ニューラルネットワーク: RNN入門" href="http://b.hatena.ne.jp/entry/http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e翻訳元を明記したほうが良いのではないでしょうか？\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\u003c/a\u003e\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-31T01:38:51+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:603200,&quot;is_team&quot;:false,&quot;item_id&quot;:361316,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;翻訳元を明記したほうが良いのではないでしょうか？\n\nhttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd#comment-fc91a09606a271f17e2f&quot;,&quot;user&quot;:{&quot;contribution&quot;:2,&quot;created_at&quot;:&quot;2015-03-17T21:11:20+09:00&quot;,&quot;id&quot;:72604,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72604/profile-images/1473699210&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;nzw0301&quot;},&quot;uuid&quot;:&quot;fc91a09606a271f17e2f&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eコメントありがとうございます！wildMLブログの著者と一緒に書いておりました。一応載っけときました\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-31T01:51:28+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:603202,&quot;is_team&quot;:false,&quot;item_id&quot;:361316,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;コメントありがとうございます！wildMLブログの著者と一緒に書いておりました。一応載っけときました\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd#comment-a94db689683da1805684&quot;,&quot;user&quot;:{&quot;contribution&quot;:698,&quot;created_at&quot;:&quot;2015-12-28T16:24:18+09:00&quot;,&quot;id&quot;:107310,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/107310/profile-images/1477882714&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kiminaka&quot;},&quot;uuid&quot;:&quot;a94db689683da1805684&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e対応ありがとうございます．\u003cbr\u003e\nそうでしたか，大変失礼しました．素晴らしい記事ありがとうございます！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-31T01:59:54+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:603203,&quot;is_team&quot;:false,&quot;item_id&quot;:361316,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;対応ありがとうございます．\nそうでしたか，大変失礼しました．素晴らしい記事ありがとうございます！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd#comment-5819cf3ab07ad3adefd6&quot;,&quot;user&quot;:{&quot;contribution&quot;:2,&quot;created_at&quot;:&quot;2015-03-17T21:11:20+09:00&quot;,&quot;id&quot;:72604,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72604/profile-images/1473699210&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;nzw0301&quot;},&quot;uuid&quot;:&quot;5819cf3ab07ad3adefd6&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eBidirectional RNN節\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eそのため、Bidirectional RNNは、2つのRNNが重なりあっているもの、と考えてみてください。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e「そのため」という言葉が前の文書とのつながりで不自然に思いました。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\u003c/a\u003e\u003cbr\u003e\nを参照しますと、以下の様な訳が自然かと思われます。\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eBidirectional RNNsは非常に簡単なものだ。それは2つのRNNが重なりあっているものだ。その出力は両方のRNNの隠れ要素を用いて計算される。\u003c/p\u003e\n\u003c/blockquote\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-11-05T21:50:57+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:671748,&quot;is_team&quot;:false,&quot;item_id&quot;:361316,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Bidirectional RNN節\n\u003e そのため、Bidirectional RNNは、2つのRNNが重なりあっているもの、と考えてみてください。\n\n「そのため」という言葉が前の文書とのつながりで不自然に思いました。\n\nhttp://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\nを参照しますと、以下の様な訳が自然かと思われます。\n\n\u003e Bidirectional RNNsは非常に簡単なものだ。それは2つのRNNが重なりあっているものだ。その出力は両方のRNNの隠れ要素を用いて計算される。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd#comment-584626f02c4593579fcb&quot;,&quot;user&quot;:{&quot;contribution&quot;:3305,&quot;created_at&quot;:&quot;2013-11-24T20:17:16+09:00&quot;,&quot;id&quot;:32870,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;7of9&quot;},&quot;uuid&quot;:&quot;584626f02c4593579fcb&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:361316,&quot;uuid&quot;:&quot;87afd4a433dc655d8cfd&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;kiminaka&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:107310,&quot;url_name&quot;:&quot;kiminaka&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/107310/profile-images/1477882714&quot;},{&quot;id&quot;:72604,&quot;url_name&quot;:&quot;nzw0301&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72604/profile-images/1473699210&quot;},{&quot;id&quot;:32870,&quot;url_name&quot;:&quot;7of9&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-4000a7b5-712b-4cce-a109-f16280fa23c9"></div>
    <div id="CommentListContainer-react-component-4000a7b5-712b-4cce-a109-f16280fa23c9"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="5M4JEkPbR+Sn3Ibom83etv7iWTAFzFfNNy1XsdELQ9ghkPdarwE1EzbyybpnSmGD0ZBjxy4gjLmmDD9P+LgmqA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kiminaka/items/87afd4a433dc655d8cfd" /><input type="hidden" name="item_uuid" id="item_uuid" value="87afd4a433dc655d8cfd" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/kiminaka/items/87afd4a433dc655d8cfd", "id": 361316, "uuid": "87afd4a433dc655d8cfd" }</script><script class="js-user" type="application/json">{&quot;id&quot;:107310,&quot;url_name&quot;:&quot;kiminaka&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/107310/profile-images/1477882714&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="1KVujFXEjvA/XHH+B+j1fgXTJ2mDpfeJGN7a4CRCeLwR+5DEuR78B65yPqz7b0pLKqEdnqhJLP2J/7IeDfEdzA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kiminaka/items/87afd4a433dc655d8cfd" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>