<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>DeepLearning研究 2016年のまとめ - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="DeepLearning Advent Calendar 2016の17日目の記事です。


はじめに

はじめまして。
Liaroという会社でエンジニアをしている@eve_ykと申します。

今年もあと僅かとなりました。
ここらで、今年のDeepLearningの主要な成果を振り返ってみましょう。

この記事は、2016年に発表されたDeepLearning関係の研究を広く浅くまとめたものです。今年のDeepLearningの研究の進歩を俯瞰するのに役立てば幸いです。..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="eve_yk" name="twitter:creator" /><meta content="DeepLearning研究 2016年のまとめ - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="[DeepLearning Advent Calendar 2016](http://qiita.com/advent-calendar/2016/deeplearning)の17日目の記事です。

# はじめに
はじめまして。
[L..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="eTQ+aOousmu7ORg82KE0PQnDNTWccA53mDPBNUXCgw9fQy+xNRhWB+iBJq76nAzUraLg+z9og0ttqnqL6nYbQw==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"eve_yk","type":"items","id":"f4b274da7042cba1ba76"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-17a69430-43a7-4046-89ba-486009bf4c54"></div>
    <div id="HeaderContainer-react-component-17a69430-43a7-4046-89ba-486009bf4c54"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/DeepLearning",        "name": "DeepLearning"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader ArticleMainHeader--adcalItem"><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><div class="adventCalendarRibbon"><span><a class="adventCalendarRibbon_title" href="/advent-calendar/2016/deeplearning">DeepLearning Advent Calendar 2016</a> Day 17</span></div><h1 class="ArticleMainHeader__title" itemprop="headline">DeepLearning研究 2016年のまとめ</h1><ul class="TagList"><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="229"><a class="u-link-unstyled TagList__label" href="/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92"><img alt="深層学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/9594cfcb56d9180f74c468e56c69ce9f69cbe6ee/medium.jpg?1480640899" /><span>深層学習</span></a></li><li class="TagList__item" data-count="254"><a class="u-link-unstyled TagList__label" href="/tags/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD"><img alt="人工知能" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/2e7c05efd215b716c8372e4bf0388a3084c98f53/medium.jpg?1433343003" /><span>人工知能</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">594</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="2 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>2</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:594,&quot;uuid&quot;:&quot;f4b274da7042cba1ba76&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="niconegoto"><a itemprop="url" href="/niconegoto"><img alt="niconegoto" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/90340/profile-images/1478265447" /></a></li><li class="js-hovercard" data-hovercard-target-name="hanaken_Nirvana"><a itemprop="url" href="/hanaken_Nirvana"><img alt="hanaken_Nirvana" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/86921/profile-images/1473703908" /></a></li><li class="js-hovercard" data-hovercard-target-name="chachay"><a itemprop="url" href="/chachay"><img alt="chachay" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/117379/profile-images/1476087329" /></a></li><li class="js-hovercard" data-hovercard-target-name="ta-ka"><a itemprop="url" href="/ta-ka"><img alt="ta-ka" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/82527/profile-images/1473702483" /></a></li><li class="js-hovercard" data-hovercard-target-name="TomokIshii"><a itemprop="url" href="/TomokIshii"><img alt="TomokIshii" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/74152/profile-images/1473699746" /></a></li><li class="js-hovercard" data-hovercard-target-name="yizumi1012xxx"><a itemprop="url" href="/yizumi1012xxx"><img alt="yizumi1012xxx" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/106291/profile-images/1473709890" /></a></li><li class="js-hovercard" data-hovercard-target-name="akiraak"><a itemprop="url" href="/akiraak"><img alt="akiraak" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/33704/profile-images/1473686182" /></a></li><li class="js-hovercard" data-hovercard-target-name="neka-nat@github"><a itemprop="url" href="/neka-nat@github"><img alt="neka-nat@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/14516/profile-images/1473683376" /></a></li><li class="js-hovercard" data-hovercard-target-name="ohtake_i"><a itemprop="url" href="/ohtake_i"><img alt="ohtake_i" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/129071/profile-images/1473717434" /></a></li><li class="js-hovercard" data-hovercard-target-name="rkato5680"><a itemprop="url" href="/rkato5680"><img alt="rkato5680" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/100758/profile-images/1473708197" /></a></li><li><a href="/eve_yk/items/f4b274da7042cba1ba76/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/eve_yk"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" alt="1473711224" /></a> <a class="u-link-unstyled" href="/eve_yk">eve_yk</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-12-17T00:45:59+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-12-17">Edited at <time datetime="2016-12-25T10:43:57+09:00" itemprop="dateModified">2016-12-25</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/eve_yk/items/f4b274da7042cba1ba76/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">20</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/eve_yk/items/f4b274da7042cba1ba76/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(20)</span></a></li><li><a href="/eve_yk/items/f4b274da7042cba1ba76.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-f4b274da7042cba1ba76" itemprop="articleBody"><p><a href="http://qiita.com/advent-calendar/2016/deeplearning">DeepLearning Advent Calendar 2016</a>の17日目の記事です。</p>

<h1>
<span id="はじめに" class="fragment"></span><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><i class="fa fa-link"></i></a>はじめに</h1>

<p>はじめまして。<br>
<a href="http://company.liaro.me/" rel="nofollow noopener" target="_blank">Liaro</a>という会社でエンジニアをしている<a href="/eve_yk" class="user-mention js-hovercard" title="eve_yk" data-hovercard-target-type="user" data-hovercard-target-name="eve_yk">@eve_yk</a>と申します。</p>

<p>今年もあと僅かとなりました。<br>
ここらで、今年のDeepLearningの主要な成果を振り返ってみましょう。</p>

<p>この記事は、2016年に発表されたDeepLearning関係の研究を広く浅くまとめたものです。今年のDeepLearningの研究の進歩を俯瞰するのに役立てば幸いです。<br>
それぞれの内容について、その要点や感想なんかを簡単にまとめられたらと思います。<br>
特に重要だと思った研究には★マークをつけておきます。<br>
非常に長くなってしまったため、興味のある分野だけ読んでいただければと思います。</p>

<h5>
<span id="言い訳とお願い" class="fragment"></span><a href="#%E8%A8%80%E3%81%84%E8%A8%B3%E3%81%A8%E3%81%8A%E9%A1%98%E3%81%84"><i class="fa fa-link"></i></a>言い訳とお願い</h5>

<ul>
<li>見つけたものはコードへのリンクも示すので、プログラミングに関係ある記事ということで…</li>
<li>分野的にかなり偏ったまとめになりましたが、ご容赦ください。</li>
<li>まとめを始めた時期の関係で、2016年の研究の中でも比較的新しいものが多い点もご容赦ください</li>
<li>注意はしておりますが、内容の正確性は無保証です。誤りや不適切な点がありましたら、ご指摘いただけると幸いです。</li>
<li>他に重要なものがあれば「こんな研究もあるよ！」という紹介をしていただけるととても嬉しく思います。</li>
</ul>

<h2>
<span id="まとめのまとめ-1224-追記" class="fragment"></span><a href="#%E3%81%BE%E3%81%A8%E3%82%81%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81-1224-%E8%BF%BD%E8%A8%98"><i class="fa fa-link"></i></a>まとめのまとめ (※12/24 追記)</h2>

<p>年末ということで、この記事に以外にもたくさんのまとめ記事が投稿されています。<br>
どれも素晴らしいまとめですので、これらもご覧ください。</p>

<h4>
<span id="2016年のディープラーニング論文100選" class="fragment"></span><a href="#2016%E5%B9%B4%E3%81%AE%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E8%AB%96%E6%96%87100%E9%81%B8"><i class="fa fa-link"></i></a><a href="http://qiita.com/sakaiakira/items/9da1edda802c4884865c" id="reference-1cc2857fd0e64f59a6cf">2016年のディープラーニング論文100選</a>
</h4>

<p>より幅広い分野の論文がまとめられています。<br>
本記事で扱っていない分野に興味がある人も、この記事を読めば満足するはずです。</p>

<h4>
<span id="foobarnet-ディープラーニング関連のnetまとめ" class="fragment"></span><a href="#foobarnet-%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E9%96%A2%E9%80%A3%E3%81%AEnet%E3%81%BE%E3%81%A8%E3%82%81"><i class="fa fa-link"></i></a><a href="http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7" id="reference-037bec310c4fa9fe8cbb">foobarNet: ディープラーニング関連の○○Netまとめ</a>
</h4>

<p>これまで発表された◯◯Netの総まとめです。<br>
圧倒的な情報量で、ひとつずつ眺めていくだけでいくらでも時間が潰せそうです。</p>

<h4>
<span id="2016年の深層学習を用いた画像認識モデル" class="fragment"></span><a href="#2016%E5%B9%B4%E3%81%AE%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB"><i class="fa fa-link"></i></a><a href="http://qiita.com/aiskoaskosd/items/59c49f2e2a6d76d62798" id="reference-c8996c8b87c40401a881">2016年の深層学習を用いた画像認識モデル</a>
</h4>

<p>近年の画像認識モデルのまとめ＋実装＋比較＋解説です。<br>
ほぼすべてのモデルにchainer実装があります。画像認識は本記事よりもこちらを読んだほうが良いです。</p>

<h4>
<span id="deep-learningの理論的論文リスト随時更新" class="fragment"></span><a href="#deep-learning%E3%81%AE%E7%90%86%E8%AB%96%E7%9A%84%E8%AB%96%E6%96%87%E3%83%AA%E3%82%B9%E3%83%88%E9%9A%8F%E6%99%82%E6%9B%B4%E6%96%B0"><i class="fa fa-link"></i></a><a href="http://mathetake.hatenablog.com/entry/2016/12/20/005632" rel="nofollow noopener" target="_blank">Deep Learningの理論的論文リスト【随時更新】</a>
</h4>

<p>深層学習の理論的に解析した論文のまとめです。<br>
深層学習そのものを理解するために行われている様々な解析が、非常に参考になります。</p>

<h2>
<span id="画像処理系" class="fragment"></span><a href="#%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86%E7%B3%BB"><i class="fa fa-link"></i></a>画像処理系</h2>

<p>画像の分野では、昨年に引き続き画像認識に関する研究が人気だと感じました。<br>
また、GANの登場により画像生成系の研究も非常に盛んであったと思います。</p>

<h3>
<span id="resnetの進化" class="fragment"></span><a href="#resnet%E3%81%AE%E9%80%B2%E5%8C%96"><i class="fa fa-link"></i></a>ResNetの進化</h3>

<p>2015年にMSRAが提案したResidual Networks(ResNet)は、より深いニューラルネットワークを訓練することの出来るアーキテクチャです。シンプルな改良で大きな成果を上げたこのモデルについて、様々な応用がなされました。<br>
ResNet自体の解説は以下の記事を参考ください。<br>
参考：<a href="http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf" rel="nofollow noopener" target="_blank">MSRAの解説スライド</a><br>
参考：<a href="http://qiita.com/supersaiakujin/items/935bbc9610d0f87607e8" id="reference-c98570ba0fdc3bed4b34">日本語の解説記事1</a><br>
参考：<a href="https://deepage.net/deep_learning/2016/11/30/resnet.html" rel="nofollow noopener" target="_blank">日本語の解説記事2</a></p>

<hr>

<h4>
<span id="deep-networks-with-stochastic-depth-20165" class="fragment"></span><a href="#deep-networks-with-stochastic-depth-20165"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1603.09382" rel="nofollow noopener" target="_blank">Deep Networks with Stochastic Depth</a> (2016/5)★</h4>

<p>ResNetの訓練時に、ランダムに幾つかの層をskip(ResNetにおけるidentity skip connection pathのみ残す)させる手法を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/a68ad0cc-e940-9fe7-a995-741472999eb2.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/a68ad0cc-e940-9fe7-a995-741472999eb2.jpeg" alt="StocasticDepth.JPG"></a><br>
<a href="https://arxiv.org/pdf/1603.09382v3.pdf" rel="nofollow noopener" target="_blank">Deep Networks with Stochastic Depth Fig. 2.</a></p>

<p>CIFAR-100等の画像分類のタスクにおいて、精度が向上することを示しました(2016/3時点のSOTA?)。また、訓練時の計算量が減るため、訓練時間が短縮されることもメリットになります。<br>
Deep Neural Networkの訓練によく使用されるDropoutに似た考え方の手法だと思います。</p>

<p>参考：<a href="http://qiita.com/supersaiakujin/items/eb0553a1ef1d46bd03fa" id="reference-99462fcdddc879f7b789">日本語のサーベイ記事</a></p>

<hr>

<h4>
<span id="fractalnet-ultra-deep-neural-networks-without-residuals-20165" class="fragment"></span><a href="#fractalnet-ultra-deep-neural-networks-without-residuals-20165"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1605.07648" rel="nofollow noopener" target="_blank">FractalNet: Ultra-Deep Neural Networks without Residuals</a> (2016/5)</h4>

<p>(ResNetの仲間ではありませんが…)<br>
複数の階層の深さが異なる畳み込み層の結果を結合するFractal Netを提案した論文です。以下の図のようなアーキテクチャです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/a6a7e4b1-6129-7d03-dd46-420bbf484080.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/a6a7e4b1-6129-7d03-dd46-420bbf484080.jpeg" alt="FractalNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1605.07648v1.pdf" rel="nofollow noopener" target="_blank">FractalNet: Ultra-Deep Neural Networks without Residuals Fig. 1.</a></p>

<p>"Deep Networks with Stochastic Depth"のようにいくつかの層をskipする訓練方法も示しています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/db5a0b7f-1eff-cb96-1ecc-f98484d65e5a.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/db5a0b7f-1eff-cb96-1ecc-f98484d65e5a.jpeg" alt="FractalNet2.JPG"></a><br>
<a href="https://arxiv.org/pdf/1605.07648v1.pdf" rel="nofollow noopener" target="_blank">FractalNet: Ultra-Deep Neural Networks without Residuals Fig. 2.</a></p>

<p>CIFAR-100等の画像分類のタスクにおいて、精度が向上することを示しました(2016/5時点のSOTA)。</p>

<p>"without Residuals"とありますが、多くの層を飛ばす図中の左端の経路の部分が、ResNetでいうidentity skip connection pathに似た働きをするのではないかと思います。</p>

<hr>

<h4>
<span id="residual-networks-of-residual-networks-multilevel-residual-networks-20168" class="fragment"></span><a href="#residual-networks-of-residual-networks-multilevel-residual-networks-20168"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1608.02908" rel="nofollow noopener" target="_blank">Residual Networks of Residual Networks: Multilevel Residual Networks</a> (2016/8)</h4>

<p>ResNetにおいて、複数のResidual Blockをまたがるshortcutを追加したResidual Networks of Residual Networks(RoR)を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/80622ff1-0d6a-cde8-8ddc-a05f9d2fcb53.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/80622ff1-0d6a-cde8-8ddc-a05f9d2fcb53.jpeg" alt="RoR.JPG"></a><br>
<a href="https://arxiv.org/pdf/1608.02908v1.pdf" rel="nofollow noopener" target="_blank">Residual Networks of Residual Networks: Multilevel Residual Networks Fig. 1.</a></p>

<p>CIFAR-100等の画像分類のタスクにおいて、ResNetと比較して精度が向上することを示しました。</p>

<hr>

<h4>
<span id="densely-connected-convolutional-networks-20168" class="fragment"></span><a href="#densely-connected-convolutional-networks-20168"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1608.06993" rel="nofollow noopener" target="_blank">Densely Connected Convolutional Networks</a> (2016/8)★</h4>

<p>複数の畳み込み層が密に結合するdense blockを取り入れたDenseNetを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/1490aa72-af1f-12de-8920-4d8abeeaf38d.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/1490aa72-af1f-12de-8920-4d8abeeaf38d.jpeg" alt="DenseNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1608.06993v3.pdf" rel="nofollow noopener" target="_blank">Densely Connected Convolutional Networks Fig. 1.</a></p>

<p>CIFAR-100等の画像分類のタスクにおいて、ResNetと比較して精度が向上することを示しました (こればっかり…)(2016/8時点のSOTA?)。</p>

<p>構造といい、アイデアがRoRとほとんど同じに感じます…。<br>
論文を詳しく読んでいないので詳しくはわからないのですが、どこに違いがあるのでしょうか？<br>
同じ時期に似たアイデアの論文が出ているところに、競争率の高さを感じます。</p>

<p>参考：<a href="https://github.com/liuzhuang13/DenseNet" rel="nofollow noopener" target="_blank">DenseNetの実装まとめ</a>。公式の実装はCaffeとTorchですが、Tensorflow, Lasagne, Keras Chainerの実装へのリンクもあります。</p>

<hr>

<h4>
<span id="aggregated-residual-transformations-for-deep-neural-networks-201611" class="fragment"></span><a href="#aggregated-residual-transformations-for-deep-neural-networks-201611"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1611.05431" rel="nofollow noopener" target="_blank">Aggregated Residual Transformations for Deep Neural Networks</a> (2016/11)</h4>

<p>ResNetのResidual Blockを改良したResNeXtを提案した論文。画像分類のコンペティションであるILSVRC 2016で２位となったモデルです。</p>

<p>Residual Block内の畳み込み層を並列にしている？</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/4f456122-c042-1244-1149-e811d48aa0d0.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/4f456122-c042-1244-1149-e811d48aa0d0.jpeg" alt="ResNeXt.JPG"></a><br>
(<a href="https://arxiv.org/pdf/1611.05431v1.pdf" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/pdf/1611.05431v1.pdf</a>)</p>

<p>DenseNetもResNeXtもFacebook AI Researchの論文です。</p>

<p>参考：<a href="https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py" rel="nofollow noopener" target="_blank">モデルの実装(MXNet)</a>。訓練済みのモデルは<a href="http://data.dmlc.ml/models/imagenet/resnext/" rel="nofollow noopener" target="_blank">ここ</a>で公開されています。</p>

<hr>

<h3>
<span id="画像生成" class="fragment"></span><a href="#%E7%94%BB%E5%83%8F%E7%94%9F%E6%88%90"><i class="fa fa-link"></i></a>画像生成</h3>

<p>昨年末にDCGANが登場してから、非常に盛り上がっている分野だと感じます。<br>
生成系は私にはとても理解できないので、要点のまとめもふわふわしています。</p>

<hr>

<h4>
<span id="pixel-recurrent-neural-networks20161" class="fragment"></span><a href="#pixel-recurrent-neural-networks20161"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1601.06759" rel="nofollow noopener" target="_blank">Pixel Recurrent Neural Networks</a>(2016/1)★</h4>

<p>あるピクセルの値をその周囲のピクセルから予測して画像生成を行うPixel RNNを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/e953cf41-aa24-66c5-b9f0-f53d5e33d984.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/e953cf41-aa24-66c5-b9f0-f53d5e33d984.jpeg" alt="PixelRNN.JPG"></a><br>
<a href="https://arxiv.org/pdf/1601.06759v3.pdf" rel="nofollow noopener" target="_blank">Pixel Recurrent Neural Networks Fig. 1.</a></p>

<p>この手法では、画像の左上のピクセルから右のピクセルを見ていき、右端まで見たら次の段の一番左のピクセルへ…という順にピクセルを順に見ていきます。あるピクセル値の予測には、これまでに見たピクセルの"文脈"を利用します。<br>
様々な"文脈"の使い方を提案・検証したこと、モデルの並列化方法を提案したことなどが貢献になります。</p>

<p>評価では、MNISTやCIFAR-10のデータセットの画像生成を行い、その対数尤度を比較しています。</p>

<p>このモデルのような、「ある時点の値を予測するためにその周囲（それ以前）の値を利用するモデル」のことを自己回帰モデルと言うそうです(間違っていたら指摘お願いします…)。<br>
この自己回帰モデルは画像以外の分野でも成功しています。詳しくは後述します。</p>

<p>参考：<a href="http://kusanohitoshi.blogspot.jp/2016/10/pixel-recurrent-neural-network.html" rel="nofollow noopener" target="_blank">日本語解説記事</a>。この記事の内容はもちろん、記事中で引用されているスライドも非常にわかりやすいです。<br>
参考：<a href="https://github.com/carpedm20/pixel-rnn-tensorflow" rel="nofollow noopener" target="_blank">tensorflow実装</a></p>

<hr>

<h4>
<span id="image-to-image-translation-with-conditional-adversarial-nets201611" class="fragment"></span><a href="#image-to-image-translation-with-conditional-adversarial-nets201611"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1611.07004" rel="nofollow noopener" target="_blank">Image-to-Image Translation with Conditional Adversarial Nets</a>(2016/11)★</h4>

<p>従来はタスクごとに独自のアプローチを検討する必要があった画像対画像変換について、汎用的なモデルであるConditional GANを提案し検証している論文です。通称pix2pix。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/33c5bc87-c453-99b7-3b23-632ddec641cb.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/33c5bc87-c453-99b7-3b23-632ddec641cb.jpeg" alt="pix2pix.JPG"></a><br>
<a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="nofollow noopener" target="_blank">Image-to-Image Translation with Conditional Adversarial Nets Fig. 2.</a></p>

<p>Conditional GANの訓練方法は上図の通り。<br>
Positive example(実際の画像)を与えるときは、変換前と変換後の画像のペアを与えて、実際の画像かどうかをDiscriminatorに解かせます。Negative example(pix2pixが生成する画像)を与えるときは、変換前の画像をGeneratorに入力して生成した画像と変換前の画像のペアを与えて、実際の画像かどうかをDiscriminatorに解かせます。<br>
また、よりリアルな画像生成のために、L1正規化やPatchGANといった工夫を取り入れているそうです。より詳細な解説は参考の解説記事を参照ください。</p>

<p>評価では、「航空画像→地図画像」、「灰色画像→カラー画像」、「日中の画像→夜間の画像」「線画→カラー画像」など様々な画像変換について、アンケート評価やFCN-scoreという尺度で行っています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/d5fffcdb-2546-00f6-5241-07252af2b532.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/d5fffcdb-2546-00f6-5241-07252af2b532.jpeg" alt="pix2pix2.JPG"></a><br>
<a href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="nofollow noopener" target="_blank">Image-to-Image Translation with Conditional Adversarial Nets Fig. 1.</a></p>

<p>個々のタスクでは独自に作成したモデルに負けていますが、汎用的に様々な画像変換を行える柔軟性はすごすぎます。</p>

<p>参考：<a href="https://phillipi.github.io/pix2pix/" rel="nofollow noopener" target="_blank">プロジェクトページ</a>。公式の実装(Torch)へのリンクもあります。<br>
参考：<a href="http://qiita.com/hiromu1996/items/38f1bd5a78336fa8ca25" id="reference-4686d51ebd9a2d6e1214">日本語の解説・検証記事</a>。より詳しい解説があります。<br>
参考：<a href="https://github.com/mattya/chainer-pix2pix" rel="nofollow noopener" target="_blank">chainer実装</a></p>

<hr>

<h4>
<span id="stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks201612" class="fragment"></span><a href="#stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks201612"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1612.03242" rel="nofollow noopener" target="_blank">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks</a>(2016/12)</h4>

<p>GANを二段階に分けることで高画質な画像生成を実現するStackGANを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/8050f0d4-ca26-e0ed-18aa-a29c0701e932.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/8050f0d4-ca26-e0ed-18aa-a29c0701e932.jpeg" alt="StackGAN.JPG"></a><br>
<a href="https://arxiv.org/pdf/1612.03242v1.pdf" rel="nofollow noopener" target="_blank">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks Fig. 1.</a></p>

<p>pix2pixは「画像→画像」の変換についての研究でしたが、StackGANは「テキスト→画像」の変換について取り組んだものです。<br>
最も特徴的なのは生成過程を二段階にしたところで、初めに低解像度(64x64)の画像を生成し、その画像を高画質(256x256)な画像生成のためのGeneratorの入力として与えます。<br>
その他の詳細についてはまだ追えていません…。</p>

<p>評価では、人間に対するアンケートに加えて、Inception scoresという指標を用いて行っています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/bc9d4bc5-332a-4bbd-3ba9-b7da3f5ba874.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/bc9d4bc5-332a-4bbd-3ba9-b7da3f5ba874.jpeg" alt="StackGAN2.JPG"></a><br>
<a href="https://arxiv.org/pdf/1612.03242v1.pdf" rel="nofollow noopener" target="_blank">StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks Fig. 3.</a></p>

<p>「２つのGANを積み重ねて上手くいくなら、もっとたくさん積み重ねればもっと良いものができるのでは？」と誰もが考えますが、より<a href="https://arxiv.org/abs/1612.04357" rel="nofollow noopener" target="_blank">多層なGAN</a>がこの論文の3日後(12/13)に公開されました。タスクが異なっており、また引用もされていないので別のチームが似たアイデアを同時期に出したもの思われますが、なんという競争率の高さ…。</p>

<h2>
<span id="動画処理系" class="fragment"></span><a href="#%E5%8B%95%E7%94%BB%E5%87%A6%E7%90%86%E7%B3%BB"><i class="fa fa-link"></i></a>動画処理系</h2>

<p>生成系のタスクが動画でも成功(?)し始めたことがこの分野で一番大きな出来事でしょうか？</p>

<h3>
<span id="動画生成" class="fragment"></span><a href="#%E5%8B%95%E7%94%BB%E7%94%9F%E6%88%90"><i class="fa fa-link"></i></a>動画生成</h3>

<p>こいつ…動くぞ！</p>

<hr>

<h4>
<span id="deep-predictive-coding-networks-for-fideo-prediction-and-unsupervised-learning20165" class="fragment"></span><a href="#deep-predictive-coding-networks-for-fideo-prediction-and-unsupervised-learning20165"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1605.08104" rel="nofollow noopener" target="_blank">Deep Predictive Coding Networks for Fideo Prediction and Unsupervised Learning</a>(2016/5)★</h4>

<p>ある時刻の画像から次の時刻の画像を予測するPredNetを提案した論文です。（厳密には動画生成ではないかもしれません…）</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/9d28077e-e495-744d-ea54-87b850aeaa9f.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/9d28077e-e495-744d-ea54-87b850aeaa9f.jpeg" alt="PredNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1605.08104v4.pdf" rel="nofollow noopener" target="_blank">Deep Predictive Coding Networks for Fideo Prediction and Unsupervised Learning Fig. 1.</a></p>

<p>PredNetは<a href="https://arxiv.org/pdf/1506.04214.pdf" rel="nofollow noopener" target="_blank">ConvLSTM</a>というアーキテクチャ(図の緑色のブロック)を用いて中間表現を生成します。その中間表現を用いて1時刻後の画像を予測(図の青色のブロック)します。予測結果と実際の画像のL1lossを計算(図の赤色のブロック)し、出力とします(次の時刻の予測が出力ではない)。また、この誤差は次の時刻の画像の予測に用います。<br>
処理の流れは、公式のページにある<a href="https://coxlab.github.io/prednet/prednet_animation.html" rel="nofollow noopener" target="_blank">こちらの動画</a>がわかりやすいです。</p>

<p>実験では、自作した3Dモデルが回転する映像や、車載カメラの映像を用いて画像の予測を行い結果を可視化して結果の検討を行っています。可視化結果の具体例は<a href="https://coxlab.github.io/prednet/" rel="nofollow noopener" target="_blank">公式のページ</a>から確認できます。</p>

<p>教師なし訓練ができる点が面白く感じました。<br>
<a href="https://wba-meetup.doorkeeper.jp/events/46611" rel="nofollow noopener" target="_blank">モデルの性質が脳の新皮質に類似している</a>そうで、わりと話題になったと記憶しています。</p>

<p>参考：<a href="http://karasunoblog.blog20.fc2.com/blog-entry-46.html" rel="nofollow noopener" target="_blank">日本語の解説記事</a>。わかりやすい良記事です。<br>
参考：<a href="http://www.slideshare.net/kojiochiai/pred-net" rel="nofollow noopener" target="_blank">日本語の検証報告スライド</a><br>
参考：<a href="https://github.com/coxlab/prednet" rel="nofollow noopener" target="_blank">keras実装</a><br>
参考：<a href="https://github.com/quadjr/PredNet" rel="nofollow noopener" target="_blank">chainer実装</a></p>

<hr>

<h4>
<span id="generating-videos-with-scene-dynamics" class="fragment"></span><a href="#generating-videos-with-scene-dynamics"><i class="fa fa-link"></i></a><a href="http://web.mit.edu/vondrick/tinyvideo/paper.pdf" rel="nofollow noopener" target="_blank">Generating Videos with Scene Dynamics</a>
</h4>

<p>GANを用いて画像から1秒程度の動画を生成する手法を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/647f944e-be1f-395f-7546-7fd9b883857c.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/647f944e-be1f-395f-7546-7fd9b883857c.jpeg" alt="GenerateVideo.JPG"></a><br>
<a href="http://web.mit.edu/vondrick/tinyvideo/paper.pdf" rel="nofollow noopener" target="_blank">Generating Videos with Scene Dynamics Fig. 1.</a></p>

<p>モデルの入力となるのは100次元のノイズのベクトルです。モデルは二つのストリームから成っており、一つは画像の前景の時刻による変化を、もうひとつは画像の背景をそれぞれ生成します。前景を処理するストリームでは画像のマスクも生成し、それらの結果を組み合わせて最終的な短時間の動画を生成します。前景の処理では" fractionally-strided spatio-temporal convolutions"(要調査…)、背景の処理では"fractionally-strided spatial convolutions"を用いているそうです。</p>

<p>モデルの訓練時は、前景を処理するストリーム部分の入出力を反転した構造(最終層だけ0or1のバイナリを出力するレイヤーに変更)のDiscriminator Networkを構築し、GANの要領で訓練を行います。</p>

<p>評価では、「ベースライン手法の生成した動画(または実際の動画)と提案手法で生成した動画のどちらがよりリアルか」をアンケートしています。<br>
ベースライン手法と比較したときは平均して82%の人が、実際の動画と比較すると平均して18%の人が、「提案手法の動画がよりリアルだ」と答えたそうです。</p>

<p>画像中の動きそうな部分（前景）と動かない部分(背景)を分けて訓練したところが賢いと感じました。(アンケートの結果では大きな差は出ていませんでしたが…)</p>

<p>参考：<a href="http://web.mit.edu/vondrick/tinyvideo/" rel="nofollow noopener" target="_blank">プロジェクトページ</a></p>

<hr>

<h2>
<span id="音声処理系" class="fragment"></span><a href="#%E9%9F%B3%E5%A3%B0%E5%87%A6%E7%90%86%E7%B3%BB"><i class="fa fa-link"></i></a>音声処理系</h2>

<p>詳しくない分野ですので、門外漢の私の耳にも届いたものでお茶を濁します。</p>

<hr>

<h4>
<span id="wavenet-a-generative-model-for-raw-audio-20169" class="fragment"></span><a href="#wavenet-a-generative-model-for-raw-audio-20169"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1609.03499" rel="nofollow noopener" target="_blank">WaveNet: A Generative Model for Raw Audio</a> (2016/9)★</h4>

<p>自己回帰モデルを用いて音声を生成するWaveNetを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/6eb22b79-ebc3-0c2f-afbb-ac513f75d064.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/6eb22b79-ebc3-0c2f-afbb-ac513f75d064.jpeg" alt="Wavenet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1609.03499v2.pdf" rel="nofollow noopener" target="_blank">WaveNet: A Generative Model for Raw Audio Fig. 3.</a></p>

<p>WaveNetは、ある時刻の波形(の値)をそれ以前の波形データから予測する自己回帰モデルです。より古い情報も効率よく利用するために、Dilated Convolutionという手法を使っています。また、Dilated Convolutionを利用すると非常に深いネットワークになるため、Residual Networksでネットワークを構築しています。</p>

<p>評価では、比較手法とWaveNetで生成した音声のどちらが自然かアンケートを取っています。結果は比較手法を大きく上回りました。また、音声認識のタスクも評価し、TIMITというコーパスで最も良いスコアを示しました。</p>

<p>自己回帰モデルの一番の成功例と感じます。<br>
自己回帰モデルの欠点として、生成に時間がかかる(WaveNetの場合、1秒の音声を生成するのに90分かかるそうです)ことがありますが、その<a href="https://arxiv.org/abs/1611.09482" rel="nofollow noopener" target="_blank">欠点を解消した手法</a>も登場しています。進歩が速すぎます。</p>

<p>参考：<a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="nofollow noopener" target="_blank">Deepmindの紹介記事</a>。デモが聞けます。またDilated Convolutionの動作がわかりやすいgifがあります。<br>
参考：<a href="http://musyoku.github.io/2016/09/18/wavenet-a-generative-model-for-raw-audio/" rel="nofollow noopener" target="_blank">日本語の解説記事</a>。図が直感的で非常にわかりやすいです。<br>
参考：<a href="https://github.com/ibab/tensorflow-wavenet" rel="nofollow noopener" target="_blank">tensorflow実装</a>。<br>
参考：<a href="https://github.com/tomlepaine/fast-wavenet" rel="nofollow noopener" target="_blank">高速化した実装</a>。生成処理を高速化した手法の実装です。<br>
参考：<a href="https://github.com/monthly-hack/chainer-wavenet" rel="nofollow noopener" target="_blank">chainer実装</a>。</p>

<hr>

<h4>
<span id="soundnet-learning-sound-representations-from-unlabeled-video201610" class="fragment"></span><a href="#soundnet-learning-sound-representations-from-unlabeled-video201610"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1610.09001" rel="nofollow noopener" target="_blank">SoundNet: Learning Sound Representations from Unlabeled Video</a>(2016/10)</h4>

<p>何のラベルも付与されていない動画の音声から、音声認識のための高品質な表現を獲得する手法を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/dd91105f-6afc-211c-486f-e8c3bd3f1e0a.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/dd91105f-6afc-211c-486f-e8c3bd3f1e0a.jpeg" alt="SoundNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1610.09001v1.pdf" rel="nofollow noopener" target="_blank">SoundNet: Learning Sound Representations from Unlabeled Video</a></p>

<p>SoundNetでは、ラベル無し動画の音声と画像をそれぞれ独立したCNNに入力します。<br>
画像を処理するCNNは、ImageNetで訓練された「画像中の物体を認識するCNN」とPlaces2で訓練された「画像のScene(撮影場所?)を認識するCNN」です。<br>
音声を処理するCNN(SoundNet)は、畳み込み層とプーリング層のみで構成された8層のものです。8層目は二又に分かれており、それぞれ画像中の物体と撮影場所を表現する値を出力します。全結合層を利用しないことで、可変長のデータにも対応できるようになります。<br>
画像のCNNの結果とSoundNetの結果とのKL-divergenceを最小化するように、SoundNetの訓練を行います。</p>

<p>評価では、訓練後のSoundNetから出力される表現を用いて音声(楽器)認識のタスクを行い、既存手法を上回る正答率を示しています。<br>
また、動画の音声のみから動画中に映る物体とその撮影場所を予測しています。</p>

<p>高精度な画像認識技術が確立しているからこその手法だと感じました。<br>
非常にシンプルなアプローチですが、画像認識のモデルを転用して高品質な音声表現を獲得できているのがすごいと思います。</p>

<p>参考：<a href="http://projects.csail.mit.edu/soundnet/" rel="nofollow noopener" target="_blank">プロジェクトページ</a>。様々なデモが見られます。また、実装と訓練済みモデル、使用したデータセットが公開されています。</p>

<hr>

<h2>
<span id="自然言語処理系" class="fragment"></span><a href="#%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B3%BB"><i class="fa fa-link"></i></a>自然言語処理系</h2>

<p>2016年は機械翻訳が大きく進歩した年だったと思います。</p>

<h3>
<span id="言語モデル" class="fragment"></span><a href="#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB"><i class="fa fa-link"></i></a>言語モデル</h3>

<p>コンピュータに「言葉らしさ」を判定させる重要な研究分野です。</p>

<hr>

<h4>
<span id="pointer-sentinel-mixture-models20169" class="fragment"></span><a href="#pointer-sentinel-mixture-models20169"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1609.07843" rel="nofollow noopener" target="_blank">Pointer Sentinel Mixture Models</a>(2016/9)</h4>

<p>直近の文脈に出現した未知語も利用することが出来るPointer Sentinel Mixture Modelを提案した論文です。<br>
また、言語モデルの評価のための大規模なデータセットであるWikiTextの作成・公開も行っています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/79e3f4d5-02c1-4c8f-bb29-ca08fb09bb1a.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/79e3f4d5-02c1-4c8f-bb29-ca08fb09bb1a.jpeg" alt="PointerLM.JPG"></a><br>
<a href="https://arxiv.org/pdf/1609.07843v1.pdf" rel="nofollow noopener" target="_blank">Pointer Sentinel Mixture Models Fig. 1.</a></p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/11a0f9cb-820a-b82e-1d8b-22ea7bcc42c4.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/11a0f9cb-820a-b82e-1d8b-22ea7bcc42c4.jpeg" alt="PointerLM2.JPG"></a><br>
<a href="https://arxiv.org/pdf/1609.07843v1.pdf" rel="nofollow noopener" target="_blank">Pointer Sentinel Mixture Models Fig. 2.</a></p>

<p>ニューラルネットを用いた言語モデルでは、Softmax層を用いて次に出現する単語の確率を予測しますが、事前に決めた語彙からしか単語を予測できないという問題点があります。つまり、語彙にない未知語は全く予測を行えません。<br>
この解決のために提案されたPointer Networkでは、以前に出現した単語にAttentionを行い、以前に出現した単語(語彙に無くても良い)も出力可能にします。<br>
この研究ではそれをさらに発展させ、次に出現する単語を「事前に定めた語彙から決定する」か「以前に出現した単語から決定する」かを、現在の文脈から判定することを行います。どちらを重視するかはMixture gate の出力gによって重み付けを行います。</p>

<p>また、この研究では言語モデルの評価用データセットであるWikiTextを作成・公開しています。このデータセットは、言語モデルの評価によく使われるPenn Treebank dataset(PTB)よりも、より現実的で多くの語彙数のあるデータセットで評価を行えるようにする目的で作成したそうです。</p>

<p>評価では、PTBとWikiTextを用いて比較実験を行っています。過去のState of the artの手法に比べて少ないパラメータ数でより良いperplexityを示しました。</p>

<p>Pointer Networkの存在を知らなかったため、未知語にも対応できるようになっていることに大変驚きました。Attentionの応用力の高さはすごいですね。</p>

<p>参考：<a href="http://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/" rel="nofollow noopener" target="_blank">WikiTextの公開ページ</a></p>

<hr>

<h3>
<span id="lightrnn-memory-and-computation-efficient-recurrent-neural-networks201610" class="fragment"></span><a href="#lightrnn-memory-and-computation-efficient-recurrent-neural-networks201610"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1610.09893" rel="nofollow noopener" target="_blank">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks</a>(2016/10)★</h3>

<p>より効率的なRNNのコンポーネント、LightRNNを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/d997435d-45f7-ca42-3886-4edfa6921939.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/d997435d-45f7-ca42-3886-4edfa6921939.jpeg" alt="LightRNN.JPG"></a><br>
<a href="https://arxiv.org/pdf/1610.09893v1.pdf" rel="nofollow noopener" target="_blank">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks Fig. 1.</a></p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/74b160c6-da24-e997-740c-d95e960f1fd8.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/74b160c6-da24-e997-740c-d95e960f1fd8.jpeg" alt="LightRNN2.JPG"></a><br>
<a href="https://arxiv.org/pdf/1610.09893v1.pdf" rel="nofollow noopener" target="_blank">LightRNN: Memory and Computation-Efficient Recurrent Neural Networks Fig. 2.</a></p>

<p>従来のRNNを用いた言語モデルは、語彙数が多くなるとパラメータの数も非常に多くなり、学習が非効率的なものになります。<br>
そこで、単語の分散表現を二種類のコンポーネントを用いたEmbeddingの複合により求めます。これにより、|V|個の単語を2√V個のベクトルで表現することができるようになり、パラメータ数の大幅な削減ができます。<br>
このアプローチでは、「どのようなベクトルのペアでどの単語を表現するようにするか(上図Fig.1.のテーブルをどのように作るか)」が重要な課題になります。この論文では、ランダムに単語を割り当てた後、"一定回数訓練を繰り返す→単語の配置を変更する"という処理を繰り返すことで単語の配置を決定しています。</p>

<p>評価では、既存手法とパラメータ数とperplexityを比較しています。BlackOut for RNNでは4.1G個のパラメータ数が必要なのに対して、LightRNNでは41M個のパラメータでこれを上回るperplexityを示しています。また、訓練時間も既存手法の半分以下となっています。</p>

<p>応用性のとても高そうなアプローチだと感じます。</p>

<hr>

<h4>
<span id="quasi-recurrent-neural-networks201611201611" class="fragment"></span><a href="#quasi-recurrent-neural-networks201611201611"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1611.01576" rel="nofollow noopener" target="_blank">QUASI-RECURRENT NEURAL NETWORKS</a>(2016/11)★(2016/11)</h4>

<p>RNN(LSTM)の機構をCNNで再現するQRNNを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/9be2e8f7-5aee-e6b9-ac09-96e9ee9864bb.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/9be2e8f7-5aee-e6b9-ac09-96e9ee9864bb.jpeg" alt="QRNN.JPG"></a><br>
<a href="https://arxiv.org/pdf/1611.01576v1.pdf" rel="nofollow noopener" target="_blank">QUASI-RECURRENT NEURAL NETWORKS Fig. 2.</a></p>

<p>単語の分散表現に対して畳み込みを行い、LSTMでいうinput, forgat, output gateに対応する値を計算します。過去の値と現在の値をマージする部分は、fo-poolingというアプローチを提案しており、Linear(行列積)を用いずにLSTMの動作を再現しています。<br>
CNNを使って行列積を使わずLSTMを再現することで、並列計算が可能になる、隠れ層の分析が容易になるなどのメリットが有ります。</p>

<p>評価では感情分類、言語モデル生成、機械翻訳のタスクで行われています。いずれのタスクも短い訓練時間でLSTMと同等かそれ以上のスコアを示しました。</p>

<p>非常に高速で動作も良好ですので、今後の応用が期待されます。</p>

<p>参考：<a href="http://metamind.io/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding/" rel="nofollow noopener" target="_blank">公式の紹介ブログ</a>。chainerコード付き。<br>
参考：<a href="http://qiita.com/icoxfog417/items/d77912e10a7c60ae680e" id="reference-e58312212676c588a856">日本語解説記事＋コード</a>。解説が背景から書かれており非常にわかりやすいです。tensorflowのコード付き。</p>

<h3>
<span id="機械翻訳" class="fragment"></span><a href="#%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3"><i class="fa fa-link"></i></a>機械翻訳</h3>

<p>GoogleがGoogle翻訳をDeepLearningベースのモデルに変更したことで話題になりました。</p>

<hr>

<h4>
<span id="googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation20169" class="fragment"></span><a href="#googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation20169"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1609.08144" rel="nofollow noopener" target="_blank">Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a>(2016/9)</h4>

<p>機械翻訳のための大規模なモデルであるGNMT(Google's Neural MAchine Translation)を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/d55b0bc0-7ac4-67c3-8c0c-69e1617d876f.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/d55b0bc0-7ac4-67c3-8c0c-69e1617d876f.jpeg" alt="GNMT.JPG"></a><br>
<a href="https://arxiv.org/pdf/1609.08144v2.pdf" rel="nofollow noopener" target="_blank">Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Fig. 1.</a></p>

<p>GNMTはEncoder-Decoder型のモデルです。Encoder、Decoder共に8層のLSTM(Encoder側は最初の層だけBi-directional)を用いており、Decoderへの入力はEncoderの出力から計算したAttentionを用いています。非常に深いモデルのため、Residual Connectionを活用しています。<br>
使われている手法はいずれも特に新しいものではありませんが、とにかくモデルがでかい、深い。このモデルの訓練方法を提案したことが大きな貢献かと思われます。</p>

<p>評価では、BLUEを用いてGNMTが既存手法より良い精度であることを示しています。</p>

<p>ここまでするのか、と思いました。</p>

<p>参考：<a href="http://smerity.com/articles/2016/google_nmt_arch.html" rel="nofollow noopener" target="_blank">英語の解説記事</a>。GNMTを理解するために非常に良い記事です。</p>

<p><strong>+αの話題</strong><br>
GNMTで作成した中間表現を用いることでZero-Shot な翻訳(A⇔B、B⇔Cの翻訳モデルを訓練し組み合わせることでA⇔Cを翻訳)がある程度行えると報告されています。非常に興味深い話題です。<br>
参考：<a href="https://arxiv.org/abs/1611.04558" rel="nofollow noopener" target="_blank">Zero-Shot Translationの論文</a><br>
参考：<a href="https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html" rel="nofollow noopener" target="_blank">英語の紹介記事</a><br>
参考：<a href="http://jp.techcrunch.com/2016/11/23/20161122googles-ai-translation-tool-seems-to-have-invented-its-own-secret-internal-language/" rel="nofollow noopener" target="_blank">日本語の紹介記事</a></p>

<hr>

<h4>
<span id="neural-machine-translation-in-linear-time201610" class="fragment"></span><a href="#neural-machine-translation-in-linear-time201610"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1610.10099" rel="nofollow noopener" target="_blank">Neural Machine Translation in Linear Time</a>(2016/10)</h4>

<p>Dilated Convolution Networkを用いた機械翻訳モデルであるByteNetを提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/a85ebf72-2804-ecc2-72b3-db3745e91ef3.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/a85ebf72-2804-ecc2-72b3-db3745e91ef3.jpeg" alt="ByteNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1610.10099v1.pdf" rel="nofollow noopener" target="_blank">Neural Machine Translation in Linear Time Fig. 1.</a></p>

<p>ByteNetもEncoder-Decoder型のモデルです。どちらにもDilated Convolution Networkを用いており、より多くの文脈の情報を用いて翻訳を行うことができるようになります。<br>
Decoder側のネットワークは過去から未来の方向の情報は使っていません(Mask Convolutionというそうです)。</p>

<p>評価では既存手法と負の対数尤度やBLUEの尺度で比較をしています。GNMTなどには負けていますが、Attentionを用いたRNNによるEncoder-Decoder型のモデルよりも高い精度を示しました。</p>

<p>機械翻訳など言語処理系のニューラルネットはRNNを用いるアプローチが多いですが、QRNNやByteNetなど畳み込みを用いたアプローチでも十分に使えることがわかりました。<br>
今後の主流となっていくのでしょうか？</p>

<p>参考：<a href="http://qiita.com/YoshikawaMasashi/items/188476ea29141d992354" id="reference-46e69926697b5d8317ef">日本語解説記事</a><br>
参考：<a href="https://github.com/buriburisuri/ByteNet" rel="nofollow noopener" target="_blank">実装その1</a><br>
参考：<a href="https://github.com/paarthneekhara/byteNet-tensorflow" rel="nofollow noopener" target="_blank">実装その2</a><br>
どちらもtensorflow実装です。</p>

<h2>
<span id="強化学習系" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B3%BB"><i class="fa fa-link"></i></a>強化学習系</h2>

<p>DeepMindが強すぎます。</p>

<hr>

<h4>
<span id="mastering-the-game-of-go-with-deep-neural-networks-and-tree-search" class="fragment"></span><a href="#mastering-the-game-of-go-with-deep-neural-networks-and-tree-search"><i class="fa fa-link"></i></a><a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html" rel="nofollow noopener" target="_blank">Mastering the game of Go with deep neural networks and tree search</a>
</h4>

<p>深層強化学習を用いて"碁"のプレイを学習する手法を提案した論文(Natureに掲載)です。"AlphaGo"という名前で有名になりました。</p>

<p>このシステムは膨大なデータと訓練時間が必要になるいくつものステップを経て構築されています。おおよその手順は以下の通り。</p>

<ol>
<li>大量の盤面データ(多い場合で3000万)を用いて、ある盤面の次の一手を予測するSL policy networkを訓練する</li>
<li>1.で作成したネットワーク同士を自己対戦させて強化学習を行い、より良い次の一手を予測するRL policy networkを訓練する。</li>
<li>1.と2.で作成したネットワークを用いて訓練データ(3000万盤面)を自動生成し、盤面の良し悪しを評価するvalue netwrokを訓練する。</li>
</ol>

<p>実戦では、上記の手順で訓練したネットワークに、将棋や囲碁のAIで良く用いられるモンテカルロ木探索を適用し、有効手の検索を行っています。</p>

<p>評価では、既存の様々な囲碁のプログラムと対戦し、その結果を出しています。また、その結果からプログラムの強さをレーティングで表しています。最終的には、囲碁のトッププロであるイ・セドル氏を超えるレーティングを示しました。</p>

<p>参考：<a href="http://blog.livedoor.jp/lunarmodule7/archives/4635352.html" rel="nofollow noopener" target="_blank">日本語の解説記事</a><br>
解説が非常に詳しいとても良い記事です。AlphaGoの訓練の途方も無さを感じられます。</p>

<hr>

<h4>
<span id="neural-architecture-search-with-reinforcement-learning201611" class="fragment"></span><a href="#neural-architecture-search-with-reinforcement-learning201611"><i class="fa fa-link"></i></a><a href="https://openreview.net/forum?id=r1Ue8Hcxg" rel="nofollow noopener" target="_blank">Neural Architecture Search with Reinforcement Learning</a>(2016/11)★</h4>

<p>あるタスクを解くためのニューラルネットのより良い構造を強化学習で獲得する手法を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/aeb9a284-91fc-2c6c-91a0-7e97e0e4e081.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/aeb9a284-91fc-2c6c-91a0-7e97e0e4e081.jpeg" alt="NNRL.JPG"></a><br>
<a href="https://openreview.net/pdf?id=r1Ue8Hcxg" rel="nofollow noopener" target="_blank">Neural Architecture Search with Reinforcement Learning Fig. 4.</a></p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/2e63045a-a52f-7c6a-6e39-d0db76690bf0.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/2e63045a-a52f-7c6a-6e39-d0db76690bf0.jpeg" alt="NNRL2.JPG"></a><br>
<a href="https://openreview.net/pdf?id=r1Ue8Hcxg" rel="nofollow noopener" target="_blank">Neural Architecture Search with Reinforcement Learning Fig. 5.</a></p>

<p>pix2pixでも述べた通り、現在は人間がタスクに合わせてニューラルネットの構造を設計しています。この研究は、ニューラルネットの構造自体もニューラルネットで設計してしまおう、という非常に挑戦的なものです。<br>
ネットワークの構造の設計にはRNNを用います。ニューラルネットの構造の要素(各種パラメータ、使用する演算や活性化関数等)を一つずつ予測し、それを1層ずつ繰り返すようなモデルになっています。上図Fig.4.はCNNの、Fig.5.はRNNの構造を設計する例です。<br>
このモデルの訓練のための報酬は、設計したニューラルネットで任意のデータセットを評価したときのAccuracyを用います。つまり、報酬の計算の為にネットワークの訓練が必要になります。当然、膨大な訓練時間がかかってしまうため、モデルを非同期で並列に訓練するためのアプローチも示しています。</p>

<p>評価ではCIFAR-10を用いた画像分類と、PTBを用いた言語モデル作成の二種類のタスクに取り組んでいます。CIFAR-10のタスクでは、DenseNetに匹敵するError rateを示しました。PTBのタスクでは過去の比較手法を上回るPerplexityを示しました。<br>
ただし、いずれのモデルも人間が設計したモデルよりも多いパラメータ数となっていました。</p>

<p>まだまだ根本的な問題は多いですが、シンギュラリティの可能性を感じました。<br>
論文内で「訓練時にはGPU800枚をぶん回した」という怖い記述があり呆然としました。やってることを考えるとこのくらいしないとだめそうですが…。</p>

<h2>
<span id="ニューラルネット応用系" class="fragment"></span><a href="#%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E5%BF%9C%E7%94%A8%E7%B3%BB"><i class="fa fa-link"></i></a>ニューラルネット応用系</h2>

<p>特定の分野に拘らずに精度や速度の向上を図る等、DeepLearning自体の可能性を追求する研究をまとめました。</p>

<h3>
<span id="汎用的なdeeplearning手法" class="fragment"></span><a href="#%E6%B1%8E%E7%94%A8%E7%9A%84%E3%81%AAdeeplearning%E6%89%8B%E6%B3%95"><i class="fa fa-link"></i></a>汎用的なDeepLearning手法</h3>

<p>汎用的というと語弊がありますが、ある特定のタスクに拘らずに使用できる手法です。</p>

<hr>

<h4>
<span id="hybrid-computing-using-a-neural-network-with-dynamic-external-memory-201610" class="fragment"></span><a href="#hybrid-computing-using-a-neural-network-with-dynamic-external-memory-201610"><i class="fa fa-link"></i></a><a href="http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz" rel="nofollow noopener" target="_blank">Hybrid computing using a neural network with dynamic external memory</a> (2016/10)★</h4>

<p>メモリ構造を持った汎用性の高いモデルであるDNC(Differentiable Neural Computers)を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/23531534-b955-d2d3-9fa4-3be1972175d9.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/23531534-b955-d2d3-9fa4-3be1972175d9.jpeg" alt="DNC.JPG"></a><br>
<a href="http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz" rel="nofollow noopener" target="_blank">Hybrid computing using a neural network with dynamic external memory Fig. 1.</a></p>

<p>こちらの研究の概要紹介は他の記事を参照ください。<br>
<del>力尽きました</del>…。</p>

<p>参考：<a href="https://deepmind.com/blog/differentiable-neural-computers/" rel="nofollow noopener" target="_blank">Deepmindの紹介記事</a><br>
参考：<a href="https://www.evernote.com/shard/s2/sh/77999024-c730-477c-9e06-5b66affb4134/e88f608704525afa" rel="nofollow noopener" target="_blank">日本語解説記事1</a><br>
参考：<a href="http://qiita.com/yos1up/items/599ff75c876f6f94d249" id="reference-28f4cfdc61d6c20fec09">日本語解説記事2+chainer実装</a><br>
参考：<a href="http://qiita.com/dblN/items/2ab6e0af244e56a7cb4a" id="reference-f4535799f28f13946a66">日本語解説記事3(未完)</a></p>

<h3>
<span id="最適化アルゴリズム" class="fragment"></span><a href="#%E6%9C%80%E9%81%A9%E5%8C%96%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0"><i class="fa fa-link"></i></a>最適化アルゴリズム</h3>

<p>いずれもDeepLearningに必要不可欠な要素です。<br>
多くの研究が出ていますが、特にインパクトのあったものをまとめます。</p>

<hr>

<h4>
<span id="learning-to-learn-by-gradient-descent-by-gradient-descent" class="fragment"></span><a href="#learning-to-learn-by-gradient-descent-by-gradient-descent"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1606.04474" rel="nofollow noopener" target="_blank">Learning to learn by gradient descent by gradient descent</a>
</h4>

<p>ニューラルネットの訓練にどの最適化アルゴリズムを使用するかを学習する手法を提案した論文です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/5c00a4b1-fbb8-5b63-260e-eb7fb85787e4.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/5c00a4b1-fbb8-5b63-260e-eb7fb85787e4.jpeg" alt="LtoL.JPG"></a><br>
<a href="https://arxiv.org/pdf/1606.04474v2.pdf" rel="nofollow noopener" target="_blank">Learning to learn by gradient descent by gradient descent Fig. 2.</a> (2016/6)</p>

<p>この手法ではLSTMを用いたRNNを使用します。図の二層目(Optimizer)でRNNを用いて最適化のための最適なパラメータ（の差分）を予測します。一層目（Optimizee）で実際にそのパラメータを使用して勾配の計算を行います。<br>
Optimizerのネットワークを訓練することで、どのような場合にどのようにパラメータを更新すれば良いのか、ということをネットワークに学習させることが目的です。</p>

<p>評価ではCIFAR-10などのタスクを解く際に、様々な最適化アルゴリズムを用いてネットワークの訓練を行い、lossの下がり方の比較を行っています。提案手法が最も安定してlossを最小化できることを示しています。</p>

<p>どのような誤差関数でもこの手法が適用できるかが気になりました。</p>

<p>※12/18 追記<br>
参考：<a href="https://github.com/deepmind/learning-to-learn" rel="nofollow noopener" target="_blank">tensorflow実装</a></p>

<hr>

<h4>
<span id="improving-stochastic-gradient-descent-with-feedback-201611" class="fragment"></span><a href="#improving-stochastic-gradient-descent-with-feedback-201611"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1611.01505" rel="nofollow noopener" target="_blank">Improving Stochastic Gradient Descent with Feedback</a> (2016/11)★</h4>

<p>目的関数からのフィードバックを用いる最適化アルゴリズム、Eveを提案した論文です。</p>

<p>最適化アルゴリズムについて、まだキチンと理解できていないため、概要は省略します。申し訳ありません…。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/9ddc15d3-90ca-b38b-07d7-87ab11d43dc0.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/9ddc15d3-90ca-b38b-07d7-87ab11d43dc0.jpeg" alt="Eve.JPG"></a><br>
<a href="https://arxiv.org/pdf/1611.01505v2.pdf" rel="nofollow noopener" target="_blank">Improving Stochastic Gradient Descent with Feedback Fig. 1.</a></p>

<p>評価ではCIFAR-10,100の訓練に様々な最適化アルゴリズムを用いて、提案手法が最も安定して訓練ができたことを示しています。</p>

<p>ニュールネットの訓練のための新しいデファクトスタンダードになりそうです。<br>
私事ですが、<strong>私はこの手法の名前が大好きです。</strong></p>

<p>参考：<a href="https://github.com/jayanthkoushik/sgd-feedback" rel="nofollow noopener" target="_blank">tensorflow実装</a><br>
参考：<a href="https://github.com/pfnet/chainer/pull/1847" rel="nofollow noopener" target="_blank">chainerでのPR</a>。私はchainerを主に使っているため、マージが待たれます。</p>

<h3>
<span id="ニューラルネットの圧縮高速化" class="fragment"></span><a href="#%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%9C%A7%E7%B8%AE%E9%AB%98%E9%80%9F%E5%8C%96"><i class="fa fa-link"></i></a>ニューラルネットの圧縮・高速化</h3>

<p>サイズが大きくなりがちなDeepLearningのモデルを精度を保ちつつ圧縮する技術は、モバイル端末への適用や訓練等の省力化に関わる重要なものだと感じます。</p>

<hr>

<h4>
<span id="binarized-neural-networks-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or--1-20162" class="fragment"></span><a href="#binarized-neural-networks-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or--1-20162"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1602.02830" rel="nofollow noopener" target="_blank">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</a> (2016/2)</h4>

<h4>
<span id="xnor-net-imagenet-classification-using-binary-convolutional-neural-networkshttpsarxivorgabs160305279" class="fragment"></span><a href="#xnor-net-imagenet-classification-using-binary-convolutional-neural-networkshttpsarxivorgabs160305279"><i class="fa fa-link"></i></a><a href="2016/3">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</a>(<a href="https://arxiv.org/abs/1603.05279" class="autolink" rel="nofollow noopener" target="_blank">https://arxiv.org/abs/1603.05279</a>)</h4>

<p>非常に似ているため二本続けて。</p>

<p>１本目はモデルのパラメータをすべて+1と-1のみで表現するBinaryNetを提案した論文です。一つのパラメータを1bitで表現できるため、メモリ使用量が大幅に削減できます。また、掛け算器をXNORに置き換えることができ、処理の高速化ができます。2016/2月に公開。</p>

<p>２本目はBinaryNetを改良して精度と計算速度を向上させたXNOR-Netを提案した論文です。内容はあまり理解できていないため、参考記事を参照してください…。2016/3月に公開。</p>

<p>評価では、XNOR-Netを用いることでAlexNetと比較して画像分類タスクの精度が11%落ちるかわりに、メモリ使用量が1/64（AlexNetが倍精度の場合）、計算速度が58倍(CPUを用いた場合)となることを示しました。</p>

<p>パラメータが二値のみでもある程度予測が行えることがすごいと思いました。<br>
訓練後のフィルタがテトリスっぽいと思いました。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/110468/58544eb4-a869-27f0-6c62-e7c718491867.jpeg" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/110468/58544eb4-a869-27f0-6c62-e7c718491867.jpeg" alt="BinaryNet.JPG"></a><br>
<a href="https://arxiv.org/pdf/1602.02830v3.pdf" rel="nofollow noopener" target="_blank">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 Fig. 2.</a></p>

<p>参考：<a href="http://qiita.com/supersaiakujin/items/81719e49a50a3fb653e8" id="reference-dee14661287b6b1d0baa">BinaryNet 日本語解説記事</a><br>
参考：<a href="https://github.com/MatthieuCourbariaux/BinaryNet" rel="nofollow noopener" target="_blank">BinaryNet 公式実装</a><br>
参考：<a href="https://github.com/hillbig/binary_net" rel="nofollow noopener" target="_blank">BinaryNet chainer実装</a></p>

<p>参考：<a href="http://qiita.com/supersaiakujin/items/6adaf9731c9475891911" id="reference-df875869dc5bb4f0d08f">XNOR-Net 日本語解説記事1</a><br>
参考：<a href="https://tkng.org/b/2016/03/28/xnor-network/" rel="nofollow noopener" target="_blank">XNOR-Net 日本語解説記事2</a></p>

<hr>

<h4>
<span id="squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-05mb-model-size" class="fragment"></span><a href="#squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-05mb-model-size"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1602.07360" rel="nofollow noopener" target="_blank">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</a>
</h4>

<p>非常に省メモリなネットワークであるSqueezeNetを提案した論文です。<br>
BinaryNetと同時期に出た研究ですが、こちらは畳み込み層のフィルタサイズやチャネル数を変更するなどしてパラメータの削減を試みています。</p>

<p>評価ではImageNetを用いた画像認識のタスクに取り組み、AlexNetと比較して同程度の精度を保ちつつメモリサイズを最大1/461(0.5MB)に出来ることを示しました。</p>

<p>参考：<a href="http://qiita.com/supersaiakujin/items/ece1e20ca4073e77bed7" id="reference-d277631ee027d0f402b0">日本語解説記事</a><br>
参考：<a href="https://github.com/DeepScale/SqueezeNet" rel="nofollow noopener" target="_blank">Caffe実装</a></p>

<h1>
<span id="おわりに" class="fragment"></span><a href="#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB"><i class="fa fa-link"></i></a>おわりに</h1>

<p>本当はまだまだ書きたい内容があったのですが、分量と体力の関係で断念しました…。<br>
改めて見返してみると、とても趣味の出ている選択になってしまったと反省しています。</p>

<p>この記事を書いている最中にも新しい話題がどんどん出てくるのがこの分野の怖いところです。<br>
最先端の手法は中々追えないですが、今後も頑張ってキャッチアップして話題についていきたいです。</p>
<div class="hidden"><form class="js-task-list-update" action="/eve_yk/items/f4b274da7042cba1ba76" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="bVprhYiZO7hHsHhJ+MBlKtfZdTqEg5ExbBoaqs04JONLLXpcV6/f1BQIRtva/V3Dc7ig9CebHA2Zg6EUYoy8rw==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1482630237" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
[DeepLearning Advent Calendar 2016](http://qiita.com/advent-calendar/2016/deeplearning)の17日目の記事です。

# はじめに
はじめまして。
[Liaro](http://company.liaro.me/)という会社でエンジニアをしている@eve_ykと申します。

今年もあと僅かとなりました。
ここらで、今年のDeepLearningの主要な成果を振り返ってみましょう。

この記事は、2016年に発表されたDeepLearning関係の研究を広く浅くまとめたものです。今年のDeepLearningの研究の進歩を俯瞰するのに役立てば幸いです。
それぞれの内容について、その要点や感想なんかを簡単にまとめられたらと思います。
特に重要だと思った研究には★マークをつけておきます。
非常に長くなってしまったため、興味のある分野だけ読んでいただければと思います。

##### 言い訳とお願い
* 見つけたものはコードへのリンクも示すので、プログラミングに関係ある記事ということで…
* 分野的にかなり偏ったまとめになりましたが、ご容赦ください。
* まとめを始めた時期の関係で、2016年の研究の中でも比較的新しいものが多い点もご容赦ください
* 注意はしておりますが、内容の正確性は無保証です。誤りや不適切な点がありましたら、ご指摘いただけると幸いです。
* 他に重要なものがあれば「こんな研究もあるよ！」という紹介をしていただけるととても嬉しく思います。


## まとめのまとめ (※12/24 追記)
年末ということで、この記事に以外にもたくさんのまとめ記事が投稿されています。
どれも素晴らしいまとめですので、これらもご覧ください。

#### [2016年のディープラーニング論文100選]  (http://qiita.com/sakaiakira/items/9da1edda802c4884865c)  
より幅広い分野の論文がまとめられています。
本記事で扱っていない分野に興味がある人も、この記事を読めば満足するはずです。

#### [foobarNet: ディープラーニング関連の○○Netまとめ](http://qiita.com/shinya7y/items/8911856125a3109378d6#_reference-a60de5539cc2a2dd8bd7)
これまで発表された◯◯Netの総まとめです。
圧倒的な情報量で、ひとつずつ眺めていくだけでいくらでも時間が潰せそうです。

#### [2016年の深層学習を用いた画像認識モデル](http://qiita.com/aiskoaskosd/items/59c49f2e2a6d76d62798)
近年の画像認識モデルのまとめ＋実装＋比較＋解説です。
ほぼすべてのモデルにchainer実装があります。画像認識は本記事よりもこちらを読んだほうが良いです。

#### [Deep Learningの理論的論文リスト【随時更新】](http://mathetake.hatenablog.com/entry/2016/12/20/005632)
深層学習の理論的に解析した論文のまとめです。
深層学習そのものを理解するために行われている様々な解析が、非常に参考になります。

## 画像処理系
画像の分野では、昨年に引き続き画像認識に関する研究が人気だと感じました。
また、GANの登場により画像生成系の研究も非常に盛んであったと思います。

### ResNetの進化
2015年にMSRAが提案したResidual Networks(ResNet)は、より深いニューラルネットワークを訓練することの出来るアーキテクチャです。シンプルな改良で大きな成果を上げたこのモデルについて、様々な応用がなされました。
ResNet自体の解説は以下の記事を参考ください。
参考：[MSRAの解説スライド](http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)
参考：[日本語の解説記事1](http://qiita.com/supersaiakujin/items/935bbc9610d0f87607e8)
参考：[日本語の解説記事2](https://deepage.net/deep_learning/2016/11/30/resnet.html)
  
--- 

#### [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382) (2016/5)★
ResNetの訓練時に、ランダムに幾つかの層をskip(ResNetにおけるidentity skip connection pathのみ残す)させる手法を提案した論文です。

![StocasticDepth.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/a68ad0cc-e940-9fe7-a995-741472999eb2.jpeg)
[Deep Networks with Stochastic Depth Fig. 2.](https://arxiv.org/pdf/1603.09382v3.pdf)

CIFAR-100等の画像分類のタスクにおいて、精度が向上することを示しました(2016/3時点のSOTA?)。また、訓練時の計算量が減るため、訓練時間が短縮されることもメリットになります。
Deep Neural Networkの訓練によく使用されるDropoutに似た考え方の手法だと思います。

参考：[日本語のサーベイ記事](http://qiita.com/supersaiakujin/items/eb0553a1ef1d46bd03fa)
  
---  

#### [FractalNet: Ultra-Deep Neural Networks without Residuals](https://arxiv.org/abs/1605.07648) (2016/5)
(ResNetの仲間ではありませんが…)
複数の階層の深さが異なる畳み込み層の結果を結合するFractal Netを提案した論文です。以下の図のようなアーキテクチャです。

![FractalNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/a6a7e4b1-6129-7d03-dd46-420bbf484080.jpeg)
[FractalNet: Ultra-Deep Neural Networks without Residuals Fig. 1.](https://arxiv.org/pdf/1605.07648v1.pdf)

&quot;Deep Networks with Stochastic Depth&quot;のようにいくつかの層をskipする訓練方法も示しています。

![FractalNet2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/db5a0b7f-1eff-cb96-1ecc-f98484d65e5a.jpeg)
[FractalNet: Ultra-Deep Neural Networks without Residuals Fig. 2.](https://arxiv.org/pdf/1605.07648v1.pdf)

CIFAR-100等の画像分類のタスクにおいて、精度が向上することを示しました(2016/5時点のSOTA)。

&quot;without Residuals&quot;とありますが、多くの層を飛ばす図中の左端の経路の部分が、ResNetでいうidentity skip connection pathに似た働きをするのではないかと思います。
  
--- 

#### [Residual Networks of Residual Networks: Multilevel Residual Networks](https://arxiv.org/abs/1608.02908) (2016/8)
ResNetにおいて、複数のResidual Blockをまたがるshortcutを追加したResidual Networks of Residual Networks(RoR)を提案した論文です。

![RoR.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/80622ff1-0d6a-cde8-8ddc-a05f9d2fcb53.jpeg)
[Residual Networks of Residual Networks: Multilevel Residual Networks Fig. 1.](https://arxiv.org/pdf/1608.02908v1.pdf)

CIFAR-100等の画像分類のタスクにおいて、ResNetと比較して精度が向上することを示しました。
  
--- 

#### [Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993) (2016/8)★
複数の畳み込み層が密に結合するdense blockを取り入れたDenseNetを提案した論文です。

![DenseNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/1490aa72-af1f-12de-8920-4d8abeeaf38d.jpeg)
[Densely Connected Convolutional Networks Fig. 1.](https://arxiv.org/pdf/1608.06993v3.pdf)

CIFAR-100等の画像分類のタスクにおいて、ResNetと比較して精度が向上することを示しました (こればっかり…)(2016/8時点のSOTA?)。

構造といい、アイデアがRoRとほとんど同じに感じます…。
論文を詳しく読んでいないので詳しくはわからないのですが、どこに違いがあるのでしょうか？
同じ時期に似たアイデアの論文が出ているところに、競争率の高さを感じます。

参考：[DenseNetの実装まとめ](https://github.com/liuzhuang13/DenseNet)。公式の実装はCaffeとTorchですが、Tensorflow, Lasagne, Keras Chainerの実装へのリンクもあります。
  
--- 

#### [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431) (2016/11)
ResNetのResidual Blockを改良したResNeXtを提案した論文。画像分類のコンペティションであるILSVRC 2016で２位となったモデルです。

Residual Block内の畳み込み層を並列にしている？

![ResNeXt.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/4f456122-c042-1244-1149-e811d48aa0d0.jpeg)
(https://arxiv.org/pdf/1611.05431v1.pdf)


DenseNetもResNeXtもFacebook AI Researchの論文です。


参考：[モデルの実装(MXNet)](https://github.com/dmlc/mxnet/blob/master/example/image-classification/symbol/resnext.py)。訓練済みのモデルは[ここ](http://data.dmlc.ml/models/imagenet/resnext/)で公開されています。
  
--- 


### 画像生成
昨年末にDCGANが登場してから、非常に盛り上がっている分野だと感じます。
生成系は私にはとても理解できないので、要点のまとめもふわふわしています。
  
--- 


#### [Pixel Recurrent Neural Networks](https://arxiv.org/abs/1601.06759)(2016/1)★
あるピクセルの値をその周囲のピクセルから予測して画像生成を行うPixel RNNを提案した論文です。

![PixelRNN.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/e953cf41-aa24-66c5-b9f0-f53d5e33d984.jpeg)
[Pixel Recurrent Neural Networks Fig. 1.](https://arxiv.org/pdf/1601.06759v3.pdf)

この手法では、画像の左上のピクセルから右のピクセルを見ていき、右端まで見たら次の段の一番左のピクセルへ…という順にピクセルを順に見ていきます。あるピクセル値の予測には、これまでに見たピクセルの&quot;文脈&quot;を利用します。
様々な&quot;文脈&quot;の使い方を提案・検証したこと、モデルの並列化方法を提案したことなどが貢献になります。

評価では、MNISTやCIFAR-10のデータセットの画像生成を行い、その対数尤度を比較しています。

このモデルのような、「ある時点の値を予測するためにその周囲（それ以前）の値を利用するモデル」のことを自己回帰モデルと言うそうです(間違っていたら指摘お願いします…)。
この自己回帰モデルは画像以外の分野でも成功しています。詳しくは後述します。

参考：[日本語解説記事](http://kusanohitoshi.blogspot.jp/2016/10/pixel-recurrent-neural-network.html)。この記事の内容はもちろん、記事中で引用されているスライドも非常にわかりやすいです。
参考：[tensorflow実装](https://github.com/carpedm20/pixel-rnn-tensorflow)

  
--- 


#### [Image-to-Image Translation with Conditional Adversarial Nets](https://arxiv.org/abs/1611.07004)(2016/11)★
従来はタスクごとに独自のアプローチを検討する必要があった画像対画像変換について、汎用的なモデルであるConditional GANを提案し検証している論文です。通称pix2pix。

![pix2pix.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/33c5bc87-c453-99b7-3b23-632ddec641cb.jpeg)
[Image-to-Image Translation with Conditional Adversarial Nets Fig. 2.](https://arxiv.org/pdf/1611.07004v1.pdf)

Conditional GANの訓練方法は上図の通り。
Positive example(実際の画像)を与えるときは、変換前と変換後の画像のペアを与えて、実際の画像かどうかをDiscriminatorに解かせます。Negative example(pix2pixが生成する画像)を与えるときは、変換前の画像をGeneratorに入力して生成した画像と変換前の画像のペアを与えて、実際の画像かどうかをDiscriminatorに解かせます。
また、よりリアルな画像生成のために、L1正規化やPatchGANといった工夫を取り入れているそうです。より詳細な解説は参考の解説記事を参照ください。

評価では、「航空画像→地図画像」、「灰色画像→カラー画像」、「日中の画像→夜間の画像」「線画→カラー画像」など様々な画像変換について、アンケート評価やFCN-scoreという尺度で行っています。

![pix2pix2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/d5fffcdb-2546-00f6-5241-07252af2b532.jpeg)
[Image-to-Image Translation with Conditional Adversarial Nets Fig. 1.](https://arxiv.org/pdf/1611.07004v1.pdf)

個々のタスクでは独自に作成したモデルに負けていますが、汎用的に様々な画像変換を行える柔軟性はすごすぎます。

参考：[プロジェクトページ](https://phillipi.github.io/pix2pix/)。公式の実装(Torch)へのリンクもあります。
参考：[日本語の解説・検証記事](http://qiita.com/hiromu1996/items/38f1bd5a78336fa8ca25)。より詳しい解説があります。
参考：[chainer実装](https://github.com/mattya/chainer-pix2pix)
  
--- 

#### [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242)(2016/12)
GANを二段階に分けることで高画質な画像生成を実現するStackGANを提案した論文です。

![StackGAN.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/8050f0d4-ca26-e0ed-18aa-a29c0701e932.jpeg)
[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks Fig. 1.](https://arxiv.org/pdf/1612.03242v1.pdf)

pix2pixは「画像→画像」の変換についての研究でしたが、StackGANは「テキスト→画像」の変換について取り組んだものです。
最も特徴的なのは生成過程を二段階にしたところで、初めに低解像度(64x64)の画像を生成し、その画像を高画質(256x256)な画像生成のためのGeneratorの入力として与えます。
その他の詳細についてはまだ追えていません…。

評価では、人間に対するアンケートに加えて、Inception scoresという指標を用いて行っています。

![StackGAN2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/bc9d4bc5-332a-4bbd-3ba9-b7da3f5ba874.jpeg)
[StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks Fig. 3.](https://arxiv.org/pdf/1612.03242v1.pdf)

「２つのGANを積み重ねて上手くいくなら、もっとたくさん積み重ねればもっと良いものができるのでは？」と誰もが考えますが、より[多層なGAN](https://arxiv.org/abs/1612.04357)がこの論文の3日後(12/13)に公開されました。タスクが異なっており、また引用もされていないので別のチームが似たアイデアを同時期に出したもの思われますが、なんという競争率の高さ…。


## 動画処理系
生成系のタスクが動画でも成功(?)し始めたことがこの分野で一番大きな出来事でしょうか？

### 動画生成
こいつ…動くぞ！
  
--- 


#### [Deep Predictive Coding Networks for Fideo Prediction and Unsupervised Learning](https://arxiv.org/abs/1605.08104)(2016/5)★
ある時刻の画像から次の時刻の画像を予測するPredNetを提案した論文です。（厳密には動画生成ではないかもしれません…）

![PredNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/9d28077e-e495-744d-ea54-87b850aeaa9f.jpeg)
[Deep Predictive Coding Networks for Fideo Prediction and Unsupervised Learning Fig. 1.](https://arxiv.org/pdf/1605.08104v4.pdf)

PredNetは[ConvLSTM](https://arxiv.org/pdf/1506.04214.pdf)というアーキテクチャ(図の緑色のブロック)を用いて中間表現を生成します。その中間表現を用いて1時刻後の画像を予測(図の青色のブロック)します。予測結果と実際の画像のL1lossを計算(図の赤色のブロック)し、出力とします(次の時刻の予測が出力ではない)。また、この誤差は次の時刻の画像の予測に用います。
処理の流れは、公式のページにある[こちらの動画](https://coxlab.github.io/prednet/prednet_animation.html)がわかりやすいです。

実験では、自作した3Dモデルが回転する映像や、車載カメラの映像を用いて画像の予測を行い結果を可視化して結果の検討を行っています。可視化結果の具体例は[公式のページ](https://coxlab.github.io/prednet/)から確認できます。

教師なし訓練ができる点が面白く感じました。
[モデルの性質が脳の新皮質に類似している](https://wba-meetup.doorkeeper.jp/events/46611)そうで、わりと話題になったと記憶しています。

参考：[日本語の解説記事](http://karasunoblog.blog20.fc2.com/blog-entry-46.html)。わかりやすい良記事です。
参考：[日本語の検証報告スライド](http://www.slideshare.net/kojiochiai/pred-net)
参考：[keras実装](https://github.com/coxlab/prednet)
参考：[chainer実装](https://github.com/quadjr/PredNet)
  
--- 


#### [Generating Videos with Scene Dynamics](http://web.mit.edu/vondrick/tinyvideo/paper.pdf)
GANを用いて画像から1秒程度の動画を生成する手法を提案した論文です。

![GenerateVideo.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/647f944e-be1f-395f-7546-7fd9b883857c.jpeg)
[Generating Videos with Scene Dynamics Fig. 1.](http://web.mit.edu/vondrick/tinyvideo/paper.pdf)

モデルの入力となるのは100次元のノイズのベクトルです。モデルは二つのストリームから成っており、一つは画像の前景の時刻による変化を、もうひとつは画像の背景をそれぞれ生成します。前景を処理するストリームでは画像のマスクも生成し、それらの結果を組み合わせて最終的な短時間の動画を生成します。前景の処理では&quot; fractionally-strided spatio-temporal convolutions&quot;(要調査…)、背景の処理では&quot;fractionally-strided spatial convolutions&quot;を用いているそうです。

モデルの訓練時は、前景を処理するストリーム部分の入出力を反転した構造(最終層だけ0or1のバイナリを出力するレイヤーに変更)のDiscriminator Networkを構築し、GANの要領で訓練を行います。

評価では、「ベースライン手法の生成した動画(または実際の動画)と提案手法で生成した動画のどちらがよりリアルか」をアンケートしています。
ベースライン手法と比較したときは平均して82%の人が、実際の動画と比較すると平均して18%の人が、「提案手法の動画がよりリアルだ」と答えたそうです。

画像中の動きそうな部分（前景）と動かない部分(背景)を分けて訓練したところが賢いと感じました。(アンケートの結果では大きな差は出ていませんでしたが…)

参考：[プロジェクトページ](http://web.mit.edu/vondrick/tinyvideo/)
  
--- 


## 音声処理系
詳しくない分野ですので、門外漢の私の耳にも届いたものでお茶を濁します。
  
--- 


#### [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/abs/1609.03499) (2016/9)★
自己回帰モデルを用いて音声を生成するWaveNetを提案した論文です。

![Wavenet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/6eb22b79-ebc3-0c2f-afbb-ac513f75d064.jpeg)
[WaveNet: A Generative Model for Raw Audio Fig. 3.](https://arxiv.org/pdf/1609.03499v2.pdf)

WaveNetは、ある時刻の波形(の値)をそれ以前の波形データから予測する自己回帰モデルです。より古い情報も効率よく利用するために、Dilated Convolutionという手法を使っています。また、Dilated Convolutionを利用すると非常に深いネットワークになるため、Residual Networksでネットワークを構築しています。

評価では、比較手法とWaveNetで生成した音声のどちらが自然かアンケートを取っています。結果は比較手法を大きく上回りました。また、音声認識のタスクも評価し、TIMITというコーパスで最も良いスコアを示しました。

自己回帰モデルの一番の成功例と感じます。
自己回帰モデルの欠点として、生成に時間がかかる(WaveNetの場合、1秒の音声を生成するのに90分かかるそうです)ことがありますが、その[欠点を解消した手法](https://arxiv.org/abs/1611.09482)も登場しています。進歩が速すぎます。

参考：[Deepmindの紹介記事](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)。デモが聞けます。またDilated Convolutionの動作がわかりやすいgifがあります。
参考：[日本語の解説記事](http://musyoku.github.io/2016/09/18/wavenet-a-generative-model-for-raw-audio/)。図が直感的で非常にわかりやすいです。
参考：[tensorflow実装](https://github.com/ibab/tensorflow-wavenet)。
参考：[高速化した実装](https://github.com/tomlepaine/fast-wavenet)。生成処理を高速化した手法の実装です。
参考：[chainer実装](https://github.com/monthly-hack/chainer-wavenet)。
  
--- 


#### [SoundNet: Learning Sound Representations from Unlabeled Video](https://arxiv.org/abs/1610.09001)(2016/10)
何のラベルも付与されていない動画の音声から、音声認識のための高品質な表現を獲得する手法を提案した論文です。

![SoundNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/dd91105f-6afc-211c-486f-e8c3bd3f1e0a.jpeg)
[SoundNet: Learning Sound Representations from Unlabeled Video](https://arxiv.org/pdf/1610.09001v1.pdf)

SoundNetでは、ラベル無し動画の音声と画像をそれぞれ独立したCNNに入力します。
画像を処理するCNNは、ImageNetで訓練された「画像中の物体を認識するCNN」とPlaces2で訓練された「画像のScene(撮影場所?)を認識するCNN」です。
音声を処理するCNN(SoundNet)は、畳み込み層とプーリング層のみで構成された8層のものです。8層目は二又に分かれており、それぞれ画像中の物体と撮影場所を表現する値を出力します。全結合層を利用しないことで、可変長のデータにも対応できるようになります。
画像のCNNの結果とSoundNetの結果とのKL-divergenceを最小化するように、SoundNetの訓練を行います。

評価では、訓練後のSoundNetから出力される表現を用いて音声(楽器)認識のタスクを行い、既存手法を上回る正答率を示しています。
また、動画の音声のみから動画中に映る物体とその撮影場所を予測しています。

高精度な画像認識技術が確立しているからこその手法だと感じました。
非常にシンプルなアプローチですが、画像認識のモデルを転用して高品質な音声表現を獲得できているのがすごいと思います。

参考：[プロジェクトページ](http://projects.csail.mit.edu/soundnet/)。様々なデモが見られます。また、実装と訓練済みモデル、使用したデータセットが公開されています。
  
--- 


## 自然言語処理系
2016年は機械翻訳が大きく進歩した年だったと思います。

### 言語モデル
コンピュータに「言葉らしさ」を判定させる重要な研究分野です。
  
--- 


#### [Pointer Sentinel Mixture Models](https://arxiv.org/abs/1609.07843)(2016/9)
直近の文脈に出現した未知語も利用することが出来るPointer Sentinel Mixture Modelを提案した論文です。
また、言語モデルの評価のための大規模なデータセットであるWikiTextの作成・公開も行っています。

![PointerLM.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/79e3f4d5-02c1-4c8f-bb29-ca08fb09bb1a.jpeg)
[Pointer Sentinel Mixture Models Fig. 1.](https://arxiv.org/pdf/1609.07843v1.pdf)

![PointerLM2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/11a0f9cb-820a-b82e-1d8b-22ea7bcc42c4.jpeg)
[Pointer Sentinel Mixture Models Fig. 2.](https://arxiv.org/pdf/1609.07843v1.pdf)

ニューラルネットを用いた言語モデルでは、Softmax層を用いて次に出現する単語の確率を予測しますが、事前に決めた語彙からしか単語を予測できないという問題点があります。つまり、語彙にない未知語は全く予測を行えません。
この解決のために提案されたPointer Networkでは、以前に出現した単語にAttentionを行い、以前に出現した単語(語彙に無くても良い)も出力可能にします。
この研究ではそれをさらに発展させ、次に出現する単語を「事前に定めた語彙から決定する」か「以前に出現した単語から決定する」かを、現在の文脈から判定することを行います。どちらを重視するかはMixture gate の出力gによって重み付けを行います。

また、この研究では言語モデルの評価用データセットであるWikiTextを作成・公開しています。このデータセットは、言語モデルの評価によく使われるPenn Treebank dataset(PTB)よりも、より現実的で多くの語彙数のあるデータセットで評価を行えるようにする目的で作成したそうです。

評価では、PTBとWikiTextを用いて比較実験を行っています。過去のState of the artの手法に比べて少ないパラメータ数でより良いperplexityを示しました。

Pointer Networkの存在を知らなかったため、未知語にも対応できるようになっていることに大変驚きました。Attentionの応用力の高さはすごいですね。

参考：[WikiTextの公開ページ](http://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/)
  
--- 


### [LightRNN: Memory and Computation-Efficient Recurrent Neural Networks](https://arxiv.org/abs/1610.09893)(2016/10)★
より効率的なRNNのコンポーネント、LightRNNを提案した論文です。

![LightRNN.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/d997435d-45f7-ca42-3886-4edfa6921939.jpeg)
[LightRNN: Memory and Computation-Efficient Recurrent Neural Networks Fig. 1.](https://arxiv.org/pdf/1610.09893v1.pdf)

![LightRNN2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/74b160c6-da24-e997-740c-d95e960f1fd8.jpeg)
[LightRNN: Memory and Computation-Efficient Recurrent Neural Networks Fig. 2.](https://arxiv.org/pdf/1610.09893v1.pdf)

従来のRNNを用いた言語モデルは、語彙数が多くなるとパラメータの数も非常に多くなり、学習が非効率的なものになります。
そこで、単語の分散表現を二種類のコンポーネントを用いたEmbeddingの複合により求めます。これにより、|V|個の単語を2√V個のベクトルで表現することができるようになり、パラメータ数の大幅な削減ができます。
このアプローチでは、「どのようなベクトルのペアでどの単語を表現するようにするか(上図Fig.1.のテーブルをどのように作るか)」が重要な課題になります。この論文では、ランダムに単語を割り当てた後、&quot;一定回数訓練を繰り返す→単語の配置を変更する&quot;という処理を繰り返すことで単語の配置を決定しています。

評価では、既存手法とパラメータ数とperplexityを比較しています。BlackOut for RNNでは4.1G個のパラメータ数が必要なのに対して、LightRNNでは41M個のパラメータでこれを上回るperplexityを示しています。また、訓練時間も既存手法の半分以下となっています。

応用性のとても高そうなアプローチだと感じます。
  
--- 

#### [QUASI-RECURRENT NEURAL NETWORKS](https://arxiv.org/abs/1611.01576)(2016/11)★(2016/11)
RNN(LSTM)の機構をCNNで再現するQRNNを提案した論文です。

![QRNN.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/9be2e8f7-5aee-e6b9-ac09-96e9ee9864bb.jpeg)
[QUASI-RECURRENT NEURAL NETWORKS Fig. 2.](https://arxiv.org/pdf/1611.01576v1.pdf)

単語の分散表現に対して畳み込みを行い、LSTMでいうinput, forgat, output gateに対応する値を計算します。過去の値と現在の値をマージする部分は、fo-poolingというアプローチを提案しており、Linear(行列積)を用いずにLSTMの動作を再現しています。
CNNを使って行列積を使わずLSTMを再現することで、並列計算が可能になる、隠れ層の分析が容易になるなどのメリットが有ります。

評価では感情分類、言語モデル生成、機械翻訳のタスクで行われています。いずれのタスクも短い訓練時間でLSTMと同等かそれ以上のスコアを示しました。

非常に高速で動作も良好ですので、今後の応用が期待されます。

参考：[公式の紹介ブログ](http://metamind.io/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding/)。chainerコード付き。
参考：[日本語解説記事＋コード](http://qiita.com/icoxfog417/items/d77912e10a7c60ae680e)。解説が背景から書かれており非常にわかりやすいです。tensorflowのコード付き。

### 機械翻訳
GoogleがGoogle翻訳をDeepLearningベースのモデルに変更したことで話題になりました。
  
--- 


#### [Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)(2016/9)
機械翻訳のための大規模なモデルであるGNMT(Google&#39;s Neural MAchine Translation)を提案した論文です。

![GNMT.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/d55b0bc0-7ac4-67c3-8c0c-69e1617d876f.jpeg)
[Google&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation Fig. 1.](https://arxiv.org/pdf/1609.08144v2.pdf)

GNMTはEncoder-Decoder型のモデルです。Encoder、Decoder共に8層のLSTM(Encoder側は最初の層だけBi-directional)を用いており、Decoderへの入力はEncoderの出力から計算したAttentionを用いています。非常に深いモデルのため、Residual Connectionを活用しています。
使われている手法はいずれも特に新しいものではありませんが、とにかくモデルがでかい、深い。このモデルの訓練方法を提案したことが大きな貢献かと思われます。

評価では、BLUEを用いてGNMTが既存手法より良い精度であることを示しています。

ここまでするのか、と思いました。

参考：[英語の解説記事](http://smerity.com/articles/2016/google_nmt_arch.html)。GNMTを理解するために非常に良い記事です。

**+αの話題**
GNMTで作成した中間表現を用いることでZero-Shot な翻訳(A⇔B、B⇔Cの翻訳モデルを訓練し組み合わせることでA⇔Cを翻訳)がある程度行えると報告されています。非常に興味深い話題です。
参考：[Zero-Shot Translationの論文](https://arxiv.org/abs/1611.04558)
参考：[英語の紹介記事](https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html)
参考：[日本語の紹介記事](http://jp.techcrunch.com/2016/11/23/20161122googles-ai-translation-tool-seems-to-have-invented-its-own-secret-internal-language/)
  
--- 


#### [Neural Machine Translation in Linear Time](https://arxiv.org/abs/1610.10099)(2016/10)
Dilated Convolution Networkを用いた機械翻訳モデルであるByteNetを提案した論文です。

![ByteNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/a85ebf72-2804-ecc2-72b3-db3745e91ef3.jpeg)
[Neural Machine Translation in Linear Time Fig. 1.](https://arxiv.org/pdf/1610.10099v1.pdf)

ByteNetもEncoder-Decoder型のモデルです。どちらにもDilated Convolution Networkを用いており、より多くの文脈の情報を用いて翻訳を行うことができるようになります。
Decoder側のネットワークは過去から未来の方向の情報は使っていません(Mask Convolutionというそうです)。

評価では既存手法と負の対数尤度やBLUEの尺度で比較をしています。GNMTなどには負けていますが、Attentionを用いたRNNによるEncoder-Decoder型のモデルよりも高い精度を示しました。

機械翻訳など言語処理系のニューラルネットはRNNを用いるアプローチが多いですが、QRNNやByteNetなど畳み込みを用いたアプローチでも十分に使えることがわかりました。
今後の主流となっていくのでしょうか？

参考：[日本語解説記事](http://qiita.com/YoshikawaMasashi/items/188476ea29141d992354)
参考：[実装その1](https://github.com/buriburisuri/ByteNet)
参考：[実装その2](https://github.com/paarthneekhara/byteNet-tensorflow)
どちらもtensorflow実装です。

## 強化学習系
DeepMindが強すぎます。
  
--- 


#### [Mastering the game of Go with deep neural networks and tree search](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)
深層強化学習を用いて&quot;碁&quot;のプレイを学習する手法を提案した論文(Natureに掲載)です。&quot;AlphaGo&quot;という名前で有名になりました。

このシステムは膨大なデータと訓練時間が必要になるいくつものステップを経て構築されています。おおよその手順は以下の通り。

1. 大量の盤面データ(多い場合で3000万)を用いて、ある盤面の次の一手を予測するSL policy networkを訓練する
2. 1.で作成したネットワーク同士を自己対戦させて強化学習を行い、より良い次の一手を予測するRL policy networkを訓練する。
3. 1.と2.で作成したネットワークを用いて訓練データ(3000万盤面)を自動生成し、盤面の良し悪しを評価するvalue netwrokを訓練する。

実戦では、上記の手順で訓練したネットワークに、将棋や囲碁のAIで良く用いられるモンテカルロ木探索を適用し、有効手の検索を行っています。

評価では、既存の様々な囲碁のプログラムと対戦し、その結果を出しています。また、その結果からプログラムの強さをレーティングで表しています。最終的には、囲碁のトッププロであるイ・セドル氏を超えるレーティングを示しました。

参考：[日本語の解説記事](http://blog.livedoor.jp/lunarmodule7/archives/4635352.html)
解説が非常に詳しいとても良い記事です。AlphaGoの訓練の途方も無さを感じられます。
  
--- 


#### [Neural Architecture Search with Reinforcement Learning](https://openreview.net/forum?id=r1Ue8Hcxg)(2016/11)★
あるタスクを解くためのニューラルネットのより良い構造を強化学習で獲得する手法を提案した論文です。

![NNRL.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/aeb9a284-91fc-2c6c-91a0-7e97e0e4e081.jpeg)
[Neural Architecture Search with Reinforcement Learning Fig. 4.](https://openreview.net/pdf?id=r1Ue8Hcxg)

![NNRL2.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/2e63045a-a52f-7c6a-6e39-d0db76690bf0.jpeg)
[Neural Architecture Search with Reinforcement Learning Fig. 5.](https://openreview.net/pdf?id=r1Ue8Hcxg)

pix2pixでも述べた通り、現在は人間がタスクに合わせてニューラルネットの構造を設計しています。この研究は、ニューラルネットの構造自体もニューラルネットで設計してしまおう、という非常に挑戦的なものです。
ネットワークの構造の設計にはRNNを用います。ニューラルネットの構造の要素(各種パラメータ、使用する演算や活性化関数等)を一つずつ予測し、それを1層ずつ繰り返すようなモデルになっています。上図Fig.4.はCNNの、Fig.5.はRNNの構造を設計する例です。
このモデルの訓練のための報酬は、設計したニューラルネットで任意のデータセットを評価したときのAccuracyを用います。つまり、報酬の計算の為にネットワークの訓練が必要になります。当然、膨大な訓練時間がかかってしまうため、モデルを非同期で並列に訓練するためのアプローチも示しています。

評価ではCIFAR-10を用いた画像分類と、PTBを用いた言語モデル作成の二種類のタスクに取り組んでいます。CIFAR-10のタスクでは、DenseNetに匹敵するError rateを示しました。PTBのタスクでは過去の比較手法を上回るPerplexityを示しました。
ただし、いずれのモデルも人間が設計したモデルよりも多いパラメータ数となっていました。

まだまだ根本的な問題は多いですが、シンギュラリティの可能性を感じました。
論文内で「訓練時にはGPU800枚をぶん回した」という怖い記述があり呆然としました。やってることを考えるとこのくらいしないとだめそうですが…。

## ニューラルネット応用系
特定の分野に拘らずに精度や速度の向上を図る等、DeepLearning自体の可能性を追求する研究をまとめました。

### 汎用的なDeepLearning手法
汎用的というと語弊がありますが、ある特定のタスクに拘らずに使用できる手法です。
  
--- 


#### [Hybrid computing using a neural network with dynamic external memory](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz) (2016/10)★
メモリ構造を持った汎用性の高いモデルであるDNC(Differentiable Neural Computers)を提案した論文です。

![DNC.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/23531534-b955-d2d3-9fa4-3be1972175d9.jpeg)
[Hybrid computing using a neural network with dynamic external memory Fig. 1.](http://www.nature.com/articles/nature20101.epdf?author_access_token=ImTXBI8aWbYxYQ51Plys8NRgN0jAjWel9jnR3ZoTv0MggmpDmwljGswxVdeocYSurJ3hxupzWuRNeGvvXnoO8o4jTJcnAyhGuZzXJ1GEaD-Z7E6X_a9R-xqJ9TfJWBqz)

こちらの研究の概要紹介は他の記事を参照ください。
~~力尽きました~~…。

参考：[Deepmindの紹介記事](https://deepmind.com/blog/differentiable-neural-computers/)
参考：[日本語解説記事1](https://www.evernote.com/shard/s2/sh/77999024-c730-477c-9e06-5b66affb4134/e88f608704525afa)
参考：[日本語解説記事2+chainer実装](http://qiita.com/yos1up/items/599ff75c876f6f94d249)
参考：[日本語解説記事3(未完)](http://qiita.com/dblN/items/2ab6e0af244e56a7cb4a)


### 最適化アルゴリズム
いずれもDeepLearningに必要不可欠な要素です。
多くの研究が出ていますが、特にインパクトのあったものをまとめます。
  
--- 


#### [Learning to learn by gradient descent by gradient descent](https://arxiv.org/abs/1606.04474)
ニューラルネットの訓練にどの最適化アルゴリズムを使用するかを学習する手法を提案した論文です。

![LtoL.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/5c00a4b1-fbb8-5b63-260e-eb7fb85787e4.jpeg)
[Learning to learn by gradient descent by gradient descent Fig. 2.](https://arxiv.org/pdf/1606.04474v2.pdf) (2016/6)

この手法ではLSTMを用いたRNNを使用します。図の二層目(Optimizer)でRNNを用いて最適化のための最適なパラメータ（の差分）を予測します。一層目（Optimizee）で実際にそのパラメータを使用して勾配の計算を行います。
Optimizerのネットワークを訓練することで、どのような場合にどのようにパラメータを更新すれば良いのか、ということをネットワークに学習させることが目的です。

評価ではCIFAR-10などのタスクを解く際に、様々な最適化アルゴリズムを用いてネットワークの訓練を行い、lossの下がり方の比較を行っています。提案手法が最も安定してlossを最小化できることを示しています。

どのような誤差関数でもこの手法が適用できるかが気になりました。

※12/18 追記
参考：[tensorflow実装](https://github.com/deepmind/learning-to-learn)

--- 


#### [Improving Stochastic Gradient Descent with Feedback](https://arxiv.org/abs/1611.01505) (2016/11)★
目的関数からのフィードバックを用いる最適化アルゴリズム、Eveを提案した論文です。

最適化アルゴリズムについて、まだキチンと理解できていないため、概要は省略します。申し訳ありません…。

![Eve.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/9ddc15d3-90ca-b38b-07d7-87ab11d43dc0.jpeg)
[Improving Stochastic Gradient Descent with Feedback Fig. 1.](https://arxiv.org/pdf/1611.01505v2.pdf)

評価ではCIFAR-10,100の訓練に様々な最適化アルゴリズムを用いて、提案手法が最も安定して訓練ができたことを示しています。

ニュールネットの訓練のための新しいデファクトスタンダードになりそうです。
私事ですが、**私はこの手法の名前が大好きです。**

参考：[tensorflow実装](https://github.com/jayanthkoushik/sgd-feedback)
参考：[chainerでのPR](https://github.com/pfnet/chainer/pull/1847)。私はchainerを主に使っているため、マージが待たれます。


### ニューラルネットの圧縮・高速化
サイズが大きくなりがちなDeepLearningのモデルを精度を保ちつつ圧縮する技術は、モバイル端末への適用や訓練等の省力化に関わる重要なものだと感じます。
  
--- 


#### [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/abs/1602.02830) (2016/2)
#### [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks] (2016/3)(https://arxiv.org/abs/1603.05279)
非常に似ているため二本続けて。

１本目はモデルのパラメータをすべて+1と-1のみで表現するBinaryNetを提案した論文です。一つのパラメータを1bitで表現できるため、メモリ使用量が大幅に削減できます。また、掛け算器をXNORに置き換えることができ、処理の高速化ができます。2016/2月に公開。

２本目はBinaryNetを改良して精度と計算速度を向上させたXNOR-Netを提案した論文です。内容はあまり理解できていないため、参考記事を参照してください…。2016/3月に公開。

評価では、XNOR-Netを用いることでAlexNetと比較して画像分類タスクの精度が11%落ちるかわりに、メモリ使用量が1/64（AlexNetが倍精度の場合）、計算速度が58倍(CPUを用いた場合)となることを示しました。

パラメータが二値のみでもある程度予測が行えることがすごいと思いました。
訓練後のフィルタがテトリスっぽいと思いました。

![BinaryNet.JPG](https://qiita-image-store.s3.amazonaws.com/0/110468/58544eb4-a869-27f0-6c62-e7c718491867.jpeg)
[Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 Fig. 2.](https://arxiv.org/pdf/1602.02830v3.pdf)

参考：[BinaryNet 日本語解説記事](http://qiita.com/supersaiakujin/items/81719e49a50a3fb653e8)
参考：[BinaryNet 公式実装](https://github.com/MatthieuCourbariaux/BinaryNet)
参考：[BinaryNet chainer実装](https://github.com/hillbig/binary_net)

参考：[XNOR-Net 日本語解説記事1](http://qiita.com/supersaiakujin/items/6adaf9731c9475891911)
参考：[XNOR-Net 日本語解説記事2](https://tkng.org/b/2016/03/28/xnor-network/)
  
--- 


#### [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size](https://arxiv.org/abs/1602.07360)
非常に省メモリなネットワークであるSqueezeNetを提案した論文です。
BinaryNetと同時期に出た研究ですが、こちらは畳み込み層のフィルタサイズやチャネル数を変更するなどしてパラメータの削減を試みています。

評価ではImageNetを用いた画像認識のタスクに取り組み、AlexNetと比較して同程度の精度を保ちつつメモリサイズを最大1/461(0.5MB)に出来ることを示しました。

参考：[日本語解説記事](http://qiita.com/supersaiakujin/items/ece1e20ca4073e77bed7)
参考：[Caffe実装](https://github.com/DeepScale/SqueezeNet)


# おわりに
本当はまだまだ書きたい内容があったのですが、分量と体力の関係で断念しました…。
改めて見返してみると、とても趣味の出ている選択になってしまったと反省しています。

この記事を書いている最中にも新しい話題がどんどん出てくるのがこの分野の怖いところです。
最先端の手法は中々追えないですが、今後も頑張ってキャッチアップして話題についていきたいです。
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="DeepLearning研究 2016年のまとめ by @eve_yk on @Qiita" data-url="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="DeepLearning研究 2016年のまとめ" href="http://b.hatena.ne.jp/entry/http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/eve_yk"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/eve_yk">eve_yk</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">886</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;eve_yk&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-a987a2d1-480f-40c6-b907-93b12c75d516"></div>
    <div id="UserFollowButton-react-component-a987a2d1-480f-40c6-b907-93b12c75d516"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/eve_yk/items/f4b274da7042cba1ba76">DeepLearning研究 2016年のまとめ</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/eve_yk/items/2ace6d4c1dad7e912df1">強化学習で参考になったサイトまとめ</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/eve_yk/items/07bc094538f2d50841f4">音声処理で参考になったサイトまとめ</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/eve_yk/items/e0ccd7917237fed1607f">ドッペルゲンガーがいたので人工知能(笑)で判別してみた(前編)</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/eve_yk/items/813110e1bf483611f8d7">ドッペルゲンガーがいたので人工知能(笑)で判別してみた(後編)</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\&quot;\u003eはじめに\u003c/a\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%80%E3%81%84%E8%A8%B3%E3%81%A8%E3%81%8A%E9%A1%98%E3%81%84\&quot;\u003e言い訳とお願い\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\n\u003ca href=\&quot;#%E3%81%BE%E3%81%A8%E3%82%81%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81-1224-%E8%BF%BD%E8%A8%98\&quot;\u003eまとめのまとめ (※12/24 追記)\u003c/a\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#2016%E5%B9%B4%E3%81%AE%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E8%AB%96%E6%96%87100%E9%81%B8\&quot;\u003e2016年のディープラーニング論文100選\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#foobarnet-%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E9%96%A2%E9%80%A3%E3%81%AEnet%E3%81%BE%E3%81%A8%E3%82%81\&quot;\u003efoobarNet: ディープラーニング関連の○○Netまとめ\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#2016%E5%B9%B4%E3%81%AE%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB\&quot;\u003e2016年の深層学習を用いた画像認識モデル\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-learning%E3%81%AE%E7%90%86%E8%AB%96%E7%9A%84%E8%AB%96%E6%96%87%E3%83%AA%E3%82%B9%E3%83%88%E9%9A%8F%E6%99%82%E6%9B%B4%E6%96%B0\&quot;\u003eDeep Learningの理論的論文リスト【随時更新】\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\n\n\n\u003ca href=\&quot;#%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86%E7%B3%BB\&quot;\u003e画像処理系\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#resnet%E3%81%AE%E9%80%B2%E5%8C%96\&quot;\u003eResNetの進化\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-networks-with-stochastic-depth-20165\&quot;\u003eDeep Networks with Stochastic Depth (2016/5)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#fractalnet-ultra-deep-neural-networks-without-residuals-20165\&quot;\u003eFractalNet: Ultra-Deep Neural Networks without Residuals (2016/5)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#residual-networks-of-residual-networks-multilevel-residual-networks-20168\&quot;\u003eResidual Networks of Residual Networks: Multilevel Residual Networks (2016/8)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#densely-connected-convolutional-networks-20168\&quot;\u003eDensely Connected Convolutional Networks (2016/8)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#aggregated-residual-transformations-for-deep-neural-networks-201611\&quot;\u003eAggregated Residual Transformations for Deep Neural Networks (2016/11)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%94%BB%E5%83%8F%E7%94%9F%E6%88%90\&quot;\u003e画像生成\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#pixel-recurrent-neural-networks20161\&quot;\u003ePixel Recurrent Neural Networks(2016/1)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#image-to-image-translation-with-conditional-adversarial-nets201611\&quot;\u003eImage-to-Image Translation with Conditional Adversarial Nets(2016/11)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#stackgan-text-to-photo-realistic-image-synthesis-with-stacked-generative-adversarial-networks201612\&quot;\u003eStackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks(2016/12)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E5%8B%95%E7%94%BB%E5%87%A6%E7%90%86%E7%B3%BB\&quot;\u003e動画処理系\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8B%95%E7%94%BB%E7%94%9F%E6%88%90\&quot;\u003e動画生成\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-predictive-coding-networks-for-fideo-prediction-and-unsupervised-learning20165\&quot;\u003eDeep Predictive Coding Networks for Fideo Prediction and Unsupervised Learning(2016/5)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#generating-videos-with-scene-dynamics\&quot;\u003eGenerating Videos with Scene Dynamics\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E9%9F%B3%E5%A3%B0%E5%87%A6%E7%90%86%E7%B3%BB\&quot;\u003e音声処理系\u003c/a\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#wavenet-a-generative-model-for-raw-audio-20169\&quot;\u003eWaveNet: A Generative Model for Raw Audio (2016/9)★\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#soundnet-learning-sound-representations-from-unlabeled-video201610\&quot;\u003eSoundNet: Learning Sound Representations from Unlabeled Video(2016/10)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\n\n\n\u003ca href=\&quot;#%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B3%BB\&quot;\u003e自然言語処理系\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB\&quot;\u003e言語モデル\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#pointer-sentinel-mixture-models20169\&quot;\u003ePointer Sentinel Mixture Models(2016/9)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#lightrnn-memory-and-computation-efficient-recurrent-neural-networks201610\&quot;\u003eLightRNN: Memory and Computation-Efficient Recurrent Neural Networks(2016/10)★\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#quasi-recurrent-neural-networks201611201611\&quot;\u003eQUASI-RECURRENT NEURAL NETWORKS(2016/11)★(2016/11)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3\&quot;\u003e機械翻訳\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#googles-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation20169\&quot;\u003eGoogle&#39;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation(2016/9)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#neural-machine-translation-in-linear-time201610\&quot;\u003eNeural Machine Translation in Linear Time(2016/10)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\u003ca href=\&quot;#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%B3%BB\&quot;\u003e強化学習系\u003c/a\u003e\n\u003cul\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#mastering-the-game-of-go-with-deep-neural-networks-and-tree-search\&quot;\u003eMastering the game of Go with deep neural networks and tree search\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#neural-architecture-search-with-reinforcement-learning201611\&quot;\u003eNeural Architecture Search with Reinforcement Learning(2016/11)★\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/ul\u003e\n\n\n\n\u003ca href=\&quot;#%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E5%BF%9C%E7%94%A8%E7%B3%BB\&quot;\u003eニューラルネット応用系\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%B1%8E%E7%94%A8%E7%9A%84%E3%81%AAdeeplearning%E6%89%8B%E6%B3%95\&quot;\u003e汎用的なDeepLearning手法\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#hybrid-computing-using-a-neural-network-with-dynamic-external-memory-201610\&quot;\u003eHybrid computing using a neural network with dynamic external memory (2016/10)★\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%9C%80%E9%81%A9%E5%8C%96%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0\&quot;\u003e最適化アルゴリズム\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#learning-to-learn-by-gradient-descent-by-gradient-descent\&quot;\u003eLearning to learn by gradient descent by gradient descent\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#improving-stochastic-gradient-descent-with-feedback-201611\&quot;\u003eImproving Stochastic Gradient Descent with Feedback (2016/11)★\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%81%AE%E5%9C%A7%E7%B8%AE%E9%AB%98%E9%80%9F%E5%8C%96\&quot;\u003eニューラルネットの圧縮・高速化\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#binarized-neural-networks-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or--1-20162\&quot;\u003eBinarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 (2016/2)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#xnor-net-imagenet-classification-using-binary-convolutional-neural-networkshttpsarxivorgabs160305279\&quot;\u003eXNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks(https://arxiv.org/abs/1603.05279)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-05mb-model-size\&quot;\u003eSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u0026lt;0.5MB model size\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\n\n\n\u003ca href=\&quot;#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\&quot;\u003eおわりに\u003c/a\u003e\n\n\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-b2b6161d-3d4e-4470-a88a-f1b0536669a5"></div>
    <div id="Toc-react-component-b2b6161d-3d4e-4470-a88a-f1b0536669a5"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:594,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;f4b274da7042cba1ba76&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="niconegoto"><a itemprop="url" href="/niconegoto"><img alt="niconegoto" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/90340/profile-images/1478265447" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hanaken_Nirvana"><a itemprop="url" href="/hanaken_Nirvana"><img alt="hanaken_Nirvana" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/86921/profile-images/1473703908" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="chachay"><a itemprop="url" href="/chachay"><img alt="chachay" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/117379/profile-images/1476087329" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ta-ka"><a itemprop="url" href="/ta-ka"><img alt="ta-ka" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/82527/profile-images/1473702483" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="TomokIshii"><a itemprop="url" href="/TomokIshii"><img alt="TomokIshii" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/74152/profile-images/1473699746" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="yizumi1012xxx"><a itemprop="url" href="/yizumi1012xxx"><img alt="yizumi1012xxx" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/106291/profile-images/1473709890" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="akiraak"><a itemprop="url" href="/akiraak"><img alt="akiraak" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/33704/profile-images/1473686182" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="neka-nat@github"><a itemprop="url" href="/neka-nat@github"><img alt="neka-nat@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/14516/profile-images/1473683376" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ohtake_i"><a itemprop="url" href="/ohtake_i"><img alt="ohtake_i" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/129071/profile-images/1473717434" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="rkato5680"><a itemprop="url" href="/rkato5680"><img alt="rkato5680" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/100758/profile-images/1473708197" /></a></div></div><div class="ArticleFooter__user"><a href="/eve_yk/items/f4b274da7042cba1ba76/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/f4b274da7042cba1ba76/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/eve_yk/items/f4b274da7042cba1ba76.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><div class="itemsShowBody_adventCalendar"><div class="itemsShowBody_adventCalendar_header"><i class="fa fa-fw fa-calendar"></i> This post is the <span class="date">No.17</span> article of <a class="title" href="/advent-calendar/2016/deeplearning">DeepLearning Advent Calendar 2016</a></div><ul class="itemsShowBody_adventCalendar_nav list-unstyled"><li class="itemsShowBody_adventCalendar_neighborItem itemsShowBody_adventCalendar_neighborItem-prev"><span class="itemsShowBody_adventCalendar_date"><i class="fa fa-fw fa-arrow-circle-left"></i> Day 16:</span><span class="itemsShowBody_adventCalendar_title"><img alt="pekatuu" class="itemsShowBody_adventCalendar_icon" src="https://pbs.twimg.com/profile_images/2503279447/8co3jo1bs4nh4wivhh05_normal.png" width="18" height="18" /> <a class="itemsShowBody_adventCalendar_link" href="/pekatuu/items/d9bd133b453c314ab31d">pix2pixでフレーム補完</a></span></li><li class="itemsShowBody_adventCalendar_neighborItem itemsShowBody_adventCalendar_neighborItem-next"><span class="itemsShowBody_adventCalendar_date"><i class="fa fa-fw fa-arrow-circle-right"></i> Day 18:</span><span class="itemsShowBody_adventCalendar_title"> <a class="itemsShowBody_adventCalendar_link" href="/advent-calendar/2016/deeplearning#day-18">Next</a></span></li></ul></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/sakaiakira/items/9da1edda802c4884865c#_reference-78a89c1645733c2b58b7"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532" />2016年のディープラーニング論文100選</a><time class="references_datetime js-dateTimeView" datetime="2016-12-16T23:01:08+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/shinya7y/items/8911856125a3109378d6#_reference-98f4d470e030eb353c7b"><img alt="" width="18" height="18" src="https://pbs.twimg.com/profile_images/716138305872486400/CrQCQQo7_normal.png" />foobarNet: ディープラーニング関連の○○Netまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-12-17T14:25:07+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/taru0216/items/dda1f9f11397f811e98a#_reference-f199a4280442ac580589"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/63371/profile-images/1473696246" />Retty流『2200万ユーザを支える機械学習基盤』の作り方</a><time class="references_datetime js-dateTimeView" datetime="2016-12-25T00:02:32+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/jiny2001/items/85af7dd163a63b3a152a#_reference-e5346047e7598fb2bace"><img alt="" width="18" height="18" src="https://avatars.githubusercontent.com/u/12959344?v=3" />Inside of Deep Learning あるいは深層学習は何を変えるのか</a><time class="references_datetime js-dateTimeView" datetime="2017-01-20T18:26:14+00:00">about 2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="DeepLearning研究 2016年のまとめ by @eve_yk on @Qiita" data-url="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="DeepLearning研究 2016年のまとめ" href="http://b.hatena.ne.jp/entry/http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/eve_yk/items/f4b274da7042cba1ba76" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e初めまして   \u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;http://qiita.com/sakaiakira/items/9da1edda802c4884865c\&quot; id=\&quot;reference-d7de8490152717fc4ced\&quot;\u003e2016年のディープラーニング論文100選\u003c/a\u003eを書いたsakaiakiraです。\u003cbr\u003e\u003cbr\u003e\n素晴らしいまとめをありがとうございます。こちらからもリンクを貼りました。\u003c/p\u003e\n\n\u003cp\u003e他人がどうとかじゃなくて、あなたは日本のディープラーニングコミュニティに大きな貢献をした訳だから、もっと胸を張ってください。\u003cbr\u003e\u003cbr\u003e\n編集リクエスト送っときます。   \u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-12-17T08:11:02+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:702116,&quot;is_team&quot;:false,&quot;item_id&quot;:450966,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;f4b274da7042cba1ba76&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:7,&quot;raw_body&quot;:&quot;初めまして   \n \n[2016年のディープラーニング論文100選](http://qiita.com/sakaiakira/items/9da1edda802c4884865c)を書いたsakaiakiraです。   \n素晴らしいまとめをありがとうございます。こちらからもリンクを貼りました。\n\n他人がどうとかじゃなくて、あなたは日本のディープラーニングコミュニティに大きな貢献をした訳だから、もっと胸を張ってください。     \n編集リクエスト送っときます。   \n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/eve_yk/items/f4b274da7042cba1ba76#comment-d8d4e9862065dd1a81a8&quot;,&quot;user&quot;:{&quot;contribution&quot;:584,&quot;created_at&quot;:&quot;2016-12-02T14:35:02+09:00&quot;,&quot;id&quot;:152471,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;sakaiakira&quot;},&quot;uuid&quot;:&quot;d8d4e9862065dd1a81a8&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eコメントと編集リクエストをいただきありがとうございます。\u003cbr\u003e\nまた、こちらこそ非常に素晴らしいまとめをありがとうございます。\u003c/p\u003e\n\n\u003cp\u003e少々卑屈な書き方をしすぎてしまいました。そう言っていただけると励みになります。\u003c/p\u003e\n\n\u003cp\u003e編集リクエストを反映いたしました！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-12-17T08:34:43+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:702119,&quot;is_team&quot;:false,&quot;item_id&quot;:450966,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;f4b274da7042cba1ba76&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:6,&quot;raw_body&quot;:&quot;コメントと編集リクエストをいただきありがとうございます。\nまた、こちらこそ非常に素晴らしいまとめをありがとうございます。\n\n少々卑屈な書き方をしすぎてしまいました。そう言っていただけると励みになります。\n\n編集リクエストを反映いたしました！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/eve_yk/items/f4b274da7042cba1ba76#comment-f8f909b7a6af45898d9f&quot;,&quot;user&quot;:{&quot;contribution&quot;:886,&quot;created_at&quot;:&quot;2016-01-23T15:18:34+09:00&quot;,&quot;id&quot;:110468,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;eve_yk&quot;},&quot;uuid&quot;:&quot;f8f909b7a6af45898d9f&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:450966,&quot;uuid&quot;:&quot;f4b274da7042cba1ba76&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;eve_yk&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:110468,&quot;url_name&quot;:&quot;eve_yk&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224&quot;},{&quot;id&quot;:152471,&quot;url_name&quot;:&quot;sakaiakira&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/152471/profile-images/1481880532&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-d16c2eec-c571-4f52-b1bb-0b7f8f87dd21"></div>
    <div id="CommentListContainer-react-component-d16c2eec-c571-4f52-b1bb-0b7f8f87dd21"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="5nFY9UQL9clEaso5VEZXHr60qdgr7YwVOPnoYdb47FLABkksmz0RpRfS9Kt2e2/3GtV8Foj1ASnNYFPfeUx0Hg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/eve_yk/items/f4b274da7042cba1ba76" /><input type="hidden" name="item_uuid" id="item_uuid" value="f4b274da7042cba1ba76" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/eve_yk/items/f4b274da7042cba1ba76", "id": 450966, "uuid": "f4b274da7042cba1ba76" }</script><script class="js-user" type="application/json">{&quot;id&quot;:110468,&quot;url_name&quot;:&quot;eve_yk&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="bExR+mh3HHL/Wh/gK9AjXPW/4L8fyaEX9DyWDAQOf5lKO0Ajt0H4HqziIXIJ7Ru1Ud41cbzRLCsBpS2yq7rn1Q==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/eve_yk/items/f4b274da7042cba1ba76" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>