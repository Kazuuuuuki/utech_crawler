<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>Pythonではじめる強化学習 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="

はじめに

みなさん、強化学習してますか？
強化学習はロボットや、囲碁や将棋のようなゲーム、対話システム等に応用できる楽しい技術です。

強化学習とは、試行錯誤を通じて環境に適応する学習制御の枠組みです。教師あり学習では入力に対する正しい出力を与えて学習させました。強化学習では、入力に対する正しい出力を与える代わりに、一連の行動に対する良し悪しを評価する「報酬」というスカラーの評価値が与え、これを手がかりに学習を行います。以下に強化学習の枠組みを示します。



エ..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="Hironsan13" name="twitter:creator" /><meta content="Pythonではじめる強化学習 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="# はじめに
みなさん、強化学習してますか？
強化学習はロボットや、囲碁や将棋のようなゲーム、対話システム等に応用できる楽しい技術です。

強化学習とは、試行錯誤を通じて環境に適応する学習制御の枠組みです。教師あり学習では入力に対する..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="5D9UwabkIoRjgetipQqAUb+6GEJDRChT/rPvV5pOMdJBZQhq+HxuD+kTK/Jbl+yVMcIm4MhHtguT3P2aM0baGQ==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"Hironsan","type":"items","id":"56f6c0b2f4cfd28dd906"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;News&quot;,&quot;content&quot;:&quot;ストックの他に「いいね」が追加されました&quot;,&quot;url&quot;:&quot;http://blog.qiita.com/post/153200849029/qiita-like-button&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-6e0ab6ac-0400-4223-9170-0e70c649f9c4"></div>
    <div id="HeaderContainer-react-component-6e0ab6ac-0400-4223-9170-0e70c649f9c4"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/Python",        "name": "Python"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">Pythonではじめる強化学習</h1><ul class="TagList"><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="76"><a class="u-link-unstyled TagList__label" href="/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><img alt="強化学習" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>強化学習</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">265</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="4 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>4</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:265,&quot;uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="K-N"><a itemprop="url" href="/K-N"><img alt="K-N" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/99622/profile-images/1473707847" /></a></li><li class="js-hovercard" data-hovercard-target-name="mlmct"><a itemprop="url" href="/mlmct"><img alt="mlmct" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/105404/profile-images/1473709605" /></a></li><li class="js-hovercard" data-hovercard-target-name="mir0401"><a itemprop="url" href="/mir0401"><img alt="mir0401" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/98098/profile-images/1473707397" /></a></li><li class="js-hovercard" data-hovercard-target-name="yasuyuki0722"><a itemprop="url" href="/yasuyuki0722"><img alt="yasuyuki0722" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/48350/profile-images/1473691321" /></a></li><li class="js-hovercard" data-hovercard-target-name="hibari_ren"><a itemprop="url" href="/hibari_ren"><img alt="hibari_ren" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/122410/profile-images/1473764063" /></a></li><li class="js-hovercard" data-hovercard-target-name="ueno3"><a itemprop="url" href="/ueno3"><img alt="ueno3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/75359/profile-images/1473700135" /></a></li><li class="js-hovercard" data-hovercard-target-name="Rydeen2045"><a itemprop="url" href="/Rydeen2045"><img alt="Rydeen2045" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77151/profile-images/1473700733" /></a></li><li class="js-hovercard" data-hovercard-target-name="mettoboshi"><a itemprop="url" href="/mettoboshi"><img alt="mettoboshi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/42925/profile-images/1473689342" /></a></li><li class="js-hovercard" data-hovercard-target-name="mofoolog"><a itemprop="url" href="/mofoolog"><img alt="mofoolog" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/73019/profile-images/1473699355" /></a></li><li class="js-hovercard" data-hovercard-target-name="dnby"><a itemprop="url" href="/dnby"><img alt="dnby" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/51522/profile-images/1473692467" /></a></li><li><a href="/Hironsan/items/56f6c0b2f4cfd28dd906/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/Hironsan"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" alt="1473700709" /></a> <a class="u-link-unstyled" href="/Hironsan">Hironsan</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-06-09T08:56:37+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-06-09">Edited at <time datetime="2017-03-13T13:39:27+09:00" itemprop="dateModified">2017-03-13</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/Hironsan/items/56f6c0b2f4cfd28dd906/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">5</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/Hironsan/items/56f6c0b2f4cfd28dd906/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(5)</span></a></li><li><a href="/Hironsan/items/56f6c0b2f4cfd28dd906.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-56f6c0b2f4cfd28dd906" itemprop="articleBody">
<h1>
<span id="はじめに" class="fragment"></span><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><i class="fa fa-link"></i></a>はじめに</h1>

<p>みなさん、強化学習してますか？<br>
強化学習はロボットや、囲碁や将棋のようなゲーム、対話システム等に応用できる楽しい技術です。</p>

<p>強化学習とは、試行錯誤を通じて環境に適応する学習制御の枠組みです。教師あり学習では入力に対する正しい出力を与えて学習させました。強化学習では、入力に対する正しい出力を与える代わりに、一連の行動に対する良し悪しを評価する「報酬」というスカラーの評価値が与え、これを手がかりに学習を行います。以下に強化学習の枠組みを示します。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/800cdce3-132d-87f2-5472-532261a5b13c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/800cdce3-132d-87f2-5472-532261a5b13c.png" alt="rl_concept.png"></a></p>

<ol>
<li>エージェントは時刻 $t$ において環境の状態 $s_t$ を観測</li>
<li>観測した状態から行動 $a_t$ を決定</li>
<li>エージェントは行動を実行</li>
<li>環境は新しい状態 $s_{t+1}$ に遷移</li>
<li>遷移に応じた報酬 $r_{t+1}$ を獲得</li>
<li>学習する</li>
<li>ステップ1から繰り返す</li>
</ol>

<p>強化学習の目的は、エージェントが取得する利得（累積報酬）を最大化するような、状態から行動へのマッピング（政策）を獲得することです。</p>



<p>強化学習では上記の枠組みをマルコフ決定過程(Markov decision processes: MDP)によってモデル化し、学習アルゴリズムを考えるといったことが行われます。本稿では、強化学習について理論編と実践編に分けて解説します。理論編では、MDPおよび学習アルゴリズムについて説明し、実践編ではプログラミング言語Pythonを用いて理論編で説明したことを実装していきます。順番に読むことをおすすめしますが、「こまけぇこたぁいいんだよ！」という方は理論編を飛ばして実践編を読んでください。</p>

<h1>
<span id="理論編" class="fragment"></span><a href="#%E7%90%86%E8%AB%96%E7%B7%A8"><i class="fa fa-link"></i></a>理論編</h1>

<p>まず、強化学習の理論を例を交えて説明していきます。ここでは例題の問題設定を説明した後、マルコフ決定過程について説明します。マルコフ決定過程について理解できたところで、その学習方法について説明します。</p>

<h2>
<span id="問題設定" class="fragment"></span><a href="#%E5%95%8F%E9%A1%8C%E8%A8%AD%E5%AE%9A"><i class="fa fa-link"></i></a>問題設定</h2>

<p>さて、MDPを説明するにあたって以下のような迷路の世界を考えてみましょう。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/d2631adc-c8f2-016e-a5ba-9f9c09c4ff7c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/d2631adc-c8f2-016e-a5ba-9f9c09c4ff7c.png" alt="grid.png"></a></p>

<p>迷路の世界のルールは以下の通りです。</p>

<ul>
<li>エージェントはあるグリッドの中にいます</li>
<li>エージェントは壁(灰色)を越えられません</li>
<li>エージェントは自分が思った通りに動けるとは限りません

<ul>
<li>80%の確率で正しく動けますが、10%の確率で動こうと思っていた方向の左側へ動き、もう10%の確率で右側へ動きます</li>
</ul>
</li>
<li>動こうと思った場所に壁があった場合は、その場にとどまります</li>
<li>エージェントは各時刻に報酬を受け取ります</li>
<li>ゴール(宝箱)やトラップ(爆発)にたどり着くとゲーム終了し、スタート位置に戻ります</li>
</ul>

<p>確率的な遷移について以下の図を用いてもう少し詳しく説明します。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/ba75a1b4-63a6-f796-794a-d40c9e380a3e.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/ba75a1b4-63a6-f796-794a-d40c9e380a3e.png" alt="transition2.png"></a></p>

<p>ここではロボットは上に動こうとしています。このとき80%の確率で上に動けますが、10%の確率で左側に動き、残りの10%の確率で右側に動いてしまいます。遷移確率は以下の表のようになります。</p>

<table>
<thead>
<tr>
<th>$s'$</th>
<th>$P(s' \mid s_1, a)$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$s_2$</td>
<td>0.1</td>
</tr>
<tr>
<td>$s_3$</td>
<td>0.8</td>
</tr>
<tr>
<td>$s_4$</td>
<td>0.1</td>
</tr>
</tbody>
</table>

<h2>
<span id="マルコフ決定過程markov-decision-processes-mdp" class="fragment"></span><a href="#%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8Bmarkov-decision-processes-mdp"><i class="fa fa-link"></i></a>マルコフ決定過程(Markov decision processes: MDP)</h2>

<p>さて、問題設定はわかったでしょうか？わかったところで、問題をMDPでモデル化します。</p>

<p>MDPは以下の4項組で定義されます</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
M = (S, A, T, R)
</pre></div></div>

<p>それぞれ何なのかというと、</p>

<ul>
<li>状態集合: $s \in S$</li>
<li>行動集合: $a \in A$</li>
<li>遷移関数: $T(s, a, s') \sim P(s' \mid s, a)$</li>
<li>報酬関数: $R(s)$, $R(s, a)$, $R(s,a,s')$</li>
</ul>

<p>それぞれ上で説明した問題設定に当てはめて考えてみます。</p>

<ul>
<li>状態集合は環境がとりうる状態の集合を表しており、例題の場合はグリッドのすべてのセルを表しています。</li>
<li>行動集合はエージェントがとりうる行動の種類のことです。例題の場合、エージェントは上下左右に動くことができるので4種類です。</li>
<li>遷移関数とは、ある状態 $s$ で行動 $a$ を取った時に次の状態 $s'$ へどのくらいの確率で遷移するかということを表しています。上の図で説明した通りです。</li>
<li>報酬は観測した状態から取るべき行動を決定し、新たに遷移した結果として環境から受け取る値のことです。後で詳しく説明します。</li>
</ul>

<p>このMDPの目的は利得(=累積報酬)を最大化する行動戦略を学習することです。この行動戦略のことを<strong>政策</strong>と呼びます。</p>

<h3>
<span id="政策についてもう少し詳しく" class="fragment"></span><a href="#%E6%94%BF%E7%AD%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%82%82%E3%81%86%E5%B0%91%E3%81%97%E8%A9%B3%E3%81%97%E3%81%8F"><i class="fa fa-link"></i></a>政策についてもう少し詳しく</h3>

<p>MDPでは最適な政策 $\pi^*: S \rightarrow A$ を得ることが目的です。政策は状態から行動へのマッピングを行う関数とみなせます。<br>
政策は各状態で取るべき行動を教えてくれます。また、最適な政策は期待利得を最大化します。</p>

<p>遷移した時に得られる報酬を変化させたときの最適な政策の変化を見てみましょう。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/16122f4e-712b-4e70-2597-5b900bef0f33.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/16122f4e-712b-4e70-2597-5b900bef0f33.gif" alt="reward_policy.gif"></a><br>
この図では状態遷移で得られる報酬をだんだん小さく（ペナルティを大きく）した時における政策の変化を示しています。政策は得られる期待利得を最大化するという考え方から学習します。そのため、状態遷移によるペナルティが小さければ遠回りしてでもゴールを目指そうとし、ペナルティが大きければ速やかにゲームを終了させる戦略を取るようになります。比較しやすいように静止画も用意しました。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/0137ccc8-fdd1-825b-46e8-bc70710689f6.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/0137ccc8-fdd1-825b-46e8-bc70710689f6.png" alt="rewards_policy.png"></a></p>

<h3>
<span id="報酬についてもう少し詳しく" class="fragment"></span><a href="#%E5%A0%B1%E9%85%AC%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%82%82%E3%81%86%E5%B0%91%E3%81%97%E8%A9%B3%E3%81%97%E3%81%8F"><i class="fa fa-link"></i></a>報酬についてもう少し詳しく</h3>

<p>さて、ここまでで政策を学習するためには累積報酬を最大化すると述べました。<br>
各時刻に報酬 $R(s_t)$ を得ることができるので、累積報酬は以下のように表せます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\begin{align}
U([s_0, s_1, s_2,\ldots]) &amp;= R(s_0) + R(s_1) + R(s_2) + \cdots \\
    &amp;= \sum_{t=0}^{\infty}R(s_t)
\end{align}
</pre></div></div>

<p>このように単純な累積報酬を使ってもいいのですが、いくつかの理由から以下の式のように将来得られる報酬を割引係数 $\gamma (0 \leq \gamma &lt; 1)$ で割り引いた累積報酬が使われます。こうすることで、将来得られる報酬より現在得られる報酬を重視するようになります。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\begin{align}
U([s_0, s_1, s_2,\ldots]) &amp;= R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots \\
    &amp;= \sum_{t=0}^{\infty}\gamma^t R(s_t)
\end{align}
</pre></div></div>

<p>このような割引報酬を使う理由としては以下があげられます。</p>

<ul>
<li>計算的に値が発散しない</li>
<li>実際の環境では、時間がたつと環境が変化する可能性があるため、時系列のすべての報酬を同じ重みで考慮するのは妥当ではない</li>
</ul>

<h2>
<span id="最適な政策の見つけ方" class="fragment"></span><a href="#%E6%9C%80%E9%81%A9%E3%81%AA%E6%94%BF%E7%AD%96%E3%81%AE%E8%A6%8B%E3%81%A4%E3%81%91%E6%96%B9"><i class="fa fa-link"></i></a>最適な政策の見つけ方</h2>

<p>ここまででMDPというモデルについてはわかりました。残された問題はどうやって最適な政策を見つけるかということです。以下では最適な政策を見つけるアルゴリズムとして、価値反復法とQ-Learningについて説明します。</p>

<h3>
<span id="価値反復法value-iteration" class="fragment"></span><a href="#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95value-iteration"><i class="fa fa-link"></i></a>価値反復法(Value Iteration)</h3>

<p>価値反復法では、状態 $s$ から最適な行動をとり続けた時の期待利得を計算します。得られる利得の期待値を考えるのは確率的な遷移が行われるためです。</p>

<p>この期待利得がわかれば、現在の状態 $s$ から将来にわたって得られる価値がわかるので、その価値を最大化するような行動を選択できるようにすればいいはずです。価値は以下の状態価値関数で求められます。複雑に見えますが、要するに(期待)割引累積報酬を計算しているだけです。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
U(s) = R(s) + \gamma \max_a\sum_{s'} P(s' \mid s,a) U(s')
</pre></div></div>

<p>この式の中にはmaxという非線形計算があり行列等で解くことは困難です。そのため反復的に計算を行って価値を徐々に更新していきます。これが価値反復法の名前の所以です。以下に価値反復法のアルゴリズムを示します。計算量は$O(|S|^2 |A|)$です。</p>

<blockquote>
<p>1.すべての状態 $s$ について $U(s)$ を適当な値(ゼロなど)に初期化</p>

<p>2.すべての $U(s)$ について以下の式を計算して値を更新</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
U(s) \leftarrow R(s) + \gamma \max_a\sum_{s'} P(s' \mid s,a) U(s')
</pre></div></div>

<p>3.ステップ2を値が収束するまで繰り返す</p>
</blockquote>

<p>上記の式にはmax計算をする部分がありますが、その値が最大値となる行動aを記録しておく必要があります。そのため $U(s)$ だけでは最適な政策(行動)がわかりにくいという問題があります。そこで以下のQ関数を導入します。行動価値関数とも呼ばれます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
Q^*(s,a) = R(s) + \gamma \sum_{s'} P(s' \mid s,a) \max_{a'}Q^*(s', a')
</pre></div></div>

<p><strong>$Q^*(s,a)$</strong>は、<strong>状態$s$で行動aを選択後はずっと最適な政策を取り続ける場合</strong>の割引累積報酬の期待値を表しています。<br>
$U(s)$ と $Q^*(s,a)$ には、各状態における最大の値だけ保持するのか、それとも各行動に対して値を保持するのかの違いがあります。したがって、状態sにおける最大のQ値は $U(s)$ に等しくなり、このQ値を持つ行動aが最適な行動となります。以下の図がそのイメージです。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/35041da9-4449-c8c6-59a2-cd0a796105d1.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/35041da9-4449-c8c6-59a2-cd0a796105d1.png" alt="v_and_q.png"></a></p>

<p>Q関数を用いた場合の価値反復法は以下のようになります。</p>

<blockquote>
<p>1.すべての状態 $s$ について $Q^*(s,a)$ を適当な値(ゼロなど)に初期化</p>

<p>2.すべての $Q^*(s,a)$ について以下の式を計算して値を更新</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
Q^*(s,a) \leftarrow R(s) + \gamma \sum_{s'} P(s' \mid s,a) \max_{a'}Q^*(s', a')
</pre></div></div>

<p>3.ステップ2を値が収束するまで繰り返す</p>
</blockquote>

<p>ここまでくれば最適な政策$\pi ^*(s)$を見つけるのは簡単です。以下で求められます。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\pi^*(s) = arg\max_{a}Q^*(s,a)
</pre></div></div>

<h3>
<span id="計算例" class="fragment"></span><a href="#%E8%A8%88%E7%AE%97%E4%BE%8B"><i class="fa fa-link"></i></a>計算例</h3>

<p>せっかくなので試しに以下の赤い場所の価値を計算してみましょう。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/dac84db0-422a-3dba-f530-f38b61328473.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/77079/dac84db0-422a-3dba-f530-f38b61328473.png" alt="value_func_example.png"></a><br>
計算には以下の式を使います。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
U(s) \leftarrow R(s) + \gamma \max_a\sum_{s'} P(s' \mid s,a) U(s')
</pre></div></div>



<p>$\gamma = 0.5$、 $R(s) = -0.04$、遷移確率は進みたい方向に0.8、その左右に0.1ずつとします。</p>

<p>以上の条件のとき、赤い場所の値は・・・</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\begin{align}
U(s) &amp;= -0.04 + 0.5 \cdot (0.8 \cdot 1 + 0.1 \cdot 0 + 0.1 \cdot 0)\\
&amp;= -0.04 + 0.4\\
&amp;= 0.36
\end{align}
</pre></div></div>



<p>と求められます。</p>

<h3>
<span id="q-learning" class="fragment"></span><a href="#q-learning"><i class="fa fa-link"></i></a>Q-Learning</h3>

<p>さて、価値反復法で最適な政策が求められて一安心。そうは問屋がおろしません。何が問題なのでしょうか？</p>

<p>今までは状態遷移確率についてわかっていました。しかし現実の問題を考えると状態遷移確率がわかっていることは珍しいです。チェスや将棋を思い浮かべてみてください。ある盤面（状態）から別の盤面（別の状態）へ遷移する確率はあらかじめ与えられているでしょうか？与えられていませんよね？そのような場合は価値反復法は使えません。</p>

<p>そこで出てくるのが価値反復法の強化学習版ともいえるQ-Learningです。Q-Learningでは<br>
$\hat{Q}^{*}(s,a)$の推定値$\hat{Q}^{*}(s_t, a_t)$を計算します。Q-Learningを使うことで、遷移確率がわからない状況でも最適な政策の学習を行うことができます。Q-Learningのアルゴリズムは以下の通りです。</p>

<blockquote>
<p>1.適当な値で $\hat Q^*(s_t, a_t)$ を初期化</p>

<p>2.以下の式を用いて行動価値の推定値を更新。ただし $\alpha$ は学習率で $0 \leq \alpha \leq 1$。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\hat Q^*(s_t, a_t) \leftarrow \hat Q^*(s_t, a_t) + \alpha \Bigl[ r_{t+1} + \gamma \max_{a \in A}\hat Q^*(s_{t+1}, a) - \hat Q^*(s_t, a_t) \Bigr]
</pre></div></div>

<p>3.時間ステップ $t$ を $t+1$ へ進めて2へ戻る</p>
</blockquote>

<p>この計算を繰り返すことで最適な $Q^*(s,a)$ を得ることができます。</p>



<h1>
<span id="実践編" class="fragment"></span><a href="#%E5%AE%9F%E8%B7%B5%E7%B7%A8"><i class="fa fa-link"></i></a>実践編</h1>

<p>上で説明したことをPythonでやったらどうなるかについて見ていきましょう。</p>

<h2>
<span id="価値反復法" class="fragment"></span><a href="#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95"><i class="fa fa-link"></i></a>価値反復法</h2>

<p>まずは価値反復法からです。<br>
価値反復法は<a href="https://github.com/aimacode/aima-python" rel="nofollow noopener" target="_blank">UC Berkeley</a>のコード(mdp.py)がきれいだったので、それを基に説明します。</p>

<h3>
<span id="mdpクラス" class="fragment"></span><a href="#mdp%E3%82%AF%E3%83%A9%E3%82%B9"><i class="fa fa-link"></i></a>MDPクラス</h3>

<p>まずは基底クラスとなるMDPから。これを継承してGridMDPクラスを作ります。</p>

<div class="code-frame" data-lang="py3"><div class="highlight"><pre>
<span class="k">class</span> <span class="nc">MDP</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">actlist</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=.</span><span class="mi">9</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init</span> <span class="o">=</span> <span class="n">init</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">actlist</span> <span class="o">=</span> <span class="n">actlist</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">terminals</span> <span class="o">=</span> <span class="n">terminals</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">gamma</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s">"An MDP must have 0 &lt;= gamma &lt; 1"</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">R</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">actions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">terminals</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="k">None</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">actlist</span>
</pre></div></div>

<p>__init__メソッドでは以下のパラメータを受け取ります:</p>

<ul>
<li>init: 初期状態.</li>
<li>actlist: 各状態でとれる行動.</li>
<li>terminals: 終了状態のリスト</li>
<li>gamma: 割引係数</li>
</ul>

<p><strong>R</strong>メソッドは各状態での報酬を返します。<br>
<strong>T</strong>メソッドは遷移モデルです。ここでは抽象メソッドとなっていますが、状態 $s$ で行動 $a$ を取った時に、次状態への遷移確率と次状態のタプル(probability, s')のリストを返します。GridMDPで実装していきます。<br>
<strong>actions</strong>メソッドは各状態でとれる行動のリストを返します。</p>

<h3>
<span id="gridmdpクラス" class="fragment"></span><a href="#gridmdp%E3%82%AF%E3%83%A9%E3%82%B9"><i class="fa fa-link"></i></a>GridMDPクラス</h3>

<p>GridMDPクラスは基底クラスMDPの具象クラスです。例題で説明した迷路の世界を表現するためのクラスです。</p>

<div class="code-frame" data-lang="py3"><div class="highlight"><pre>
<span class="k">class</span> <span class="nc">GridMDP</span><span class="p">(</span><span class="n">MDP</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">terminals</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">gamma</span><span class="o">=.</span><span class="mi">9</span><span class="p">):</span>
        <span class="n">grid</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>  <span class="c"># because we want row 0 on bottom, not on top                                                                                                  </span>
        <span class="n">MDP</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">init</span><span class="p">,</span> <span class="n">actlist</span><span class="o">=</span><span class="n">orientations</span><span class="p">,</span>
                     <span class="n">terminals</span><span class="o">=</span><span class="n">terminals</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="k">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">states</span><span class="o">.</span><span class="n">add</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">action</span> <span class="ow">is</span> <span class="k">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">state</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[(</span><span class="mf">0.8</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">go</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)),</span>
                    <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">go</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">turn_right</span><span class="p">(</span><span class="n">action</span><span class="p">))),</span>
                    <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">go</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">turn_left</span><span class="p">(</span><span class="n">action</span><span class="p">)))]</span>

    <span class="k">def</span> <span class="nf">go</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">direction</span><span class="p">):</span>
        <span class="n">state1</span> <span class="o">=</span> <span class="n">vector_add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">direction</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state1</span> <span class="k">if</span> <span class="n">state1</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">states</span> <span class="k">else</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">to_grid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mapping</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">([[</span><span class="n">mapping</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="k">None</span><span class="p">)</span>
                               <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cols</span><span class="p">)]</span>
                              <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rows</span><span class="p">)]))</span>

    <span class="k">def</span> <span class="nf">to_arrows</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
        <span class="n">chars</span> <span class="o">=</span> <span class="p">{(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s">'&gt;'</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="s">'^'</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="s">'&lt;'</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="s">'v'</span><span class="p">,</span> <span class="k">None</span><span class="p">:</span> <span class="s">'.'</span><span class="p">}</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_grid</span><span class="p">({</span><span class="n">s</span><span class="p">:</span> <span class="n">chars</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
</pre></div></div>

<p>__init__メソッドが受け取る引数はほぼMDPクラスと同じですが、grid引数を受け取る点が違います。gridには各状態での報酬が格納されています。以下のような感じで渡します。Noneは壁を表しています。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">GridMDP</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">]],</span>
        <span class="n">terminals</span><span class="o">=</span><span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
</pre></div></div>

<p><strong>go</strong>メソッドは指定した方向に移動した場合の状態を返します。<br>
<strong>T</strong>メソッドはMDPで説明したとおりです。ここでは実際に実装されています。<br>
<strong>to_grid</strong>や<strong>to_arrows</strong> メソッドは表示用のメソッドです。</p>

<h3>
<span id="価値反復法の実装" class="fragment"></span><a href="#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95%E3%81%AE%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>価値反復法の実装</h3>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="p">{</span><span class="n">s</span><span class="p">:</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">states</span><span class="p">}</span>
    <span class="n">R</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">R</span><span class="p">,</span> <span class="n">mdp</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">mdp</span><span class="o">.</span><span class="n">gamma</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">U1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
            <span class="n">U1</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="nb">max</span><span class="p">([</span><span class="nb">sum</span><span class="p">([</span><span class="n">p</span> <span class="o">*</span> <span class="n">U</span><span class="p">[</span><span class="n">s1</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s1</span><span class="p">)</span> <span class="ow">in</span> <span class="n">T</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)])</span>
                                        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">(</span><span class="n">s</span><span class="p">)])</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">U1</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">-</span> <span class="n">U</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">epsilon</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gamma</span><span class="p">)</span> <span class="o">/</span> <span class="n">gamma</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">U</span>


<span class="k">def</span> <span class="nf">best_policy</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">U</span><span class="p">):</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">states</span><span class="p">:</span>
        <span class="n">pi</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">actions</span><span class="p">(</span><span class="n">s</span><span class="p">),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">expected_utility</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">mdp</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">pi</span>


<span class="k">def</span> <span class="nf">expected_utility</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">mdp</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">p</span> <span class="o">*</span> <span class="n">U</span><span class="p">[</span><span class="n">s1</span><span class="p">]</span> <span class="k">for</span> <span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">s1</span><span class="p">)</span> <span class="ow">in</span> <span class="n">mdp</span><span class="o">.</span><span class="n">T</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">)])</span>
</pre></div></div>

<p><strong>value_iteration</strong>関数は入力としてGridMDPのインスタンスと小さな値epsilonを受け取り、出力として各状態におけるU(s)を返します。以下のような感じです。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="o">&gt;&gt;</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">sequential_decision_environment</span><span class="p">)</span>
<span class="p">{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.2962883154554812</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mf">0.3984432178350045</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mf">0.5093943765842497</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.25386699846479516</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mf">0.649585681261095</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.3447542300124158</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mf">0.48644001739269643</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mf">0.7953620878466678</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.12987274656746342</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
 <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span> <span class="mf">1.0</span><span class="p">}</span>
</pre></div></div>

<p>そして、value_iteration関数で計算した結果を<strong>best_policy</strong>関数に渡してあげることで、最適な政策を求めます。</p>

<h3>
<span id="実行" class="fragment"></span><a href="#%E5%AE%9F%E8%A1%8C"><i class="fa fa-link"></i></a>実行</h3>

<p>以下が試すためのコードです。ただし、このままでは必要なモジュールをいくつかimportしていないので動きません。そのため実際に動かしたい場合は<a href="https://github.com/aimacode/aima-python" rel="nofollow noopener" target="_blank">ここ</a>のリポジトリをクローンして動かしてください。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">sequential_decision_environment</span> <span class="o">=</span> <span class="n">GridMDP</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">],</span>
                                           <span class="p">[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span>  <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                           <span class="p">[</span><span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.04</span><span class="p">]],</span>
                                          <span class="n">terminals</span><span class="o">=</span><span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>

<span class="n">pi</span> <span class="o">=</span> <span class="n">best_policy</span><span class="p">(</span><span class="n">sequential_decision_environment</span><span class="p">,</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">sequential_decision_environment</span><span class="p">,</span> <span class="o">.</span><span class="mo">01</span><span class="p">))</span>

<span class="n">print_table</span><span class="p">(</span><span class="n">sequential_decision_environment</span><span class="o">.</span><span class="n">to_arrows</span><span class="p">(</span><span class="n">pi</span><span class="p">))</span>
</pre></div></div>

<p>実行結果として、以下の政策を得られます。</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="o">&gt;</span>   <span class="o">&gt;</span>      <span class="o">&gt;</span>   <span class="o">.</span>
<span class="o">^</span>   <span class="bp">None</span>   <span class="o">^</span>   <span class="o">.</span>
<span class="o">^</span>   <span class="o">&gt;</span>      <span class="o">^</span>   <span class="o">&lt;</span>
</pre></div></div>

<h2>
<span id="q-learning-1" class="fragment"></span><a href="#q-learning-1"><i class="fa fa-link"></i></a>Q-Learning</h2>

<p>へんじがない、ただのしかばねのようだ</p>

<p>Q-Learningは体力のある時に書かせてください。</p>

<h1>
<span id="参考資料" class="fragment"></span><a href="#%E5%8F%82%E8%80%83%E8%B3%87%E6%96%99"><i class="fa fa-link"></i></a>参考資料</h1>

<p>Udacityの講義動画。文字は読みにくいが、話は分かりやすい。</p>

<ul>
<li><a href="https://www.udacity.com/course/machine-learning-reinforcement-learning--ud820" rel="nofollow noopener" target="_blank">Machine Learning: Reinforcement Learning</a></li>
</ul>

<p>日本語の資料</p>

<ul>
<li><a href="http://www.sist.ac.jp/%7Ekanakubo/research/reinforcement_learning.html" rel="nofollow noopener" target="_blank">強化学習</a></li>
<li><a href="http://sysplan.nams.kyushu-u.ac.jp/gen/papers/paper2012/A_BasisOfRL.pdf" rel="nofollow noopener" target="_blank">強化学習の基礎</a></li>
<li><a href="http://www.jnns.org/niss/2000/lecturenote/lecturenote_koike.pdf" rel="nofollow noopener" target="_blank">強化学習の基礎</a></li>
<li><a href="http://yamaimo.hatenablog.jp/entry/2015/09/03/200000" rel="nofollow noopener" target="_blank">強化学習について学んでみた。</a></li>
<li><a href="http://www.sakurai.comp.ae.keio.ac.jp/classes/infosem-class/2009/12RL.pdf" rel="nofollow noopener" target="_blank">情報意味論(12)強化学習</a></li>
<li><a href="http://mikilab.doshisha.ac.jp/dia/seminar/2001/pdf/ec_03.pdf" rel="nofollow noopener" target="_blank">Emergent Computation</a></li>
<li><a href="http://d.hatena.ne.jp/poor_code/20090628/1246176165" rel="nofollow noopener" target="_blank">Q学習による最短経路学習</a></li>
</ul>

<p>英語の資料</p>

<ul>
<li><a href="http://www2.econ.iastate.edu/tesfatsi/RLUsersGuide.ICAC2005.pdf" rel="nofollow noopener" target="_blank">Reinforcement Learning: A User’s Guide</a></li>
<li><a href="https://cs.uwaterloo.ca/%7Eppoupart/ICML-07-tutorial-slides/icml07-brl-tutorial-part2-intro-ghavamzadeh.pdf" rel="nofollow noopener" target="_blank">Introduction to Reinforcement Learning</a></li>
<li><a href="http://ssll.cecs.anu.edu.au/files/slides/sanner.pdf" rel="nofollow noopener" target="_blank">Introduction Reinforcement Learning</a></li>
</ul>

<p>Value Iterationに関して図を交えてわかりやすく解説している資料</p>

<ul>
<li><a href="http://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa12/slides/mdps-exact-methods.pdf" rel="nofollow noopener" target="_blank">Markov Decision Processes and Exact Solution Methods: </a></li>
<li><a href="http://people.eecs.berkeley.edu/%7Epabbeel/cs287-fa11/slides/mdps-intro-value-iteration.pdf" rel="nofollow noopener" target="_blank">Markov Decision Processes Value Iteration</a></li>
<li><a href="http://www.ccs.neu.edu/home/rplatt/cs5100_2015/slides/mdps.pdf" rel="nofollow noopener" target="_blank">Markov Decision Processes</a></li>
</ul>

<p>Value Iteration実装するときに参考になる資料</p>

<ul>
<li><a href="http://miromannino.com/artificial-intelligence-markov-decision-process/" rel="nofollow noopener" target="_blank">Artificial Intelligence: Markov Decision Process</a></li>
<li><a href="http://aima.cs.berkeley.edu/python/mdp.html" rel="nofollow noopener" target="_blank">Artificial Intelligence: A Modern Approach</a></li>
<li><a href="http://artint.info/html/ArtInt_227.html" rel="nofollow noopener" target="_blank">Value Iteration</a></li>
<li><a href="https://vrde.wordpress.com/2008/01/13/pythonic-markov-decision-process-mdp/" rel="nofollow noopener" target="_blank">Pythonic Markov Decision Process (MDP)</a></li>
</ul>

<p>価値関数の数式展開が丁寧</p>

<ul>
<li><a href="http://www.cs.upc.edu/%7Emmartin/Ag4-4x.pdf" rel="nofollow noopener" target="_blank">Reinforcement Learning Searching for optimal policies I:Bellman equations and optimal policies</a></li>
</ul>


<div class="hidden"><form class="js-task-list-update" action="/Hironsan/items/56f6c0b2f4cfd28dd906" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="TQrHZihZrsPfAjqDkt2p6rq2om5rX7JJuexVAWI5+u7oUJvNdsHiSFWQ+hNsQMUuNM6czOBcLBHUg0fMyzERJQ==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1489379967" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
# はじめに
みなさん、強化学習してますか？
強化学習はロボットや、囲碁や将棋のようなゲーム、対話システム等に応用できる楽しい技術です。

強化学習とは、試行錯誤を通じて環境に適応する学習制御の枠組みです。教師あり学習では入力に対する正しい出力を与えて学習させました。強化学習では、入力に対する正しい出力を与える代わりに、一連の行動に対する良し悪しを評価する「報酬」というスカラーの評価値が与え、これを手がかりに学習を行います。以下に強化学習の枠組みを示します。
![rl_concept.png](https://qiita-image-store.s3.amazonaws.com/0/77079/800cdce3-132d-87f2-5472-532261a5b13c.png)

1. エージェントは時刻 $t$ において環境の状態 $s_t$ を観測
2. 観測した状態から行動 $a_t$ を決定
3. エージェントは行動を実行
4. 環境は新しい状態 $s_{t+1}$ に遷移
5. 遷移に応じた報酬 $r_{t+1}$ を獲得
6. 学習する
7. ステップ1から繰り返す

強化学習の目的は、エージェントが取得する利得（累積報酬）を最大化するような、状態から行動へのマッピング（政策）を獲得することです。

&lt;!--
強化学習とは、試行錯誤を通じて環境に適応する学習制御の枠組みです。教師あり学習と異なり、状態入力に対する正しい行動出力を明示的に示す教師が存在しない代わりに、一連の行動に対して結果としての良し悪しを評価する「報酬」というスカラーの評価値が与えられ、これを手がかりに学習を行います。しかし、報酬や状態遷移には不確実性や時間的遅れがあるため、一般に行動を実行した直後の報酬を見るだけでは、学習主体はその行動が正しかったかどうかを判断できないという困難を伴います。

強化学習の枠組みを以下に示します。
![rl_framework1.png](https://qiita-image-store.s3.amazonaws.com/0/77079/913066da-3bda-8e73-88a2-a5cfadcac58d.png)

![rl_framework2.png](https://qiita-image-store.s3.amazonaws.com/0/77079/193f6d15-df91-b74d-33d8-6d3285d79b05.png)

1. エージェントは時刻 $t$ において環境の状態観測 $s_t$ に応じて意思決定を行い行動 $a_t$ を出力
2. エージェントの行動により環境は $s_{t+1}$ に状態遷移し、その遷移に応じた報酬 $r_t$ をエージェントへ与える
3. 時刻 $t$ を $t+1$ に進めてステップ1へ戻る
--&gt;

強化学習では上記の枠組みをマルコフ決定過程(Markov decision processes: MDP)によってモデル化し、学習アルゴリズムを考えるといったことが行われます。本稿では、強化学習について理論編と実践編に分けて解説します。理論編では、MDPおよび学習アルゴリズムについて説明し、実践編ではプログラミング言語Pythonを用いて理論編で説明したことを実装していきます。順番に読むことをおすすめしますが、「こまけぇこたぁいいんだよ！」という方は理論編を飛ばして実践編を読んでください。

# 理論編
まず、強化学習の理論を例を交えて説明していきます。ここでは例題の問題設定を説明した後、マルコフ決定過程について説明します。マルコフ決定過程について理解できたところで、その学習方法について説明します。

## 問題設定
さて、MDPを説明するにあたって以下のような迷路の世界を考えてみましょう。

![grid.png](https://qiita-image-store.s3.amazonaws.com/0/77079/d2631adc-c8f2-016e-a5ba-9f9c09c4ff7c.png)


迷路の世界のルールは以下の通りです。

* エージェントはあるグリッドの中にいます
* エージェントは壁(灰色)を越えられません
* エージェントは自分が思った通りに動けるとは限りません
 * 80%の確率で正しく動けますが、10%の確率で動こうと思っていた方向の左側へ動き、もう10%の確率で右側へ動きます
* 動こうと思った場所に壁があった場合は、その場にとどまります
* エージェントは各時刻に報酬を受け取ります
* ゴール(宝箱)やトラップ(爆発)にたどり着くとゲーム終了し、スタート位置に戻ります

確率的な遷移について以下の図を用いてもう少し詳しく説明します。
![transition2.png](https://qiita-image-store.s3.amazonaws.com/0/77079/ba75a1b4-63a6-f796-794a-d40c9e380a3e.png)

ここではロボットは上に動こうとしています。このとき80%の確率で上に動けますが、10%の確率で左側に動き、残りの10%の確率で右側に動いてしまいます。遷移確率は以下の表のようになります。

| $s&#39;$  | $P(s&#39; \mid s_1, a)$  |
|---|---|
| $s_2$  | 0.1  |
| $s_3$  | 0.8  |
| $s_4$  | 0.1  |


## マルコフ決定過程(Markov decision processes: MDP)
さて、問題設定はわかったでしょうか？わかったところで、問題をMDPでモデル化します。

MDPは以下の4項組で定義されます

```math
M = (S, A, T, R)
```

それぞれ何なのかというと、

* 状態集合: $s \in S$
* 行動集合: $a \in A$
* 遷移関数: $T(s, a, s&#39;) \sim P(s&#39; \mid s, a)$
* 報酬関数: $R(s)$, $R(s, a)$, $R(s,a,s&#39;)$

それぞれ上で説明した問題設定に当てはめて考えてみます。

* 状態集合は環境がとりうる状態の集合を表しており、例題の場合はグリッドのすべてのセルを表しています。
* 行動集合はエージェントがとりうる行動の種類のことです。例題の場合、エージェントは上下左右に動くことができるので4種類です。
* 遷移関数とは、ある状態 $s$ で行動 $a$ を取った時に次の状態 $s&#39;$ へどのくらいの確率で遷移するかということを表しています。上の図で説明した通りです。
* 報酬は観測した状態から取るべき行動を決定し、新たに遷移した結果として環境から受け取る値のことです。後で詳しく説明します。

このMDPの目的は利得(=累積報酬)を最大化する行動戦略を学習することです。この行動戦略のことを**政策**と呼びます。

### 政策についてもう少し詳しく
MDPでは最適な政策 $\pi^*: S \rightarrow A$ を得ることが目的です。政策は状態から行動へのマッピングを行う関数とみなせます。
政策は各状態で取るべき行動を教えてくれます。また、最適な政策は期待利得を最大化します。

遷移した時に得られる報酬を変化させたときの最適な政策の変化を見てみましょう。
![reward_policy.gif](https://qiita-image-store.s3.amazonaws.com/0/77079/16122f4e-712b-4e70-2597-5b900bef0f33.gif)
この図では状態遷移で得られる報酬をだんだん小さく（ペナルティを大きく）した時における政策の変化を示しています。政策は得られる期待利得を最大化するという考え方から学習します。そのため、状態遷移によるペナルティが小さければ遠回りしてでもゴールを目指そうとし、ペナルティが大きければ速やかにゲームを終了させる戦略を取るようになります。比較しやすいように静止画も用意しました。
![rewards_policy.png](https://qiita-image-store.s3.amazonaws.com/0/77079/0137ccc8-fdd1-825b-46e8-bc70710689f6.png)


### 報酬についてもう少し詳しく
さて、ここまでで政策を学習するためには累積報酬を最大化すると述べました。
各時刻に報酬 $R(s_t)$ を得ることができるので、累積報酬は以下のように表せます。

```math
\begin{align}
U([s_0, s_1, s_2,\ldots]) &amp;= R(s_0) + R(s_1) + R(s_2) + \cdots \\
    &amp;= \sum_{t=0}^{\infty}R(s_t)
\end{align}
```

このように単純な累積報酬を使ってもいいのですが、いくつかの理由から以下の式のように将来得られる報酬を割引係数 $\gamma (0 \leq \gamma &lt; 1)$ で割り引いた累積報酬が使われます。こうすることで、将来得られる報酬より現在得られる報酬を重視するようになります。

```math
\begin{align}
U([s_0, s_1, s_2,\ldots]) &amp;= R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots \\
    &amp;= \sum_{t=0}^{\infty}\gamma^t R(s_t)
\end{align}
```

このような割引報酬を使う理由としては以下があげられます。

* 計算的に値が発散しない
* 実際の環境では、時間がたつと環境が変化する可能性があるため、時系列のすべての報酬を同じ重みで考慮するのは妥当ではない

## 最適な政策の見つけ方
ここまででMDPというモデルについてはわかりました。残された問題はどうやって最適な政策を見つけるかということです。以下では最適な政策を見つけるアルゴリズムとして、価値反復法とQ-Learningについて説明します。

### 価値反復法(Value Iteration)
価値反復法では、状態 $s$ から最適な行動をとり続けた時の期待利得を計算します。得られる利得の期待値を考えるのは確率的な遷移が行われるためです。

この期待利得がわかれば、現在の状態 $s$ から将来にわたって得られる価値がわかるので、その価値を最大化するような行動を選択できるようにすればいいはずです。価値は以下の状態価値関数で求められます。複雑に見えますが、要するに(期待)割引累積報酬を計算しているだけです。

```math
U(s) = R(s) + \gamma \max_a\sum_{s&#39;} P(s&#39; \mid s,a) U(s&#39;)
```

この式の中にはmaxという非線形計算があり行列等で解くことは困難です。そのため反復的に計算を行って価値を徐々に更新していきます。これが価値反復法の名前の所以です。以下に価値反復法のアルゴリズムを示します。計算量は$O(|S|^2 |A|)$です。

&gt;1.すべての状態 $s$ について $U(s)$ を適当な値(ゼロなど)に初期化

&gt;2.すべての $U(s)$ について以下の式を計算して値を更新

&gt;```math
U(s) \leftarrow R(s) + \gamma \max_a\sum_{s&#39;} P(s&#39; \mid s,a) U(s&#39;)
```
&gt;3.ステップ2を値が収束するまで繰り返す

上記の式にはmax計算をする部分がありますが、その値が最大値となる行動aを記録しておく必要があります。そのため $U(s)$ だけでは最適な政策(行動)がわかりにくいという問題があります。そこで以下のQ関数を導入します。行動価値関数とも呼ばれます。

```math
Q^*(s,a) = R(s) + \gamma \sum_{s&#39;} P(s&#39; \mid s,a) \max_{a&#39;}Q^*(s&#39;, a&#39;)
```

**$Q^*(s,a)$**は、**状態$s$で行動aを選択後はずっと最適な政策を取り続ける場合**の割引累積報酬の期待値を表しています。
$U(s)$ と $Q^*(s,a)$ には、各状態における最大の値だけ保持するのか、それとも各行動に対して値を保持するのかの違いがあります。したがって、状態sにおける最大のQ値は $U(s)$ に等しくなり、このQ値を持つ行動aが最適な行動となります。以下の図がそのイメージです。
![v_and_q.png](https://qiita-image-store.s3.amazonaws.com/0/77079/35041da9-4449-c8c6-59a2-cd0a796105d1.png)

Q関数を用いた場合の価値反復法は以下のようになります。

&gt;1.すべての状態 $s$ について $Q^*(s,a)$ を適当な値(ゼロなど)に初期化

&gt;2.すべての $Q^*(s,a)$ について以下の式を計算して値を更新

&gt;```math
Q^*(s,a) \leftarrow R(s) + \gamma \sum_{s&#39;} P(s&#39; \mid s,a) \max_{a&#39;}Q^*(s&#39;, a&#39;)
```

&gt;3.ステップ2を値が収束するまで繰り返す


ここまでくれば最適な政策$\pi ^*(s)$を見つけるのは簡単です。以下で求められます。

```math
\pi^*(s) = arg\max_{a}Q^*(s,a)
```

### 計算例
せっかくなので試しに以下の赤い場所の価値を計算してみましょう。
![value_func_example.png](https://qiita-image-store.s3.amazonaws.com/0/77079/dac84db0-422a-3dba-f530-f38b61328473.png)
計算には以下の式を使います。

```math
U(s) \leftarrow R(s) + \gamma \max_a\sum_{s&#39;} P(s&#39; \mid s,a) U(s&#39;)
```
&lt;!--

```math
V^*(s) \leftarrow \max_{a \in A} \sum_{s&#39; \in S}P(s&#39;|s,a) \Bigl[R(s,a,s&#39;) + \gamma V^*(s&#39;) \Bigr]
```
--&gt;
$\gamma = 0.5$、 $R(s) = -0.04$、遷移確率は進みたい方向に0.8、その左右に0.1ずつとします。

以上の条件のとき、赤い場所の値は・・・

```math
\begin{align}
U(s) &amp;= -0.04 + 0.5 \cdot (0.8 \cdot 1 + 0.1 \cdot 0 + 0.1 \cdot 0)\\
&amp;= -0.04 + 0.4\\
&amp;= 0.36
\end{align}
```
&lt;!--

```math
\begin{align}
V^*(s) &amp;= 0.8 \cdot (-0.04 + 0.5 \cdot 1) + 0.1 \cdot (-0.04 + 0.5 \cdot 0) + 0.1 \cdot (-0.04 + 0.5 \cdot 0)\\
&amp;= 0.8 \cdot 0.46 + (-0.004) \cdot 2\\
&amp;= 0.36
\end{align}
```
--&gt;
と求められます。

### Q-Learning
さて、価値反復法で最適な政策が求められて一安心。そうは問屋がおろしません。何が問題なのでしょうか？

今までは状態遷移確率についてわかっていました。しかし現実の問題を考えると状態遷移確率がわかっていることは珍しいです。チェスや将棋を思い浮かべてみてください。ある盤面（状態）から別の盤面（別の状態）へ遷移する確率はあらかじめ与えられているでしょうか？与えられていませんよね？そのような場合は価値反復法は使えません。

そこで出てくるのが価値反復法の強化学習版ともいえるQ-Learningです。Q-Learningでは
$\hat{Q}^{\*}(s,a)$の推定値$\hat{Q}^{\*}(s_t, a_t)$を計算します。Q-Learningを使うことで、遷移確率がわからない状況でも最適な政策の学習を行うことができます。Q-Learningのアルゴリズムは以下の通りです。

&gt;1.適当な値で $\hat Q^*(s_t, a_t)$ を初期化

&gt;2.以下の式を用いて行動価値の推定値を更新。ただし $\alpha$ は学習率で $0 \leq \alpha \leq 1$。

&gt;```math
\hat Q^*(s_t, a_t) \leftarrow \hat Q^*(s_t, a_t) + \alpha \Bigl[ r_{t+1} + \gamma \max_{a \in A}\hat Q^*(s_{t+1}, a) - \hat Q^*(s_t, a_t) \Bigr]
```
&gt;3.時間ステップ $t$ を $t+1$ へ進めて2へ戻る

この計算を繰り返すことで最適な $Q^*(s,a)$ を得ることができます。

&lt;!--
実際に価値反復法の更新式(Q関数版)とQ-Learningの更新式を比べてみると、価値反復法ではQ関数の計算に遷移確率 $P(s&#39; \mid s,a)$を用いているのに対し、Q-Learningを使う状況では未知なので使用していない。繰り返し計算を行うことで
--&gt;

# 実践編
上で説明したことをPythonでやったらどうなるかについて見ていきましょう。

## 価値反復法
まずは価値反復法からです。
価値反復法は[UC Berkeley](https://github.com/aimacode/aima-python)のコード(mdp.py)がきれいだったので、それを基に説明します。

### MDPクラス
まずは基底クラスとなるMDPから。これを継承してGridMDPクラスを作ります。

```py3
class MDP:

    def __init__(self, init, actlist, terminals, gamma=.9):
        self.init = init
        self.actlist = actlist
        self.terminals = terminals
        if not (0 &lt;= gamma &lt; 1):
            raise ValueError(&quot;An MDP must have 0 &lt;= gamma &lt; 1&quot;)
        self.gamma = gamma
        self.states = set()
        self.reward = {}

    def R(self, state):
	return self.reward[state]

    def T(self, state, action):
        raise NotImplementedError

    def actions(self, state):
        if state in self.terminals:
            return [None]
        else:
            return self.actlist
```
\_\_init\_\_メソッドでは以下のパラメータを受け取ります:

* init: 初期状態.
* actlist: 各状態でとれる行動.
* terminals: 終了状態のリスト
* gamma: 割引係数

**R**メソッドは各状態での報酬を返します。
**T**メソッドは遷移モデルです。ここでは抽象メソッドとなっていますが、状態 $s$ で行動 $a$ を取った時に、次状態への遷移確率と次状態のタプル(probability, s&#39;)のリストを返します。GridMDPで実装していきます。
**actions**メソッドは各状態でとれる行動のリストを返します。

### GridMDPクラス
GridMDPクラスは基底クラスMDPの具象クラスです。例題で説明した迷路の世界を表現するためのクラスです。

```py3
class GridMDP(MDP):

    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):
        grid.reverse()  # because we want row 0 on bottom, not on top                                                                                                  
        MDP.__init__(self, init, actlist=orientations,
                     terminals=terminals, gamma=gamma)
        self.grid = grid
        self.rows = len(grid)
        self.cols = len(grid[0])
        for x in range(self.cols):
            for y in range(self.rows):
                self.reward[x, y] = grid[y][x]
                if grid[y][x] is not None:
                    self.states.add((x, y))

    def T(self, state, action):
        if action is None:
            return [(0.0, state)]
        else:
            return [(0.8, self.go(state, action)),
                    (0.1, self.go(state, turn_right(action))),
                    (0.1, self.go(state, turn_left(action)))]

    def go(self, state, direction):
        state1 = vector_add(state, direction)
        return state1 if state1 in self.states else state

    def to_grid(self, mapping):
        return list(reversed([[mapping.get((x, y), None)
                               for x in range(self.cols)]
                              for y in range(self.rows)]))

    def to_arrows(self, policy):
        chars = {(1, 0): &#39;&gt;&#39;, (0, 1): &#39;^&#39;, (-1, 0): &#39;&lt;&#39;, (0, -1): &#39;v&#39;, None: &#39;.&#39;}
        return self.to_grid({s: chars[a] for (s, a) in policy.items()})
```
\_\_init\_\_メソッドが受け取る引数はほぼMDPクラスと同じですが、grid引数を受け取る点が違います。gridには各状態での報酬が格納されています。以下のような感じで渡します。Noneは壁を表しています。

```py
GridMDP([[-0.04, -0.04, -0.04, +1],
        [-0.04, None,  -0.04, -1],
        [-0.04, -0.04, -0.04, -0.04]],
        terminals=[(3, 2), (3, 1)])
```

**go**メソッドは指定した方向に移動した場合の状態を返します。
**T**メソッドはMDPで説明したとおりです。ここでは実際に実装されています。
**to_grid**や**to_arrows** メソッドは表示用のメソッドです。

### 価値反復法の実装

```py
def value_iteration(mdp, epsilon=0.001):
    U1 = {s: 0 for s in mdp.states}
    R, T, gamma = mdp.R, mdp.T, mdp.gamma
    while True:
        U = U1.copy()
        delta = 0
        for s in mdp.states:
            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])
                                        for a in mdp.actions(s)])
            delta = max(delta, abs(U1[s] - U[s]))
        if delta &lt; epsilon * (1 - gamma) / gamma:
            return U


def best_policy(mdp, U):
    pi = {}
    for s in mdp.states:
        pi[s] = argmax(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))
    return pi


def expected_utility(a, s, U, mdp):
    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])
```

**value_iteration**関数は入力としてGridMDPのインスタンスと小さな値epsilonを受け取り、出力として各状態におけるU(s)を返します。以下のような感じです。

```py
&gt;&gt; value_iteration(sequential_decision_environment)
{(0, 0): 0.2962883154554812,
 (0, 1): 0.3984432178350045,
 (0, 2): 0.5093943765842497,
 (1, 0): 0.25386699846479516,
 (1, 2): 0.649585681261095,
 (2, 0): 0.3447542300124158,
 (2, 1): 0.48644001739269643,
 (2, 2): 0.7953620878466678,
 (3, 0): 0.12987274656746342,
 (3, 1): -1.0,
 (3, 2): 1.0}
```

そして、value_iteration関数で計算した結果を**best_policy**関数に渡してあげることで、最適な政策を求めます。

### 実行
以下が試すためのコードです。ただし、このままでは必要なモジュールをいくつかimportしていないので動きません。そのため実際に動かしたい場合は[ここ](https://github.com/aimacode/aima-python)のリポジトリをクローンして動かしてください。

```py
sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],
                                           [-0.04, None,  -0.04, -1],
                                           [-0.04, -0.04, -0.04, -0.04]],
                                          terminals=[(3, 2), (3, 1)])

pi = best_policy(sequential_decision_environment, value_iteration(sequential_decision_environment, .01))

print_table(sequential_decision_environment.to_arrows(pi))
```

実行結果として、以下の政策を得られます。

```py
&gt;   &gt;      &gt;   .
^   None   ^   .
^   &gt;      ^   &lt;
```

## Q-Learning
へんじがない、ただのしかばねのようだ

Q-Learningは体力のある時に書かせてください。

# 参考資料
Udacityの講義動画。文字は読みにくいが、話は分かりやすい。

* [Machine Learning: Reinforcement Learning](https://www.udacity.com/course/machine-learning-reinforcement-learning--ud820)

日本語の資料

* [強化学習](http://www.sist.ac.jp/~kanakubo/research/reinforcement_learning.html)
* [強化学習の基礎](http://sysplan.nams.kyushu-u.ac.jp/gen/papers/paper2012/A_BasisOfRL.pdf)
* [強化学習の基礎](http://www.jnns.org/niss/2000/lecturenote/lecturenote_koike.pdf)
* [強化学習について学んでみた。](http://yamaimo.hatenablog.jp/entry/2015/09/03/200000)
* [情報意味論(12)強化学習](http://www.sakurai.comp.ae.keio.ac.jp/classes/infosem-class/2009/12RL.pdf)
* [Emergent Computation](http://mikilab.doshisha.ac.jp/dia/seminar/2001/pdf/ec_03.pdf)
* [Q学習による最短経路学習](http://d.hatena.ne.jp/poor_code/20090628/1246176165)

英語の資料

* [Reinforcement Learning: A User’s Guide](http://www2.econ.iastate.edu/tesfatsi/RLUsersGuide.ICAC2005.pdf)
* [Introduction to Reinforcement Learning](https://cs.uwaterloo.ca/~ppoupart/ICML-07-tutorial-slides/icml07-brl-tutorial-part2-intro-ghavamzadeh.pdf)
* [Introduction Reinforcement Learning](http://ssll.cecs.anu.edu.au/files/slides/sanner.pdf)

Value Iterationに関して図を交えてわかりやすく解説している資料

* [Markov Decision Processes and Exact Solution Methods: ](http://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf)
* [Markov Decision Processes Value Iteration](http://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/mdps-intro-value-iteration.pdf)
* [Markov Decision Processes](http://www.ccs.neu.edu/home/rplatt/cs5100_2015/slides/mdps.pdf)


Value Iteration実装するときに参考になる資料

* [Artificial Intelligence: Markov Decision Process](http://miromannino.com/artificial-intelligence-markov-decision-process/)
* [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/python/mdp.html)
* [Value Iteration](http://artint.info/html/ArtInt_227.html)
* [Pythonic Markov Decision Process (MDP)](https://vrde.wordpress.com/2008/01/13/pythonic-markov-decision-process-mdp/)

価値関数の数式展開が丁寧

* [Reinforcement Learning Searching for optimal policies I:Bellman equations and optimal policies](http://www.cs.upc.edu/~mmartin/Ag4-4x.pdf)

&lt;!--

* https://www.cs.tcd.ie/~luzs/t/cs7032/mdps-notes.pdf
* http://mas.cs.umass.edu/classes/cs683/lectures-2010/Lec13_MDP2-F2010-4up.pdf
* http://chercheurs.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf
* http://castlelab.princeton.edu/ORF569papers/Powell_ADP_2ndEdition_Chapter%203.pdf
* http://www.cs.cmu.edu/~guestrin/Class/10701/slides/mdps-rl.pdf

--&gt;
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Pythonではじめる強化学習 by @Hironsan13 on @Qiita" data-url="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Pythonではじめる強化学習" href="http://b.hatena.ne.jp/entry/http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/Hironsan"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/Hironsan">Hironsan</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">4816</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;Hironsan&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-491998f3-f910-4af1-8da6-08fc2b13140d"></div>
    <div id="UserFollowButton-react-component-491998f3-f910-4af1-8da6-08fc2b13140d"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/6425787ccbee75dfae36">機械学習を使って作る対話システム</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/8ad9b11bcc0c618ec5e2">DeepLearningで上司を認識して画面を隠す</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/20951116a2650f45fea5">学年ビリのアホが1年半でTOEICスコアを300点から840点に上げた英語勉強法の話</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/326b66711eb4196aa9d4">【チュートリアル】機械学習を使って30分で固有表現抽出器を作る</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/ca0b176fd859490dde08">まだ機械学習の論文を追うのに消耗してるの？それBotで解決したよ</a></li></ul></section><section class="itemsShowAuthorInfo_organization"><h5 class="itemsShowAuthorInfo_organizationTitle">ORGANIZATION</h5><span itemprop="memberOf" itemscope="" itemtype="http://schema.org/Organization"><a itemprop="url" href="/organizations/tis"><img alt="TIS株式会社" class="itemsShowAuthorInfo_organizationLogo" itemprop="image" src="https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/5710e4c30854dd4ab3658e7f585930ab0d81a12c/original.jpg?1484790468" /></a></span></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\&quot;\u003eはじめに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%90%86%E8%AB%96%E7%B7%A8\&quot;\u003e理論編\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%95%8F%E9%A1%8C%E8%A8%AD%E5%AE%9A\&quot;\u003e問題設定\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%9E%E3%83%AB%E3%82%B3%E3%83%95%E6%B1%BA%E5%AE%9A%E9%81%8E%E7%A8%8Bmarkov-decision-processes-mdp\&quot;\u003eマルコフ決定過程(Markov decision processes: MDP)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%94%BF%E7%AD%96%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%82%82%E3%81%86%E5%B0%91%E3%81%97%E8%A9%B3%E3%81%97%E3%81%8F\&quot;\u003e政策についてもう少し詳しく\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%A0%B1%E9%85%AC%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%82%82%E3%81%86%E5%B0%91%E3%81%97%E8%A9%B3%E3%81%97%E3%81%8F\&quot;\u003e報酬についてもう少し詳しく\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%9C%80%E9%81%A9%E3%81%AA%E6%94%BF%E7%AD%96%E3%81%AE%E8%A6%8B%E3%81%A4%E3%81%91%E6%96%B9\&quot;\u003e最適な政策の見つけ方\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95value-iteration\&quot;\u003e価値反復法(Value Iteration)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%88%E7%AE%97%E4%BE%8B\&quot;\u003e計算例\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#q-learning\&quot;\u003eQ-Learning\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%AE%9F%E8%B7%B5%E7%B7%A8\&quot;\u003e実践編\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95\&quot;\u003e価値反復法\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#mdp%E3%82%AF%E3%83%A9%E3%82%B9\&quot;\u003eMDPクラス\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#gridmdp%E3%82%AF%E3%83%A9%E3%82%B9\&quot;\u003eGridMDPクラス\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E4%BE%A1%E5%80%A4%E5%8F%8D%E5%BE%A9%E6%B3%95%E3%81%AE%E5%AE%9F%E8%A3%85\&quot;\u003e価値反復法の実装\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%AE%9F%E8%A1%8C\&quot;\u003e実行\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#q-learning-1\&quot;\u003eQ-Learning\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8F%82%E8%80%83%E8%B3%87%E6%96%99\&quot;\u003e参考資料\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-6b6a950c-d7b3-4a2d-b5e6-3cf387a56b6b"></div>
    <div id="Toc-react-component-6b6a950c-d7b3-4a2d-b5e6-3cf387a56b6b"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:265,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="K-N"><a itemprop="url" href="/K-N"><img alt="K-N" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/99622/profile-images/1473707847" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="mlmct"><a itemprop="url" href="/mlmct"><img alt="mlmct" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/105404/profile-images/1473709605" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="mir0401"><a itemprop="url" href="/mir0401"><img alt="mir0401" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/98098/profile-images/1473707397" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="yasuyuki0722"><a itemprop="url" href="/yasuyuki0722"><img alt="yasuyuki0722" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/48350/profile-images/1473691321" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hibari_ren"><a itemprop="url" href="/hibari_ren"><img alt="hibari_ren" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/122410/profile-images/1473764063" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ueno3"><a itemprop="url" href="/ueno3"><img alt="ueno3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/75359/profile-images/1473700135" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Rydeen2045"><a itemprop="url" href="/Rydeen2045"><img alt="Rydeen2045" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77151/profile-images/1473700733" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="mettoboshi"><a itemprop="url" href="/mettoboshi"><img alt="mettoboshi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/42925/profile-images/1473689342" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="mofoolog"><a itemprop="url" href="/mofoolog"><img alt="mofoolog" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/73019/profile-images/1473699355" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="dnby"><a itemprop="url" href="/dnby"><img alt="dnby" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/51522/profile-images/1473692467" /></a></div></div><div class="ArticleFooter__user"><a href="/Hironsan/items/56f6c0b2f4cfd28dd906/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/56f6c0b2f4cfd28dd906/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/Hironsan/items/56f6c0b2f4cfd28dd906.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/eve_yk/items/2ace6d4c1dad7e912df1#_reference-a1a1585799bf6f055124"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" />強化学習で参考になったサイトまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-09-10T12:17:02+00:00">6 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/yukiB/items/0a3faa759ca5561e12f8#_reference-01ac2b38ac9d960cdd14"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/59864/profile-images/1473695058" />[Python]強化学習(DQN)を実装しながらKerasに慣れる</a><time class="references_datetime js-dateTimeView" datetime="2017-02-17T02:43:48+00:00">about 1 month ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Pythonではじめる強化学習 by @Hironsan13 on @Qiita" data-url="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Pythonではじめる強化学習" href="http://b.hatena.ne.jp/entry/http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eQ-Learningがとても読みたいなぁーーー！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-06-11T00:08:37+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:561849,&quot;is_team&quot;:false,&quot;item_id&quot;:398800,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Q-Learningがとても読みたいなぁーーー！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#comment-ab4bc585752c512848be&quot;,&quot;user&quot;:{&quot;contribution&quot;:885,&quot;created_at&quot;:&quot;2015-12-19T13:53:22+09:00&quot;,&quot;id&quot;:106267,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/106267/profile-images/1484913680&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Dr_ASA&quot;},&quot;uuid&quot;:&quot;ab4bc585752c512848be&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eとても面白い。私もQ-Learning実践編が読みたいです。体力回復祈ってます。ホイミ！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-06-19T11:39:36+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:568501,&quot;is_team&quot;:false,&quot;item_id&quot;:398800,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;とても面白い。私もQ-Learning実践編が読みたいです。体力回復祈ってます。ホイミ！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#comment-64d256233f1098d34ecb&quot;,&quot;user&quot;:{&quot;contribution&quot;:41,&quot;created_at&quot;:&quot;2013-11-29T13:26:36+09:00&quot;,&quot;id&quot;:33286,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/33286/profile-images/1473686089&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;fuji70&quot;},&quot;uuid&quot;:&quot;64d256233f1098d34ecb&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e今日から強化学習の勉強をはじめました。\u003cbr\u003e\nとても興味深い記事です。半分くらい分かりました。ｗ\u003cbr\u003e\nところで、このPhthon programはGithubのmdp.pyですね。\u003cbr\u003e\nどれだか最初分かりませんでした。ｗ\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2017-03-10T20:06:17+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:757497,&quot;is_team&quot;:false,&quot;item_id&quot;:398800,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;今日から強化学習の勉強をはじめました。\nとても興味深い記事です。半分くらい分かりました。ｗ\nところで、このPhthon programはGithubのmdp.pyですね。\nどれだか最初分かりませんでした。ｗ\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#comment-d627262dba33bfaa0584&quot;,&quot;user&quot;:{&quot;contribution&quot;:1,&quot;created_at&quot;:&quot;2017-03-10T10:54:47+09:00&quot;,&quot;id&quot;:169640,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=50&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;noriyukin&quot;},&quot;uuid&quot;:&quot;d627262dba33bfaa0584&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eおお！\u003cbr\u003e\n確かにどのファイルか書き忘れていましたw\u003c/p\u003e\n\n\u003cp\u003eありがとうございます！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2017-03-13T13:40:23+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:758240,&quot;is_team&quot;:false,&quot;item_id&quot;:398800,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;おお！\n確かにどのファイルか書き忘れていましたw\n\nありがとうございます！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906#comment-4dec0780db18df7a9cc3&quot;,&quot;user&quot;:{&quot;contribution&quot;:4816,&quot;created_at&quot;:&quot;2015-04-28T22:11:02+09:00&quot;,&quot;id&quot;:77079,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Hironsan&quot;},&quot;uuid&quot;:&quot;4dec0780db18df7a9cc3&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:398800,&quot;uuid&quot;:&quot;56f6c0b2f4cfd28dd906&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;Hironsan&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:77079,&quot;url_name&quot;:&quot;Hironsan&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709&quot;},{&quot;id&quot;:106267,&quot;url_name&quot;:&quot;Dr_ASA&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/106267/profile-images/1484913680&quot;},{&quot;id&quot;:33286,&quot;url_name&quot;:&quot;fuji70&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/33286/profile-images/1473686089&quot;},{&quot;id&quot;:169640,&quot;url_name&quot;:&quot;noriyukin&quot;,&quot;profile_image_url&quot;:&quot;https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=50&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-8e463525-1d79-4619-869a-0795039683dd"></div>
    <div id="CommentListContainer-react-component-8e463525-1d79-4619-869a-0795039683dd"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="36R8/p+752s0Pz+ySqXeRuY1N4RS6hbsLuNcqHGEvRR6/iBVwSOr4L6t/yK0OLKCaE0JJtnpiLRDjE5l2IxW3w==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Hironsan/items/56f6c0b2f4cfd28dd906" /><input type="hidden" name="item_uuid" id="item_uuid" value="56f6c0b2f4cfd28dd906" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906", "id": 398800, "uuid": "56f6c0b2f4cfd28dd906" }</script><script class="js-user" type="application/json">{&quot;id&quot;:77079,&quot;url_name&quot;:&quot;Hironsan&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="nM1fxDipXVUR5SP+di2duYnciLvfyMWwEUWNfmjh2jE5lwNvZjER3pt3426IsPF9B6S2GVTLW+h8Kp+zwekx+g==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Hironsan/items/56f6c0b2f4cfd28dd906" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>