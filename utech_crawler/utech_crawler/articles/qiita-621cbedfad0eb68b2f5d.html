<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="最近、Computer Vision API を触っていて使い方を把握できてきたと共に、予想以上の性能の高さに驚いたのでせっかくだからまとめてみることにしました。
Computer Vision API とはそもそも何なのか、なぜすごいのか、実際使ってみるとどれほどの実力を叩きだすのかをご紹介した後、コードもお見せしたいと思います。
この記事を通して、コードを数行書くだけで手軽にこんなにパワフルな画像認識機能が使えるんだということが伝わり、ぜひ試していただけたらと思いま..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="kosfuji" name="twitter:creator" /><meta content="最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="最近、Computer Vision API を触っていて使い方を把握できてきたと共に、予想以上の性能の高さに驚いたのでせっかくだからまとめてみることにしました。
Computer Vision API とはそもそも何なのか、なぜすご..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="T0Cndal9dJ4S4kOHqmlv+/jOp+t4pGqAg25zFfgDaRFpN7asdkuQ8kFafRWIVFcSXK9yJdu857x298irV7fxXQ==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"kosfuji","type":"items","id":"621cbedfad0eb68b2f5d"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-0a78f44e-f4aa-44c8-b9cc-94cf2c89c6c4"></div>
    <div id="HeaderContainer-react-component-0a78f44e-f4aa-44c8-b9cc-94cf2c89c6c4"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/Python",        "name": "Python"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた</h1><ul class="TagList"><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="814"><a class="u-link-unstyled TagList__label" href="/tags/MachineLearning"><img alt="MachineLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/b85c97772bddbfbb48a8b116669349c7ec92e4bf/medium.jpg?1395227038" /><span>MachineLearning</span></a></li><li class="TagList__item" data-count="91"><a class="u-link-unstyled TagList__label" href="/tags/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98"><img alt="画像認識" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>画像認識</span></a></li><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">575</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="2 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>2</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:575,&quot;uuid&quot;:&quot;621cbedfad0eb68b2f5d&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="YoheiEn"><a itemprop="url" href="/YoheiEn"><img alt="YoheiEn" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/116025/profile-images/1473713113" /></a></li><li class="js-hovercard" data-hovercard-target-name="samacoba"><a itemprop="url" href="/samacoba"><img alt="samacoba" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/109458/profile-images/1481540220" /></a></li><li class="js-hovercard" data-hovercard-target-name="hat22"><a itemprop="url" href="/hat22"><img alt="hat22" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/131046/profile-images/1475732622" /></a></li><li class="js-hovercard" data-hovercard-target-name="junichia"><a itemprop="url" href="/junichia"><img alt="junichia" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/134539/profile-images/1473719327" /></a></li><li class="js-hovercard" data-hovercard-target-name="haray_isa"><a itemprop="url" href="/haray_isa"><img alt="haray_isa" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/81725/profile-images/1473702211" /></a></li><li class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></li><li class="js-hovercard" data-hovercard-target-name="yonahaty"><a itemprop="url" href="/yonahaty"><img alt="yonahaty" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/121941/profile-images/1473715075" /></a></li><li class="js-hovercard" data-hovercard-target-name="qqq"><a itemprop="url" href="/qqq"><img alt="qqq" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/66315/profile-images/1473697168" /></a></li><li class="js-hovercard" data-hovercard-target-name="izariuo440"><a itemprop="url" href="/izariuo440"><img alt="izariuo440" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/3970/profile-images/1473683719" /></a></li><li class="js-hovercard" data-hovercard-target-name="kogakem"><a itemprop="url" href="/kogakem"><img alt="kogakem" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/23212/profile-images/1473683783" /></a></li><li><a href="/kosfuji/items/621cbedfad0eb68b2f5d/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/kosfuji"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264" alt="1473709264" /></a> <a class="u-link-unstyled" href="/kosfuji">kosfuji</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-07-22T08:11:20+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-07-22">Edited at <time datetime="2016-08-05T09:35:10+09:00" itemprop="dateModified">2016-08-05</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/kosfuji/items/621cbedfad0eb68b2f5d/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">6</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/kosfuji/items/621cbedfad0eb68b2f5d/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(6)</span></a></li><li><a href="/kosfuji/items/621cbedfad0eb68b2f5d.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-621cbedfad0eb68b2f5d" itemprop="articleBody"><p>最近、Computer Vision API を触っていて使い方を把握できてきたと共に、予想以上の性能の高さに驚いたのでせっかくだからまとめてみることにしました。<br>
Computer Vision API とはそもそも何なのか、なぜすごいのか、実際使ってみるとどれほどの実力を叩きだすのかをご紹介した後、コードもお見せしたいと思います。<br>
この記事を通して、コードを数行書くだけで手軽にこんなにパワフルな画像認識機能が使えるんだということが伝わり、ぜひ試していただけたらと思います。</p>

<h2>
<span id="そもそもcomputer-vision-apiって何" class="fragment"></span><a href="#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82computer-vision-api%E3%81%A3%E3%81%A6%E4%BD%95"><i class="fa fa-link"></i></a>そもそも、Computer Vision APIって何？</h2>

<p>Microsoft が出している、REST で画像を投げると JSON 形式で画像を分析した結果を返してくれる API サービスのことです。<br>
画像に写っている物体を認識しタグを出力するのはもちろん、画像上の物体の状態や状況を動詞や形容詞でタグとして出力してくれます。<br>
（ex. water, sport, swimming, man）<br>
さらに、それらをベースに画像に1文でもっともらしいキャプションを付けてくれます。<br>
（ex. a man swimming in a pool of water）</p>

<p><a href="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h33_10.png" target="_blank" rel="nofollow noopener"><img src="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h33_10.png" alt="sample image"></a><br>
(<a href="https://www.microsoft.com/cognitive-services/en-us/computer-vision-api" rel="nofollow noopener" target="_blank">公式ページ</a>参照)</p>

<p>裏では Microsoft Research の最先端の研究成果を盛り込んだ最新の画像認識エンジンが動いており、それが一般ユーザー向けに開放されています。</p>

<h2>
<span id="computer-vision-api-のバックで活躍する-microsoft-research-の実力" class="fragment"></span><a href="#computer-vision-api-%E3%81%AE%E3%83%90%E3%83%83%E3%82%AF%E3%81%A7%E6%B4%BB%E8%BA%8D%E3%81%99%E3%82%8B-microsoft-research-%E3%81%AE%E5%AE%9F%E5%8A%9B"><i class="fa fa-link"></i></a>Computer Vision API のバックで活躍する Microsoft Research の実力</h2>

<p>最先端のといっても、そもそも Microsoft Research がどれほどこの分野に強いのかというのが気になります。<br>
画像認識の分野では <a href="http://image-net.org/" rel="nofollow noopener" target="_blank">ImageNet</a> という、膨大な量の画像と付随するアノテーションをオープンなデータセットとして公開しているプロジェクトがあります。そこが主催の <a href="http://image-net.org/challenges/LSVRC/2016/index" rel="nofollow noopener" target="_blank">Large Scale Visual Recognition Challenge</a> というざっくり言うと大量の画像データの画像認識精度を競い合うコンペティションがあって、その一番最新の大会（ILSVRC 2015）で、MSRA (Microsoft Research Asia)は、Google 等の研究部門や海外トップ大学の研究チームがこぞって競い合う中、多数のタスクで1位を取ってるんですね。</p>

<p><a href="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h53_22.png" target="_blank" rel="nofollow noopener"><img src="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h53_22.png" alt=""></a><br>
（<a href="http://image-net.org/challenges/LSVRC/2015/results" rel="nofollow noopener" target="_blank">ILSVRC2015 Results</a>からの引用）</p>

<p>学生時代にこの分野に近めな研究室にいたので、MS は"強い論文"をよく出しているというイメージはぼんやり持っていましたが、今回調べることで具体的な状況を知れてよかったです。<br>
この記事の最後に Computer Vision API に使われている関連論文も引用していますが、そちらを見てみると普通に最新の研究成果が使われていたりするので、だからすごいのかと納得です。</p>

<h2>
<span id="どれぐらいの精度なの" class="fragment"></span><a href="#%E3%81%A9%E3%82%8C%E3%81%90%E3%82%89%E3%81%84%E3%81%AE%E7%B2%BE%E5%BA%A6%E3%81%AA%E3%81%AE"><i class="fa fa-link"></i></a>どれぐらいの精度なの？</h2>

<p>実際に自分の画像を投げて分析結果を見てみましょう。<br>
今回は、Computer Vision API の画像認識機能の中で特に個人的に有用で面白いと思った、<strong>キャプション生成機能</strong>と<strong>タグ生成機能</strong>をご紹介します。<br>
勉強のために、今回は画像を上げて分析結果を表示する簡易 Web サービスを作って Azure 上に展開してみました。<br>
（コードを Git に上げて次回はそれについてもまとめたい）<br>
以下、スクショと感想です。</p>

<p>まずは　<strong>カフェで取ったコーヒーとパウンドケーキの写真</strong></p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/104326/355c1374-a616-5b58-8eb9-9853dea6bda0.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/104326/355c1374-a616-5b58-8eb9-9853dea6bda0.png" alt="image"></a></p>

<p>ケーキ1ピースとカップに入ったコーヒー1つ、と見事に正解を当てられました。<br>
タグを見てみると、テーブルやチョコレートや、食べかけを示す eaten まで入っていてびっくり。</p>

<p>次は　<strong>旅先で取った何気ない写真</strong><br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/104326/3d3c43d1-f309-bfbf-072b-b3511d64965c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/104326/3d3c43d1-f309-bfbf-072b-b3511d64965c.png" alt="image"></a></p>

<p>生成されたキャプションを見ると、教会の前に車が停められている、とまたまた見事に当てられました。<br>
画像内での物体の位置関係までキャプション内で表現されているのには感動しました。</p>

<p>こうやって出力されるタグを活用して、アプリや Web サービスに画像検索機能をつけたり、グルーピングを行ったりというのは簡単に実装できそうです。また Web サービス内に大量の画像が存在する場合は分析をかけてタグ総数でランキング付けをしてみるのも新たな洞察が得られて面白そう。</p>

<h2>
<span id="どう実装するの" class="fragment"></span><a href="#%E3%81%A9%E3%81%86%E5%AE%9F%E8%A3%85%E3%81%99%E3%82%8B%E3%81%AE"><i class="fa fa-link"></i></a>どう実装するの？</h2>

<p>どのようなコード、どれぐらいのコード量でこれが実現できるのかをお見せしたいと思います。<br>
Python2 系で書いてみました。Python3 系やC#, Java, JavaScript, Objective-C, PHP, Rubyの実装サンプルは<a href="https://dev.projectoxford.ai/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa" rel="nofollow noopener" target="_blank">こちら</a>の下部に載っています。</p>

<div class="code-frame" data-lang="python">
<div class="code-lang"><span class="bold">ComputerVisionAPI使用部分</span></div>
<div class="highlight"><pre>
<span class="kn">import</span> <span class="nn">httplib</span>
<span class="kn">import</span> <span class="nn">urllib</span>

<span class="n">image_url</span> <span class="o">=</span> <span class="s">''</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
    <span class="c"># Request headers</span>
    <span class="s">'Content-Type'</span><span class="p">:</span> <span class="s">'application/json'</span><span class="p">,</span>
    <span class="c"># 'Content-Type': 'application/octet-stream',</span>
    <span class="s">'Ocp-Apim-Subscription-Key'</span><span class="p">:</span> <span class="s">'Your key'</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">urlencode</span><span class="p">({</span>
    <span class="c"># Request parameters</span>
    <span class="s">'visualFeatures'</span><span class="p">:</span> <span class="s">'Description'</span><span class="p">,</span>
    <span class="c"># 'visualFeatures': 'Categories,Tags, Description, Faces, ImageType, Color, Adult',</span>
    <span class="c"># 'details': 'Celebrities'</span>
<span class="p">})</span>
<span class="n">conn</span> <span class="o">=</span> <span class="n">httplib</span><span class="o">.</span><span class="n">HTTPSConnection</span><span class="p">(</span><span class="s">'api.projectoxford.ai'</span><span class="p">)</span>
<span class="n">body</span> <span class="o">=</span> <span class="s">"""{'url': '</span><span class="si">%s</span><span class="s">'}"""</span> <span class="o">%</span> <span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
<span class="n">conn</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s">"POST"</span><span class="p">,</span> <span class="s">"/vision/v1.0/analyze?</span><span class="si">%s</span><span class="s">"</span> <span class="o">%</span> <span class="n">params</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">headers</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">conn</span><span class="o">.</span><span class="n">getresponse</span><span class="p">()</span>
<span class="n">caption_data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">conn</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>

<p>上記の例では画像URLを投げていますが、<br>
直接画像データを投げることも可能です。<br>
切り替えは、headersの中のContent-Typeの値を変更することで可能です。</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>
'Content-Type': 'application/json'  　　　　 画像URLを与える場合
'Content-Type': 'application/octet-stream'  直接画像データを与える場合
</pre></div></div>

<p>なので、直接データを投げる場合はoctet-streamを指定して、<br>
下記の部分を</p>

<div class="code-frame" data-lang="py"><div class="highlight"><pre>
<span class="n">body</span> <span class="o">=</span> <span class="s">"""{'url': '</span><span class="si">%s</span><span class="s">'}"""</span> <span class="o">%</span> <span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
<span class="n">conn</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s">"POST"</span><span class="p">,</span> <span class="s">"/vision/v1.0/analyze?</span><span class="si">%s</span><span class="s">"</span> <span class="o">%</span> <span class="n">params</span><span class="p">,</span> <span class="n">body</span><span class="p">,</span> <span class="n">headers</span><span class="p">)</span>
</pre></div></div>

<p>を下記に変更でいけます。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="n">file_name</span> <span class="o">=</span> <span class="s">''</span>
<span class="n">img</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_name</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">conn</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s">"POST"</span><span class="p">,</span> <span class="s">"/vision/v1.0/analyze?</span><span class="si">%s</span><span class="s">"</span> <span class="o">%</span> <span class="n">params</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">headers</span><span class="p">)</span>
</pre></div></div>

<p>paramsでは欲しい分析結果をパラメータで指定します。<br>
visualFeaturesに指定できる値は現段階では以下の7つです。</p>

<ul>
<li>Categories</li>
<li>Tags</li>
<li>Description</li>
<li>Faces</li>
<li>ImageType</li>
<li>Color</li>
<li>Adult</li>
</ul>

<p>（詳しくは<a href="https://dev.projectoxford.ai/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa" rel="nofollow noopener" target="_blank">こちら</a>のRequest parameters 参照）<br>
自分の写真を分析した上記の例は、visualFeatures に Description のみ（キャプション生成機能とタグ生成機能）を指定した場合の結果となっています。</p>

<p>visualFeatrues にすべて指定した場合は以下のようなJSONが返ってきます。<br>
公式ページ上で使われている画像を使用した場合の結果を見てみましょう。<br>
各タグごとの確信度やアダルト画像かのチェック、映り込んでいる人の顔の位置や性別と年齢等も得られるようになっています。</p>

<table>
<thead>
<tr>
<th style="text-align: center">分析画像</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><a href="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h55_23.png" target="_blank" rel="nofollow noopener"><img src="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h55_23.png" alt="image"></a></td>
</tr>
</tbody>
</table>

<div class="code-frame" data-lang="json">
<div class="code-lang"><span class="bold">返ってくるJSON</span></div>
<div class="highlight"><pre>
<span class="p">{</span>
  <span class="nt">"categories"</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"people_swimming"</span><span class="p">,</span>
      <span class="nt">"score"</span><span class="p">:</span> <span class="mf">0.98046875</span>
    <span class="p">}</span>
  <span class="p">],</span>
  <span class="nt">"adult"</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">"isAdultContent"</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">"isRacyContent"</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
    <span class="nt">"adultScore"</span><span class="p">:</span> <span class="mf">0.14750830829143524</span><span class="p">,</span>
    <span class="nt">"racyScore"</span><span class="p">:</span> <span class="mf">0.12601403892040253</span>
  <span class="p">},</span>
  <span class="nt">"tags"</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"water"</span><span class="p">,</span>
      <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.99964427947998047</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"sport"</span><span class="p">,</span>
      <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.95049923658370972</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"swimming"</span><span class="p">,</span>
      <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.90628182888031006</span><span class="p">,</span>
      <span class="nt">"hint"</span><span class="p">:</span> <span class="s2">"sport"</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"pool"</span><span class="p">,</span>
      <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.87875884771347046</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">"name"</span><span class="p">:</span> <span class="s2">"water sport"</span><span class="p">,</span>
      <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.631849467754364</span><span class="p">,</span>
      <span class="nt">"hint"</span><span class="p">:</span> <span class="s2">"sport"</span>
    <span class="p">}</span>
  <span class="p">],</span>
  <span class="nt">"description"</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">"tags"</span><span class="p">:</span> <span class="p">[</span>
      <span class="s2">"water"</span><span class="p">,</span>
      <span class="s2">"sport"</span><span class="p">,</span>
      <span class="s2">"swimming"</span><span class="p">,</span>
      <span class="s2">"pool"</span><span class="p">,</span>
      <span class="s2">"man"</span><span class="p">,</span>
      <span class="s2">"riding"</span><span class="p">,</span>
      <span class="s2">"blue"</span><span class="p">,</span>
      <span class="s2">"top"</span><span class="p">,</span>
      <span class="s2">"ocean"</span><span class="p">,</span>
      <span class="s2">"young"</span><span class="p">,</span>
      <span class="s2">"wave"</span><span class="p">,</span>
      <span class="s2">"bird"</span><span class="p">,</span>
      <span class="s2">"game"</span><span class="p">,</span>
      <span class="s2">"large"</span><span class="p">,</span>
      <span class="s2">"standing"</span><span class="p">,</span>
      <span class="s2">"body"</span><span class="p">,</span>
      <span class="s2">"frisbee"</span><span class="p">,</span>
      <span class="s2">"board"</span><span class="p">,</span>
      <span class="s2">"playing"</span>
    <span class="p">],</span>
    <span class="nt">"captions"</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
        <span class="nt">"text"</span><span class="p">:</span> <span class="s2">"a man swimming in a pool of water"</span><span class="p">,</span>
        <span class="nt">"confidence"</span><span class="p">:</span> <span class="mf">0.78501081244404836</span>
      <span class="p">}</span>
    <span class="p">]</span>
  <span class="p">},</span>
  <span class="nt">"requestId"</span><span class="p">:</span> <span class="s2">"your request Id"</span><span class="p">,</span>
  <span class="nt">"metadata"</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">"width"</span><span class="p">:</span> <span class="mi">1500</span><span class="p">,</span>
    <span class="nt">"height"</span><span class="p">:</span> <span class="mi">1155</span><span class="p">,</span>
    <span class="nt">"format"</span><span class="p">:</span> <span class="s2">"Jpeg"</span>
  <span class="p">},</span>
  <span class="nt">"faces"</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">"age"</span><span class="p">:</span> <span class="mi">29</span><span class="p">,</span>
      <span class="nt">"gender"</span><span class="p">:</span> <span class="s2">"Male"</span><span class="p">,</span>
      <span class="nt">"faceRectangle"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"left"</span><span class="p">:</span> <span class="mi">748</span><span class="p">,</span>
        <span class="nt">"top"</span><span class="p">:</span> <span class="mi">336</span><span class="p">,</span>
        <span class="nt">"width"</span><span class="p">:</span> <span class="mi">304</span><span class="p">,</span>
        <span class="nt">"height"</span><span class="p">:</span> <span class="mi">304</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">],</span>
  <span class="nt">"color"</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">"dominantColorForeground"</span><span class="p">:</span> <span class="s2">"Grey"</span><span class="p">,</span>
    <span class="nt">"dominantColorBackground"</span><span class="p">:</span> <span class="s2">"White"</span><span class="p">,</span>
    <span class="nt">"dominantColors"</span><span class="p">:</span> <span class="p">[</span>
      <span class="s2">"White"</span>
    <span class="p">],</span>
    <span class="nt">"accentColor"</span><span class="p">:</span> <span class="s2">"19A4B2"</span><span class="p">,</span>
    <span class="nt">"isBWImg"</span><span class="p">:</span> <span class="kc">false</span>
  <span class="p">},</span>
  <span class="nt">"imageType"</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">"clipArtType"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="nt">"lineDrawingType"</span><span class="p">:</span> <span class="mi">0</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>

<p>基本的には返ってくるタグは汎用的なものになっていますが（ex. プードルではなく犬）有名人に関しては、個人特定まで出来るようになっています。<br>
その機能を ON にしたい場合は、params 中の details で</p>

<div class="code-frame" data-lang="text"><div class="highlight"><pre>
'details' : 'Celebrities'
</pre></div></div>

<p>を指定すれば分析結果を JSON に含んでくれるようになります。<br>
有名人判定は、約20万もの人を特定できるらしく、他の触った人から聞くとなんとデーモン小暮にも対応しているらしいです！（年齢は何歳と判定されるのだろう…？）</p>

<h2>
<span id="どうやって始めるの" class="fragment"></span><a href="#%E3%81%A9%E3%81%86%E3%82%84%E3%81%A3%E3%81%A6%E5%A7%8B%E3%82%81%E3%82%8B%E3%81%AE"><i class="fa fa-link"></i></a>どうやって始めるの？</h2>

<p>Cognitive Services の利用の際には Subscription Key が必要になります。無料試用が可能なので<a href="https://www.microsoft.com/cognitive-services/en-us/subscriptions?productId=/products/54d873dd5eefd00dc474a0f4" rel="nofollow noopener" target="_blank">こちら</a>からご登録ください。Microsoft Account でのログインが必要となります。<br>
登録して得られた Subscription Key をサンプルコード上の Ocp-Apim-Subscription-Key で指定してください。<br>
有料プランもあるのでお試しではなくしっかり使いたい方は次の<strong>使用プラン</strong>の項目をご覧ください。</p>

<p>自分でコードを書く前にまずはどんなものか試してみたいという場合は、<a href="https://www.microsoft.com/cognitive-services/en-us/computer-vision-api" rel="nofollow noopener" target="_blank">公式解説ページ</a>を見てもらうと準備されている画像を選ぶ or 自分の画像をアップロードして確認できるようになっているのでおすすめです。</p>

<p><a href="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-21_14h51_45.png" target="_blank" rel="nofollow noopener"><img src="https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-21_14h51_45.png" alt="image"></a></p>

<p>使ってみてここもっとこうすればいいのに等色々と意見やアイデアが生まれた場合は、<br>
<a href="https://cognitive.uservoice.com/forums/356325-apis/category/165762-computer-vision-api" rel="nofollow noopener" target="_blank">Give your apps a human side</a> (Computer Vision API 開発チームが参考にするフィードバック共有ページ)をご利用ください。<br>
こういうふうにユーザーサイドの意見を直接開発チームに届けられるのはいいですね。<br>
各改善アイデアには投票ができるようになっていて、投票が集まれば上位に来るのでより注目を集めやすくなります。一緒に使い込んで良くしていきましょう！</p>

<h2>
<span id="使用プラン-201685-追記" class="fragment"></span><a href="#%E4%BD%BF%E7%94%A8%E3%83%97%E3%83%A9%E3%83%B3-201685-%E8%BF%BD%E8%A8%98"><i class="fa fa-link"></i></a>使用プラン (2016/8/5 追記)</h2>

<p>本日より、以前のお試しプラン Free に加え <strong>Standard plan</strong> が追加されました！<br>
まだ Cognitive Services 全体が Preview 段階なので SLA やサポートはありませんが、<br>
ビジネスユースでもご検討いただけるのではないかと思います。</p>

<table>
<thead>
<tr>
<th style="text-align: left">Plan</th>
<th style="text-align: left">Description</th>
<th style="text-align: left">Price</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left">Free</td>
<td style="text-align: left">5,000 transactions per month</td>
<td style="text-align: left">Free</td>
</tr>
<tr>
<td style="text-align: left"><strong>Standard</strong></td>
<td style="text-align: left">10 transactions per second</td>
<td style="text-align: left">$ 1.50 per 1000 transactions</td>
</tr>
</tbody>
</table>

<p><a href="https://portal.azure.com/#create/Microsoft.CognitiveServices/apitype/ComputerVision/pricingtier/S0" rel="nofollow noopener" target="_blank">こちら</a>からダイレクトに Standard Plan のアカウントを立てることができます。<br>
Standard Plan をお使いいただく場合は、Microsoft Azure の Subscription が必要となるので、もしお持ちでない方は<a href="https://azure.microsoft.com/ja-jp/free/" rel="nofollow noopener" target="_blank">こちら</a>から 1か月 ￥20.500 の無料試用アカウント等もぜひお試しください。</p>

<p>またこちらの API を使ったビジネスインパクトのあるような使い方があればコメントでも個人的に連絡でもいいので教えてください。<br>
個人的にとても興味があります！笑</p>

<h2>
<span id="参考-the-research-behind-computer-vision-api" class="fragment"></span><a href="#%E5%8F%82%E8%80%83-the-research-behind-computer-vision-api"><i class="fa fa-link"></i></a>参考 (The research behind Computer Vision API)</h2>

<p>最後に公式ページからそのまま引っ張ってきた Computer Vision API の裏に使われている研究成果群のご紹介です。（<a href="https://www.microsoft.com/cognitive-services/en-us/computer-vision-api/documentation" rel="nofollow noopener" target="_blank">参照公式URL</a>）<br>
一つ目の論文は CVPR 2015 に採択された論文ですね。CVPR はコンピュータビジョン系の国際会議でトップカンファレンスなので、かなり質とレベルが高い研究だといえます。一年前に発表されたばかりの最先端の研究がもう一般利用可能なサービスとして展開されているのは、かつてないスピード感で非常に面白いと思います。</p>

<ul>
<li><p>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, Lawrence Zitnick, and Geoffrey Zweig, <a href="http://research.microsoft.com/apps/pubs/default.aspx?id=241127" rel="nofollow noopener" target="_blank">From Captions to Visual Concepts and Back</a>, CVPR, June 2015 (<em>won 1st Prize at the COCO Image Captioning Challenge 2015</em>)</p></li>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. <a href="http://arxiv.org/abs/1512.03385" rel="nofollow noopener" target="_blank">Deep Residual Learning for Image Recognition</a>. <a href="http://arxiv.org/abs/1512.03385" rel="nofollow noopener" target="_blank">arXiv</a> (<em>won both <a href="http://image-net.org/challenges/LSVRC/2015/" rel="nofollow noopener" target="_blank">ImageNet</a> and <a href="http://mscoco.org/dataset/#detections-challenge2015" rel="nofollow noopener" target="_blank">MS COCO</a> competitions 2015</em>)</p></li>
<li><p>Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, <a href="http://research.microsoft.com/en-US/um/people/leizhang/paper/MS-Celeb-1M_EI_v1.pdf" rel="nofollow noopener" target="_blank">MS-Celeb-1M: Challenge of Recognizing One Million Celebrities in the Real World</a>, IS&amp;T International Symposium on Electronic Imaging, 2016</p></li>
<li><p>Xiao Zhang,Lei Zhang, Xin-Jing Wang, Heung-Yeung Shum, <a href="http://research.microsoft.com/en-us/um/people/leizhang/paper/TMM2011_Xiao.pdf" rel="nofollow noopener" target="_blank">Finding Celebrities in Billions of Web Images</a>, accepted by IEEE Transaction on Multimedia, 2012</p></li>
</ul>

<h2>
<span id="参考2-computer-vision-api-の-ms-内での位置づけ" class="fragment"></span><a href="#%E5%8F%82%E8%80%832-computer-vision-api-%E3%81%AE-ms-%E5%86%85%E3%81%A7%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%81%A5%E3%81%91"><i class="fa fa-link"></i></a>参考2 (Computer Vision API の MS 内での位置づけ)</h2>

<p><a href="https://www.microsoft.com/cognitive-services/en-us/computer-vision-api" rel="nofollow noopener" target="_blank">Computer Vision API</a> は、Microsoft の <a href="https://www.microsoft.com/cognitive-services/" rel="nofollow noopener" target="_blank">Cognitive Services</a> というクラウドベースの API サービス群の1つで<br>
下記5つのカテゴリのうちの Vision カテゴリに属する API です。</p>

<ul>
<li><strong>Vision</strong></li>
<li>Speech</li>
<li>Language</li>
<li>Knowledge</li>
<li>Search</li>
</ul>

<p>Vision カテゴリ内の API は下記のようになっています。</p>

<ul>
<li>
<strong>Computer Vision</strong> (今回はこれ)</li>
<li>Emotion (画像、動画に写った人の感情を推定)</li>
<li>Face (画像に写った人の年齢と性別を推定)</li>
<li>Video (動画の手ぶれ補正、顔のトラッキング、動作検出、ビデオから短時間のビデオサムネイルを作成)</li>
</ul>

<p>Computer Vision API では画像認識を含んだ以下の機能が提供されています。</p>

<ul>
<li>
<strong>画像認識</strong> (今回はこれ)</li>
<li>OCR (画像上の文字をテキストとして抽出)</li>
<li>画像上の注視点（ROI）を中心として指定したサイズの画像サムネイルを作成（スマホとPC向けに異なるサイズの画像を準備したい場合に使えそう）</li>
</ul>
<div class="hidden"><form class="js-task-list-update" action="/kosfuji/items/621cbedfad0eb68b2f5d" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="sZrp01/nka5XdmwfV0GX+zt2oyGh8ZaVcytX+ipf2PyX7fgKgNF1wgTOUo11fK8Snxd27wLpG6mGsuxEhetAsA==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1470357310" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
最近、Computer Vision API を触っていて使い方を把握できてきたと共に、予想以上の性能の高さに驚いたのでせっかくだからまとめてみることにしました。
Computer Vision API とはそもそも何なのか、なぜすごいのか、実際使ってみるとどれほどの実力を叩きだすのかをご紹介した後、コードもお見せしたいと思います。
この記事を通して、コードを数行書くだけで手軽にこんなにパワフルな画像認識機能が使えるんだということが伝わり、ぜひ試していただけたらと思います。

## そもそも、Computer Vision APIって何？
Microsoft が出している、REST で画像を投げると JSON 形式で画像を分析した結果を返してくれる API サービスのことです。
画像に写っている物体を認識しタグを出力するのはもちろん、画像上の物体の状態や状況を動詞や形容詞でタグとして出力してくれます。
（ex. water, sport, swimming, man）
さらに、それらをベースに画像に1文でもっともらしいキャプションを付けてくれます。
（ex. a man swimming in a pool of water）

![sample image](https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h33_10.png)
([公式ページ](https://www.microsoft.com/cognitive-services/en-us/computer-vision-api)参照)

裏では Microsoft Research の最先端の研究成果を盛り込んだ最新の画像認識エンジンが動いており、それが一般ユーザー向けに開放されています。

## Computer Vision API のバックで活躍する Microsoft Research の実力
最先端のといっても、そもそも Microsoft Research がどれほどこの分野に強いのかというのが気になります。
画像認識の分野では [ImageNet](http://image-net.org/) という、膨大な量の画像と付随するアノテーションをオープンなデータセットとして公開しているプロジェクトがあります。そこが主催の [Large Scale Visual Recognition Challenge](http://image-net.org/challenges/LSVRC/2016/index) というざっくり言うと大量の画像データの画像認識精度を競い合うコンペティションがあって、その一番最新の大会（ILSVRC 2015）で、MSRA (Microsoft Research Asia)は、Google 等の研究部門や海外トップ大学の研究チームがこぞって競い合う中、多数のタスクで1位を取ってるんですね。

![](https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h53_22.png)
（[ILSVRC2015 Results](http://image-net.org/challenges/LSVRC/2015/results)からの引用）

学生時代にこの分野に近めな研究室にいたので、MS は&quot;強い論文&quot;をよく出しているというイメージはぼんやり持っていましたが、今回調べることで具体的な状況を知れてよかったです。
この記事の最後に Computer Vision API に使われている関連論文も引用していますが、そちらを見てみると普通に最新の研究成果が使われていたりするので、だからすごいのかと納得です。

## どれぐらいの精度なの？
実際に自分の画像を投げて分析結果を見てみましょう。
今回は、Computer Vision API の画像認識機能の中で特に個人的に有用で面白いと思った、**キャプション生成機能**と**タグ生成機能**をご紹介します。
勉強のために、今回は画像を上げて分析結果を表示する簡易 Web サービスを作って Azure 上に展開してみました。
（コードを Git に上げて次回はそれについてもまとめたい）
以下、スクショと感想です。

まずは　**カフェで取ったコーヒーとパウンドケーキの写真**

![image](https://qiita-image-store.s3.amazonaws.com/0/104326/355c1374-a616-5b58-8eb9-9853dea6bda0.png)

ケーキ1ピースとカップに入ったコーヒー1つ、と見事に正解を当てられました。
タグを見てみると、テーブルやチョコレートや、食べかけを示す eaten まで入っていてびっくり。

次は　**旅先で取った何気ない写真**
![image](https://qiita-image-store.s3.amazonaws.com/0/104326/3d3c43d1-f309-bfbf-072b-b3511d64965c.png)

生成されたキャプションを見ると、教会の前に車が停められている、とまたまた見事に当てられました。
画像内での物体の位置関係までキャプション内で表現されているのには感動しました。

こうやって出力されるタグを活用して、アプリや Web サービスに画像検索機能をつけたり、グルーピングを行ったりというのは簡単に実装できそうです。また Web サービス内に大量の画像が存在する場合は分析をかけてタグ総数でランキング付けをしてみるのも新たな洞察が得られて面白そう。
## どう実装するの？
どのようなコード、どれぐらいのコード量でこれが実現できるのかをお見せしたいと思います。
Python2 系で書いてみました。Python3 系やC#, Java, JavaScript, Objective-C, PHP, Rubyの実装サンプルは[こちら](https://dev.projectoxford.ai/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa)の下部に載っています。

```python:ComputerVisionAPI使用部分
import httplib
import urllib

image_url = &#39;&#39;
headers = {
    # Request headers
    &#39;Content-Type&#39;: &#39;application/json&#39;,
    # &#39;Content-Type&#39;: &#39;application/octet-stream&#39;,
    &#39;Ocp-Apim-Subscription-Key&#39;: &#39;Your key&#39;,
}
params = urllib.urlencode({
    # Request parameters
    &#39;visualFeatures&#39;: &#39;Description&#39;,
    # &#39;visualFeatures&#39;: &#39;Categories,Tags, Description, Faces, ImageType, Color, Adult&#39;,
    # &#39;details&#39;: &#39;Celebrities&#39;
})
conn = httplib.HTTPSConnection(&#39;api.projectoxford.ai&#39;)
body = &quot;&quot;&quot;{&#39;url&#39;: &#39;%s&#39;}&quot;&quot;&quot; % (image_url)
conn.request(&quot;POST&quot;, &quot;/vision/v1.0/analyze?%s&quot; % params, body, headers)
response = conn.getresponse()
caption_data = response.read()
conn.close()
```

上記の例では画像URLを投げていますが、
直接画像データを投げることも可能です。
切り替えは、headersの中のContent-Typeの値を変更することで可能です。

```
&#39;Content-Type&#39;: &#39;application/json&#39;  　　　　 画像URLを与える場合
&#39;Content-Type&#39;: &#39;application/octet-stream&#39;  直接画像データを与える場合
```

なので、直接データを投げる場合はoctet-streamを指定して、
下記の部分を

```py
body = &quot;&quot;&quot;{&#39;url&#39;: &#39;%s&#39;}&quot;&quot;&quot; % (image_url)
conn.request(&quot;POST&quot;, &quot;/vision/v1.0/analyze?%s&quot; % params, body, headers)
```
を下記に変更でいけます。

```python
file_name = &#39;&#39;
img = open(file_name, &#39;rb&#39;).read()
conn.request(&quot;POST&quot;, &quot;/vision/v1.0/analyze?%s&quot; % params, img, headers)
```

paramsでは欲しい分析結果をパラメータで指定します。
visualFeaturesに指定できる値は現段階では以下の7つです。

* Categories
* Tags
* Description
* Faces
* ImageType
* Color
* Adult

（詳しくは[こちら](https://dev.projectoxford.ai/docs/services/56f91f2d778daf23d8ec6739/operations/56f91f2e778daf14a499e1fa)のRequest parameters 参照）
自分の写真を分析した上記の例は、visualFeatures に Description のみ（キャプション生成機能とタグ生成機能）を指定した場合の結果となっています。

visualFeatrues にすべて指定した場合は以下のようなJSONが返ってきます。
公式ページ上で使われている画像を使用した場合の結果を見てみましょう。
各タグごとの確信度やアダルト画像かのチェック、映り込んでいる人の顔の位置や性別と年齢等も得られるようになっています。

|分析画像|
|:---:|
|![image](https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-22_01h55_23.png)|


```json:返ってくるJSON
{
  &quot;categories&quot;: [
    {
      &quot;name&quot;: &quot;people_swimming&quot;,
      &quot;score&quot;: 0.98046875
    }
  ],
  &quot;adult&quot;: {
    &quot;isAdultContent&quot;: false,
    &quot;isRacyContent&quot;: false,
    &quot;adultScore&quot;: 0.14750830829143524,
    &quot;racyScore&quot;: 0.12601403892040253
  },
  &quot;tags&quot;: [
    {
      &quot;name&quot;: &quot;water&quot;,
      &quot;confidence&quot;: 0.99964427947998047
    },
    {
      &quot;name&quot;: &quot;sport&quot;,
      &quot;confidence&quot;: 0.95049923658370972
    },
    {
      &quot;name&quot;: &quot;swimming&quot;,
      &quot;confidence&quot;: 0.90628182888031006,
      &quot;hint&quot;: &quot;sport&quot;
    },
    {
      &quot;name&quot;: &quot;pool&quot;,
      &quot;confidence&quot;: 0.87875884771347046
    },
    {
      &quot;name&quot;: &quot;water sport&quot;,
      &quot;confidence&quot;: 0.631849467754364,
      &quot;hint&quot;: &quot;sport&quot;
    }
  ],
  &quot;description&quot;: {
    &quot;tags&quot;: [
      &quot;water&quot;,
      &quot;sport&quot;,
      &quot;swimming&quot;,
      &quot;pool&quot;,
      &quot;man&quot;,
      &quot;riding&quot;,
      &quot;blue&quot;,
      &quot;top&quot;,
      &quot;ocean&quot;,
      &quot;young&quot;,
      &quot;wave&quot;,
      &quot;bird&quot;,
      &quot;game&quot;,
      &quot;large&quot;,
      &quot;standing&quot;,
      &quot;body&quot;,
      &quot;frisbee&quot;,
      &quot;board&quot;,
      &quot;playing&quot;
    ],
    &quot;captions&quot;: [
      {
        &quot;text&quot;: &quot;a man swimming in a pool of water&quot;,
        &quot;confidence&quot;: 0.78501081244404836
      }
    ]
  },
  &quot;requestId&quot;: &quot;your request Id&quot;,
  &quot;metadata&quot;: {
    &quot;width&quot;: 1500,
    &quot;height&quot;: 1155,
    &quot;format&quot;: &quot;Jpeg&quot;
  },
  &quot;faces&quot;: [
    {
      &quot;age&quot;: 29,
      &quot;gender&quot;: &quot;Male&quot;,
      &quot;faceRectangle&quot;: {
        &quot;left&quot;: 748,
        &quot;top&quot;: 336,
        &quot;width&quot;: 304,
        &quot;height&quot;: 304
      }
    }
  ],
  &quot;color&quot;: {
    &quot;dominantColorForeground&quot;: &quot;Grey&quot;,
    &quot;dominantColorBackground&quot;: &quot;White&quot;,
    &quot;dominantColors&quot;: [
      &quot;White&quot;
    ],
    &quot;accentColor&quot;: &quot;19A4B2&quot;,
    &quot;isBWImg&quot;: false
  },
  &quot;imageType&quot;: {
    &quot;clipArtType&quot;: 0,
    &quot;lineDrawingType&quot;: 0
  }
}
```
基本的には返ってくるタグは汎用的なものになっていますが（ex. プードルではなく犬）有名人に関しては、個人特定まで出来るようになっています。
その機能を ON にしたい場合は、params 中の details で

    &#39;details&#39; : &#39;Celebrities&#39;

を指定すれば分析結果を JSON に含んでくれるようになります。
有名人判定は、約20万もの人を特定できるらしく、他の触った人から聞くとなんとデーモン小暮にも対応しているらしいです！（年齢は何歳と判定されるのだろう…？）

## どうやって始めるの？

Cognitive Services の利用の際には Subscription Key が必要になります。無料試用が可能なので[こちら](https://www.microsoft.com/cognitive-services/en-us/subscriptions?productId=/products/54d873dd5eefd00dc474a0f4
)からご登録ください。Microsoft Account でのログインが必要となります。
登録して得られた Subscription Key をサンプルコード上の Ocp-Apim-Subscription-Key で指定してください。
有料プランもあるのでお試しではなくしっかり使いたい方は次の**使用プラン**の項目をご覧ください。

自分でコードを書く前にまずはどんなものか試してみたいという場合は、[公式解説ページ](https://www.microsoft.com/cognitive-services/en-us/computer-vision-api)を見てもらうと準備されている画像を選ぶ or 自分の画像をアップロードして確認できるようになっているのでおすすめです。

![image](https://qiitaimagesfuji.blob.core.windows.net/images/2016-07-21_14h51_45.png)


使ってみてここもっとこうすればいいのに等色々と意見やアイデアが生まれた場合は、
[Give your apps a human side](https://cognitive.uservoice.com/forums/356325-apis/category/165762-computer-vision-api
) (Computer Vision API 開発チームが参考にするフィードバック共有ページ)をご利用ください。
こういうふうにユーザーサイドの意見を直接開発チームに届けられるのはいいですね。
各改善アイデアには投票ができるようになっていて、投票が集まれば上位に来るのでより注目を集めやすくなります。一緒に使い込んで良くしていきましょう！


## 使用プラン (2016/8/5 追記)
本日より、以前のお試しプラン Free に加え **Standard plan** が追加されました！
まだ Cognitive Services 全体が Preview 段階なので SLA やサポートはありませんが、
ビジネスユースでもご検討いただけるのではないかと思います。

| Plan  | Description | Price |
|:-------------|:------------|:------------|
|Free	    |5,000 transactions per month|	Free|
|**Standard**   |10 transactions per second| $ 1.50 per 1000 transactions|

[こちら](https://portal.azure.com/#create/Microsoft.CognitiveServices/apitype/ComputerVision/pricingtier/S0)からダイレクトに Standard Plan のアカウントを立てることができます。
Standard Plan をお使いいただく場合は、Microsoft Azure の Subscription が必要となるので、もしお持ちでない方は[こちら](https://azure.microsoft.com/ja-jp/free/)から 1か月 ￥20.500 の無料試用アカウント等もぜひお試しください。

またこちらの API を使ったビジネスインパクトのあるような使い方があればコメントでも個人的に連絡でもいいので教えてください。
個人的にとても興味があります！笑

## 参考 (The research behind Computer Vision API)

最後に公式ページからそのまま引っ張ってきた Computer Vision API の裏に使われている研究成果群のご紹介です。（[参照公式URL](https://www.microsoft.com/cognitive-services/en-us/computer-vision-api/documentation)）
一つ目の論文は CVPR 2015 に採択された論文ですね。CVPR はコンピュータビジョン系の国際会議でトップカンファレンスなので、かなり質とレベルが高い研究だといえます。一年前に発表されたばかりの最先端の研究がもう一般利用可能なサービスとして展開されているのは、かつてないスピード感で非常に面白いと思います。

* Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, Lawrence Zitnick, and Geoffrey Zweig, [From Captions to Visual Concepts and Back](http://research.microsoft.com/apps/pubs/default.aspx?id=241127), CVPR, June 2015 (*won 1st Prize at the COCO Image Captioning Challenge 2015*)

* Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. [Deep Residual Learning for Image Recognition](http://arxiv.org/abs/1512.03385). [arXiv](http://arxiv.org/abs/1512.03385) (*won both [ImageNet](http://image-net.org/challenges/LSVRC/2015/) and [MS COCO](http://mscoco.org/dataset/#detections-challenge2015) competitions 2015*)

* Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, Jianfeng Gao, [MS-Celeb-1M: Challenge of Recognizing One Million Celebrities in the Real World](http://research.microsoft.com/en-US/um/people/leizhang/paper/MS-Celeb-1M_EI_v1.pdf), IS&amp;T International Symposium on Electronic Imaging, 2016

* Xiao Zhang,Lei Zhang, Xin-Jing Wang, Heung-Yeung Shum, [Finding Celebrities in Billions of Web Images](http://research.microsoft.com/en-us/um/people/leizhang/paper/TMM2011_Xiao.pdf), accepted by IEEE Transaction on Multimedia, 2012

## 参考2 (Computer Vision API の MS 内での位置づけ)

[Computer Vision API](https://www.microsoft.com/cognitive-services/en-us/computer-vision-api) は、Microsoft の [Cognitive Services](https://www.microsoft.com/cognitive-services/) というクラウドベースの API サービス群の1つで
下記5つのカテゴリのうちの Vision カテゴリに属する API です。

* **Vision**
* Speech
* Language
* Knowledge
* Search

Vision カテゴリ内の API は下記のようになっています。

* **Computer Vision** (今回はこれ)
* Emotion (画像、動画に写った人の感情を推定)
* Face (画像に写った人の年齢と性別を推定)
* Video (動画の手ぶれ補正、顔のトラッキング、動作検出、ビデオから短時間のビデオサムネイルを作成)

Computer Vision API では画像認識を含んだ以下の機能が提供されています。

* **画像認識** (今回はこれ)
* OCR (画像上の文字をテキストとして抽出)
* 画像上の注視点（ROI）を中心として指定したサイズの画像サムネイルを作成（スマホとPC向けに異なるサイズの画像を準備したい場合に使えそう）
 
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた by @kosfuji on @Qiita" data-url="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた" href="http://b.hatena.ne.jp/entry/http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/kosfuji"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/kosfuji">kosfuji</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">766</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;kosfuji&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-a4879abb-2eed-4197-b2e6-d7fae8efbeaf"></div>
    <div id="UserFollowButton-react-component-a4879abb-2eed-4197-b2e6-d7fae8efbeaf"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">Popular Posts</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kosfuji/items/621cbedfad0eb68b2f5d">最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kosfuji/items/575408ae17113d7b58e9">MS の Emotion API を使って Unity 上にインタラクティブいちゃいちゃギャルゲーを作ろう</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/kosfuji/items/90e54fc0df3186d930de">ついに来た！ Azure GPU インスタンス上に30分で構築する TensorFlow on GPU 実行環境構築手順</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82computer-vision-api%E3%81%A3%E3%81%A6%E4%BD%95\&quot;\u003eそもそも、Computer Vision APIって何？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#computer-vision-api-%E3%81%AE%E3%83%90%E3%83%83%E3%82%AF%E3%81%A7%E6%B4%BB%E8%BA%8D%E3%81%99%E3%82%8B-microsoft-research-%E3%81%AE%E5%AE%9F%E5%8A%9B\&quot;\u003eComputer Vision API のバックで活躍する Microsoft Research の実力\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%A9%E3%82%8C%E3%81%90%E3%82%89%E3%81%84%E3%81%AE%E7%B2%BE%E5%BA%A6%E3%81%AA%E3%81%AE\&quot;\u003eどれぐらいの精度なの？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%A9%E3%81%86%E5%AE%9F%E8%A3%85%E3%81%99%E3%82%8B%E3%81%AE\&quot;\u003eどう実装するの？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%A9%E3%81%86%E3%82%84%E3%81%A3%E3%81%A6%E5%A7%8B%E3%82%81%E3%82%8B%E3%81%AE\&quot;\u003eどうやって始めるの？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E4%BD%BF%E7%94%A8%E3%83%97%E3%83%A9%E3%83%B3-201685-%E8%BF%BD%E8%A8%98\&quot;\u003e使用プラン (2016/8/5 追記)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8F%82%E8%80%83-the-research-behind-computer-vision-api\&quot;\u003e参考 (The research behind Computer Vision API)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8F%82%E8%80%832-computer-vision-api-%E3%81%AE-ms-%E5%86%85%E3%81%A7%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%81%A5%E3%81%91\&quot;\u003e参考2 (Computer Vision API の MS 内での位置づけ)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-8a5cbb16-9251-41bc-b399-1978be42e7b9"></div>
    <div id="Toc-react-component-8a5cbb16-9251-41bc-b399-1978be42e7b9"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:575,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;621cbedfad0eb68b2f5d&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="YoheiEn"><a itemprop="url" href="/YoheiEn"><img alt="YoheiEn" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/116025/profile-images/1473713113" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="samacoba"><a itemprop="url" href="/samacoba"><img alt="samacoba" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/109458/profile-images/1481540220" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hat22"><a itemprop="url" href="/hat22"><img alt="hat22" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/131046/profile-images/1475732622" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="junichia"><a itemprop="url" href="/junichia"><img alt="junichia" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/134539/profile-images/1473719327" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="haray_isa"><a itemprop="url" href="/haray_isa"><img alt="haray_isa" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/81725/profile-images/1473702211" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="yonahaty"><a itemprop="url" href="/yonahaty"><img alt="yonahaty" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/121941/profile-images/1473715075" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="qqq"><a itemprop="url" href="/qqq"><img alt="qqq" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/66315/profile-images/1473697168" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="izariuo440"><a itemprop="url" href="/izariuo440"><img alt="izariuo440" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/3970/profile-images/1473683719" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kogakem"><a itemprop="url" href="/kogakem"><img alt="kogakem" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/23212/profile-images/1473683783" /></a></div></div><div class="ArticleFooter__user"><a href="/kosfuji/items/621cbedfad0eb68b2f5d/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/621cbedfad0eb68b2f5d/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/kosfuji/items/621cbedfad0eb68b2f5d.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/amedama/items/54a22dff00fcb3650362#_reference-f1eb9985c2505ce5a053"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/14365/profile-images/1473683310" />MicrosoftのComputer Vision API使ってみたらガラパゴス標識「止まれ」を認識した</a><time class="references_datetime js-dateTimeView" datetime="2016-07-28T09:07:01+00:00">8 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/HirofumiYashima/items/2a97ad1101f83823bb5a#_reference-c3d5659253af78ffa37e"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" />Microsoft Computer Vision API に いろいろな画像ファイルを食わせて、返されるキャプションの内容一致精度 を 試してみる。</a><time class="references_datetime js-dateTimeView" datetime="2016-08-02T08:33:28+00:00">8 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/kosfuji/items/575408ae17113d7b58e9#_reference-16e6982ce8527337db3f"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264" />MS の Emotion API を使って Unity 上にインタラクティブいちゃいちゃギャルゲーを作ろう</a><time class="references_datetime js-dateTimeView" datetime="2016-08-26T04:36:14+00:00">7 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/kaibadash@github/items/d204a05f620e98341ad2#_reference-7198faa7b30624d2e97e"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/21799/profile-images/1480818108" />MicrosoftのContent Moderator APIを試す</a><time class="references_datetime js-dateTimeView" datetime="2017-01-18T16:57:55+00:00">about 2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた by @kosfuji on @Qiita" data-url="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="最近の画像認識の実力～MS の最先端の研究成果 Computer Vision API を Python で使ってみた" href="http://b.hatena.ne.jp/entry/http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eこの精度やばいですね\u003cbr\u003e\nある程度の自動タグ付け、カテゴライズが誰でもリアルタイムでできてしまいます\u003cbr\u003e\nそして安い\u003cbr\u003e\n＞10 image transactions per second　$0.25 per 1000 transactions\u003cbr\u003e\n10万枚なら3時間くらいで250ドル\u003c/p\u003e\n\n\u003cp\u003eところでビデオもあるようですね\u003cbr\u003e\nこちらもかなり需要がありそうです（シーケンシャルデータなので、人力でやるには無理がありすぎる）\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-22T10:59:16+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:596765,&quot;is_team&quot;:false,&quot;item_id&quot;:409896,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;621cbedfad0eb68b2f5d&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;この精度やばいですね\nある程度の自動タグ付け、カテゴライズが誰でもリアルタイムでできてしまいます\nそして安い\n＞10 image transactions per second　$0.25 per 1000 transactions\n10万枚なら3時間くらいで250ドル\n\nところでビデオもあるようですね\nこちらもかなり需要がありそうです（シーケンシャルデータなので、人力でやるには無理がありすぎる）\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d#comment-80aad53d15b2147f9e4b&quot;,&quot;user&quot;:{&quot;contribution&quot;:1203,&quot;created_at&quot;:&quot;2013-05-01T15:07:11+09:00&quot;,&quot;id&quot;:20784,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/20784/profile-images/1473683117&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;netetahito&quot;},&quot;uuid&quot;:&quot;80aad53d15b2147f9e4b&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eコメントありがとうございます！\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eそして安い\u003cbr\u003e\n＞10 image transactions per second　$0.25 per 1000 transactions\u003cbr\u003e\n10万枚なら3時間くらいで250ドル\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eこちらは、Cognitive Services の中で同じ Vision Group の中にある \u003cstrong\u003eEmotion API\u003c/strong\u003e の \u003cstrong\u003eStandard Plan\u003c/strong\u003e の条件となります。\u003cbr\u003e\nそして 10万枚の場合は\u003cstrong\u003e25ドル\u003c/strong\u003eですね！\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;https://qiita-image-store.s3.amazonaws.com/0/104326/09fd7b4d-341b-83bb-c091-d87b29bba737.png\&quot; target=\&quot;_blank\&quot; rel=\&quot;nofollow noopener\&quot;\u003e\u003cimg src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/104326/09fd7b4d-341b-83bb-c091-d87b29bba737.png\&quot; alt=\&quot;image\&quot;\u003e\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\&quot;https://www.microsoft.com/cognitive-services/en-us/emotion-api\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003eEmotion API Pricing Options 参照URL\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eComputer Vision API では、提供されているプランが2016/7/31現在 \u003cstrong\u003eFree\u003c/strong\u003e のみなので、\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003ePlan\u003c/th\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003eDescription\u003c/th\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003ePrice\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003eFree\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003e5,000 transactions per month\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003eFree\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eとなります。\u003c/p\u003e\n\n\u003cp\u003e(2016/8/5 追記)\u003cbr\u003e\n Standard planが追加されました！\u003c/p\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003ePlan\u003c/th\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003eDescription\u003c/th\u003e\n\u003cth style=\&quot;text-align: left\&quot;\u003ePrice\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003eFree\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003e5,000 transactions per month\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003eFree\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003e\u003cstrong\u003eStandard\u003c/strong\u003e\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003e10 transactions per second\u003c/td\u003e\n\u003ctd style=\&quot;text-align: left\&quot;\u003e$ 1.50 per 1000 transactions\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e(追記終)\u003c/p\u003e\n\n\u003cp\u003eただ Cognitive Services 全体で、各 API がFree プランのみからスタートし、実用に足る Basic, Standard プランの追加展開がどんどんされてきているので、Computer Vision API も近いうちにそういったプランが追加されるのではないかと思います。\u003cbr\u003e\n現在のCognitive Services の値段一覧は\u003ca href=\&quot;https://www.microsoft.com/cognitive-services/en-us/pricing\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003eこちら\u003c/a\u003eにまとめて掲載されています。\u003cbr\u003e\nまた Computer Vision API のプラン等の更新があれば上の投稿に追記しますね！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-31T20:04:01+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:603277,&quot;is_team&quot;:false,&quot;item_id&quot;:409896,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;621cbedfad0eb68b2f5d&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:2,&quot;raw_body&quot;:&quot;コメントありがとうございます！\n\n\u003eそして安い\n\u003e＞10 image transactions per second　$0.25 per 1000 transactions\n\u003e10万枚なら3時間くらいで250ドル\n\nこちらは、Cognitive Services の中で同じ Vision Group の中にある **Emotion API** の **Standard Plan** の条件となります。\nそして 10万枚の場合は**25ドル**ですね！\n\n![image](https://qiita-image-store.s3.amazonaws.com/0/104326/09fd7b4d-341b-83bb-c091-d87b29bba737.png)\n[Emotion API Pricing Options 参照URL](https://www.microsoft.com/cognitive-services/en-us/emotion-api)\n\nComputer Vision API では、提供されているプランが2016/7/31現在 **Free** のみなので、\n\n| Plan | Description | Price |\n|:-----------|:------------|:------------|\n|Free\t|5,000 transactions per month|\tFree|\n\nとなります。\n\n(2016/8/5 追記)\n Standard planが追加されました！\n\n| Plan  | Description | Price |\n|:-------------|:------------|:------------|\n|Free\t    |5,000 transactions per month|\tFree|\n|**Standard**   |10 transactions per second| $ 1.50 per 1000 transactions|\n\n(追記終)\n\nただ Cognitive Services 全体で、各 API がFree プランのみからスタートし、実用に足る Basic, Standard プランの追加展開がどんどんされてきているので、Computer Vision API も近いうちにそういったプランが追加されるのではないかと思います。\n現在のCognitive Services の値段一覧は[こちら](https://www.microsoft.com/cognitive-services/en-us/pricing)にまとめて掲載されています。\nまた Computer Vision API のプラン等の更新があれば上の投稿に追記しますね！\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d#comment-224b8573d440883ccaa9&quot;,&quot;user&quot;:{&quot;contribution&quot;:766,&quot;created_at&quot;:&quot;2015-12-06T17:29:37+09:00&quot;,&quot;id&quot;:104326,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;kosfuji&quot;},&quot;uuid&quot;:&quot;224b8573d440883ccaa9&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:409896,&quot;uuid&quot;:&quot;621cbedfad0eb68b2f5d&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;kosfuji&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:104326,&quot;url_name&quot;:&quot;kosfuji&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264&quot;},{&quot;id&quot;:20784,&quot;url_name&quot;:&quot;netetahito&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/20784/profile-images/1473683117&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-603fec9b-c161-49e2-b7b0-3a2631e24d3f"></div>
    <div id="CommentListContainer-react-component-603fec9b-c161-49e2-b7b0-3a2631e24d3f"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="lALzMwQh1oJCxbaBvQ9qBNxfLUVvD/ZUyQGovJKffAKydeLq2xcy7hF9iBOfMlLteD74i8wXe2g8mBMCPSvkTg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kosfuji/items/621cbedfad0eb68b2f5d" /><input type="hidden" name="item_uuid" id="item_uuid" value="621cbedfad0eb68b2f5d" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/kosfuji/items/621cbedfad0eb68b2f5d", "id": 409896, "uuid": "621cbedfad0eb68b2f5d" }</script><script class="js-user" type="application/json">{&quot;id&quot;:104326,&quot;url_name&quot;:&quot;kosfuji&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/104326/profile-images/1473709264&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="yCT+jTFqlkH+o5RyF3WasQ0dfj9ltwxwrZfcN7V4tMzuU+9U7lxyLa0bquA1SKJYqXyr8cavgUxYDmeJGswsgA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/kosfuji/items/621cbedfad0eb68b2f5d" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
          ga('create', 'UA-81129751-1', { name: 'user' });
          ga('user.send', 'pageview');
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>