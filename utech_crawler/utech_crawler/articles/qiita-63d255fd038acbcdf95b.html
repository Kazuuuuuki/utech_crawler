<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>自然言語処理における畳み込みニューラルネットワークを用いたモデル - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="

はじめに

最近、畳み込みニューラルネットワーク(CNN)を用いた自然言語処理が注目を集めています。CNNはRNNと比べて並列化しやすく、またGPUを使うことで畳み込み演算を高速に行えるので、処理速度が圧倒的に速いという利点があります。

この記事は、自然言語処理における畳み込みニューラルネットワークを用いたモデルをまとめたものです。CNNを用いた自然言語処理の研究の進歩を俯瞰するのに役立てば幸いです。


文の分類(評判分析・トピック分類・質問タイプ分類)


C..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="Hironsan13" name="twitter:creator" /><meta content="自然言語処理における畳み込みニューラルネットワークを用いたモデル - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="## はじめに
最近、畳み込みニューラルネットワーク(CNN)を用いた自然言語処理が注目を集めています。CNNはRNNと比べて並列化しやすく、またGPUを使うことで畳み込み演算を高速に行えるので、処理速度が圧倒的に速いという利点があり..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-e89b2462e454a13b67eaa536fcb0b04a.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="1evDTyezTC3q5PPPrcAYkOxxws1pqRwXfM9V1YCc92tyTYTzt/lhb6WvDZCjYnTZ7FoL+vhu/s8e8x2O79C/og==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"Hironsan","type":"items","id":"63d255fd038acbcdf95b"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-d09f2a5f-0b8a-4b1b-bfcc-c182ad7f8a1b"></div>
    <div id="HeaderContainer-react-component-d09f2a5f-0b8a-4b1b-bfcc-c182ad7f8a1b"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86",        "name": "自然言語処理"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">自然言語処理における畳み込みニューラルネットワークを用いたモデル</h1><ul class="TagList"><li class="TagList__item" data-count="418"><a class="u-link-unstyled TagList__label" href="/tags/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86"><img alt="自然言語処理" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/8f3d1fa5956802137842d298176db395ebb6ed5e/medium.jpg?1439789898" /><span>自然言語処理</span></a></li><li class="TagList__item" data-count="1830"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="9868"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="1070"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">128</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="0 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>0</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:128,&quot;uuid&quot;:&quot;63d255fd038acbcdf95b&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="TokyoMickey"><a itemprop="url" href="/TokyoMickey"><img alt="TokyoMickey" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/141216/profile-images/1483272512" /></a></li><li class="js-hovercard" data-hovercard-target-name="hogefugabar"><a itemprop="url" href="/hogefugabar"><img alt="hogefugabar" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31899/profile-images/1473685791" /></a></li><li class="js-hovercard" data-hovercard-target-name="ryu0322"><a itemprop="url" href="/ryu0322"><img alt="ryu0322" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/123224/profile-images/1473715494" /></a></li><li class="js-hovercard" data-hovercard-target-name="tashirovii"><a itemprop="url" href="/tashirovii"><img alt="tashirovii" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/29825/profile-images/1473760346" /></a></li><li class="js-hovercard" data-hovercard-target-name="hiki_neet_p"><a itemprop="url" href="/hiki_neet_p"><img alt="hiki_neet_p" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/53668/profile-images/1473693131" /></a></li><li class="js-hovercard" data-hovercard-target-name="kubor"><a itemprop="url" href="/kubor"><img alt="kubor" class="thumb thumb--xs" src="https://pbs.twimg.com/profile_images/798903621358219264/y1bDatvh_normal.jpg" /></a></li><li class="js-hovercard" data-hovercard-target-name="tukkyr"><a itemprop="url" href="/tukkyr"><img alt="tukkyr" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/92230/profile-images/1473705563" /></a></li><li class="js-hovercard" data-hovercard-target-name="kiichi54321"><a itemprop="url" href="/kiichi54321"><img alt="kiichi54321" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/43633/profile-images/1473689593" /></a></li><li class="js-hovercard" data-hovercard-target-name="saicologic"><a itemprop="url" href="/saicologic"><img alt="saicologic" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/2432/profile-images/1473681518" /></a></li><li class="js-hovercard" data-hovercard-target-name="naotaka1128"><a itemprop="url" href="/naotaka1128"><img alt="naotaka1128" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/99986/profile-images/1484449518" /></a></li><li><a href="/Hironsan/items/63d255fd038acbcdf95b/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/Hironsan"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" alt="1473700709" /></a> <a class="u-link-unstyled" href="/Hironsan">Hironsan</a> </div><div class="ArticleAsideHeader__date"><meta content="2017-02-03T09:09:34+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2017-02-03">Edited at <time datetime="2017-02-03T12:15:23+09:00" itemprop="dateModified">2017-02-03</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/Hironsan/items/63d255fd038acbcdf95b/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">13</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/Hironsan/items/63d255fd038acbcdf95b/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(13)</span></a></li><li><a href="/Hironsan/items/63d255fd038acbcdf95b.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-63d255fd038acbcdf95b" itemprop="articleBody">
<h2>
<span id="はじめに" class="fragment"></span><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><i class="fa fa-link"></i></a>はじめに</h2>

<p>最近、畳み込みニューラルネットワーク(CNN)を用いた自然言語処理が注目を集めています。CNNはRNNと比べて並列化しやすく、またGPUを使うことで畳み込み演算を高速に行えるので、処理速度が圧倒的に速いという利点があります。</p>

<p>この記事は、自然言語処理における畳み込みニューラルネットワークを用いたモデルをまとめたものです。CNNを用いた自然言語処理の研究の進歩を俯瞰するのに役立てば幸いです。</p>

<h2>
<span id="文の分類評判分析トピック分類質問タイプ分類" class="fragment"></span><a href="#%E6%96%87%E3%81%AE%E5%88%86%E9%A1%9E%E8%A9%95%E5%88%A4%E5%88%86%E6%9E%90%E3%83%88%E3%83%94%E3%83%83%E3%82%AF%E5%88%86%E9%A1%9E%E8%B3%AA%E5%95%8F%E3%82%BF%E3%82%A4%E3%83%97%E5%88%86%E9%A1%9E"><i class="fa fa-link"></i></a>文の分類(評判分析・トピック分類・質問タイプ分類)</h2>

<h3>
<span id="convolutional-neural-networks-for-sentence-classification201408" class="fragment"></span><a href="#convolutional-neural-networks-for-sentence-classification201408"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1408.5882" rel="nofollow noopener" target="_blank">Convolutional Neural Networks for Sentence Classification</a>(2014/08)</h3>

<p>評判分析や質問タイプの分類などの文分類を行うCNNを提案している論文。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/b56a94b5-d54c-207e-98bb-474fe3d22e8b.png" target="_blank" rel="nofollow noopener"><img width="893" alt="スクリーンショット 2017-02-03 5.39.48.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/b56a94b5-d54c-207e-98bb-474fe3d22e8b.png"></a></p>

<p>具体的には文を単語ベクトルの列として表し、それに対してCNNを用いて特徴抽出・分類を行っている。論文では事前学習済みの単語ベクトル(Google Newsをword2vecで学習したもの)を使うことで性能が向上したことが報告されている。2つのチャンネルそれぞれで単語ベクトルを表し、一方は学習中に更新、もう一方は更新しないようにすることで性能が上がるのが面白いところ。評判分析や質問タイプ分類を含む7つの文書分類タスクで評価したところ、7つ中4つのタスクで今までで最高の結果になった。</p>

<p>著者によるTheano実装とGoogle BrainのDenny BritzによるTensorFlow実装です:<br>
<a href="https://github.com/yoonkim/CNN_sentence" rel="nofollow noopener" target="_blank">https://github.com/yoonkim/CNN_sentence</a><br>
<a href="https://github.com/dennybritz/cnn-text-classification-tf" rel="nofollow noopener" target="_blank">https://github.com/dennybritz/cnn-text-classification-tf</a></p>

<p><a href="http://qiita.com/ichiroex">ichiroex</a>さんによる日本語での簡単な解説と実装<br>
<a href="http://qiita.com/ichiroex/items/f225f6d8eceb6796cc7e" id="reference-511fb7b1be3357a2e5d7">【Chainer】畳み込みニューラルネットワークによる文書分類</a></p>

<h3>
<span id="deep-convolutional-neural-networks-for-sentiment-analysis-of-short-texts201408" class="fragment"></span><a href="#deep-convolutional-neural-networks-for-sentiment-analysis-of-short-texts201408"><i class="fa fa-link"></i></a><a href="http://www.aclweb.org/anthology/C14-1008" rel="nofollow noopener" target="_blank">Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts</a>(2014/08)</h3>

<p>映画レビューやTwitterに対する評判分析を行うCNN(CharSCNN)を提案した論文。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/823f7936-0600-0433-364b-4c316b6a86c9.png" target="_blank" rel="nofollow noopener"><img width="735" alt="スクリーンショット 2017-02-03 5.42.02.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/823f7936-0600-0433-364b-4c316b6a86c9.png"></a></p>

<p>Twitter等の短いテキストに対する評判分析は文脈情報が限られているので難しいという問題がある。その問題に対処するために、評判分析をする際に通常使われる単語レベルのベクトル表現だけでなく、文字レベルのベクトル表現を構築し、それらを用いて文のベクトル表現を得ることで性能を向上させるということを行った。映画レビュー(SSTb)とTwitter(STS)に対するデータセットを用いて実験した結果、今までの手法より良い結果となった。</p>

<p><a href="https://github.com/hogefugabar" rel="nofollow noopener" target="_blank">hogefugabar</a>さんによるTheano実装です:<br>
<a href="https://github.com/hogefugabar/CharSCNN-theano" rel="nofollow noopener" target="_blank">https://github.com/hogefugabar/CharSCNN-theano</a></p>

<p>解説記事も書いておられます:<br>
<a href="http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6" id="reference-50f985ed3355ccdf1e81">深層学習でツイートの感情分析</a></p>

<h3>
<span id="tagspace-semantic-embeddings-from-hashtags201410" class="fragment"></span><a href="#tagspace-semantic-embeddings-from-hashtags201410"><i class="fa fa-link"></i></a><a href="http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf" rel="nofollow noopener" target="_blank">#TAGSPACE: Semantic Embeddings from Hashtags</a>(2014/10)</h3>

<p>SNSに使われるハッシュタグを教師として短いテキストの表現を学習するCNNを提案した論文。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/8049b0e7-ea89-f49b-5af1-24b881b72b54.png" target="_blank" rel="nofollow noopener"><img width="863" alt="スクリーンショット 2017-02-03 5.47.14.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/8049b0e7-ea89-f49b-5af1-24b881b72b54.png"></a></p>

<p>具体的にはCNNを用いて、入力テキストと対応するハッシュタグのペアに対してスコアを出力し、ハッシュタグのランク付けを行う過程でテキストの表現を学習する。ハッシュタグの予測と文書推薦タスクで評価を行った結果、ベースラインの手法よりも良い結果となった。</p>

<h3>
<span id="effective-use-of-word-order-for-text-categorization-with-convolutional-neural-networks201412" class="fragment"></span><a href="#effective-use-of-word-order-for-text-categorization-with-convolutional-neural-networks201412"><i class="fa fa-link"></i></a><a href="http://www.anthology.aclweb.org/N/N15/N15-1011.pdf" rel="nofollow noopener" target="_blank">Effective Use of Word Order for Text Categorization with Convolutional Neural Networks</a>(2014/12)</h3>

<p>語順を考慮したテキスト分類を行うためのCNNを提案した論文。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/77079/46eeb96d-48a1-723e-18e8-56a396b3e620.png" target="_blank" rel="nofollow noopener"><img width="656" alt="スクリーンショット 2017-02-03 5.59.33.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/46eeb96d-48a1-723e-18e8-56a396b3e620.png"></a></p>

<p>文書分類には様々なタスクがあるが、評判分析のようなタスクでは語順を考慮しないと高い性能が出ない。その問題に対処するために語順を考慮した文書分類を行うことのできるCNNを提案している。具体的には、たいていのCNNの手法では入力としてword embeddingを入力するが、この研究では高次元のone-hotベクトルをそのまま入力して、小さなテキスト領域のembeddingを学習する。評判分析(IMDB含む)とトピック分類に関する3つのデータセットでSOTAな手法と比較した結果、提案手法の有効性を示せた。</p>

<p>論文著者による実装です:<br>
<a href="http://riejohnson.com/cnn_download.html" rel="nofollow noopener" target="_blank">http://riejohnson.com/cnn_download.html</a></p>

<h3>
<span id="semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding201504" class="fragment"></span><a href="#semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding201504"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1504.01255" rel="nofollow noopener" target="_blank">Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding</a>(2015/04)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/f7911571-b91f-5572-4d44-85227869c640.png" target="_blank" rel="nofollow noopener"><img width="820" alt="スクリーンショット 2017-02-03 5.48.32.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/f7911571-b91f-5572-4d44-85227869c640.png"></a></p>

<p>テキスト分類のためにCNNを使った半教師あり学習のフレームワークを提案した話。従来モデルでは事前学習済みのword embeddingを畳み込み層の入力に使っていたが、本研究では小さいテキストの領域から教師なしでembeddingを学習し、教師ありCNNにおける畳み込み層の入力の一部として使う。評判分析(IMDB, Elec)とトピック分類(RCV1)で実験したところ、先行研究より高い性能を示した。</p>

<p>論文著者による実装です:<br>
<a href="http://riejohnson.com/cnn_download.html" rel="nofollow noopener" target="_blank">http://riejohnson.com/cnn_download.html</a></p>

<h3>
<span id="character-level-convolutional-networks-for-text-classification201509" class="fragment"></span><a href="#character-level-convolutional-networks-for-text-classification201509"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1509.01626" rel="nofollow noopener" target="_blank">Character-level Convolutional Networks for Text Classification</a>(2015/09)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/848c6556-ae42-bf67-28c2-c3e8445f2c73.png" target="_blank" rel="nofollow noopener"><img width="794" alt="スクリーンショット 2017-02-03 5.43.09.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/848c6556-ae42-bf67-28c2-c3e8445f2c73.png"></a></p>

<p>文字レベルの畳み込みニューラルネットワークをテキスト分類に使った話。シソーラスを使ってテキスト中の単語を同義語で置換することでデータを増やしている。比較は、伝統的な手法としてbow、bag-of-ngram、bag-of-means、Deep Learning手法として、単語ベースのCNN、LSTMを対象に行っている。8つのデータセットを作成してベースの手法と比較した結果、いくつかのデータセットでは有効性を示せた。</p>

<p>論文著者によるLua実装です:<br>
<a href="https://github.com/zhangxiangxiao/Crepe" rel="nofollow noopener" target="_blank">https://github.com/zhangxiangxiao/Crepe</a></p>

<h3>
<span id="a-sensitivity-analysis-of-and-practitioners-guide-to-convolutional-neural-networks-for-sentence-classification201510" class="fragment"></span><a href="#a-sensitivity-analysis-of-and-practitioners-guide-to-convolutional-neural-networks-for-sentence-classification201510"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1510.03820" rel="nofollow noopener" target="_blank">A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification</a>(2015/10)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/94361fc0-a88a-98a7-2cd7-0d6925edbf22.png" target="_blank" rel="nofollow noopener"><img width="792" alt="スクリーンショット 2017-02-03 5.46.08.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/94361fc0-a88a-98a7-2cd7-0d6925edbf22.png"></a></p>

<p>CNNのモデルは文分類でいい結果を残しているけど、熟練者がアーキテクチャ決めたりハイパーパラメータを設定する必要がある。これらの変更がどのような結果を及ぼすのかよくわからないので、一層のCNNを使って検証した話。最後に、CNNで文分類するときにモデルのアーキテクチャやハイパーパラメータをどう設定すべきか実践的なアドバイスをしている。</p>

<h2>
<span id="系列ラベリング品詞タグ付け固有表現認識チャンキング" class="fragment"></span><a href="#%E7%B3%BB%E5%88%97%E3%83%A9%E3%83%99%E3%83%AA%E3%83%B3%E3%82%B0%E5%93%81%E8%A9%9E%E3%82%BF%E3%82%B0%E4%BB%98%E3%81%91%E5%9B%BA%E6%9C%89%E8%A1%A8%E7%8F%BE%E8%AA%8D%E8%AD%98%E3%83%81%E3%83%A3%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0"><i class="fa fa-link"></i></a>系列ラベリング(品詞タグ付け・固有表現認識・チャンキング)</h2>

<h3>
<span id="natural-language-processing-almost-from-scratch201103" class="fragment"></span><a href="#natural-language-processing-almost-from-scratch201103"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1103.0398" rel="nofollow noopener" target="_blank">Natural Language Processing (almost) from Scratch</a>(2011/03)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/7c0bca50-c300-35d3-21d8-ed3bbe8e6bcf.png" target="_blank" rel="nofollow noopener"><img width="654" alt="スクリーンショット 2017-02-03 5.52.26.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/7c0bca50-c300-35d3-21d8-ed3bbe8e6bcf.png"></a></p>

<p>品詞タグ付け、チャンキング、固有表現抽出、意味役割付与を学習できるニューラルネットワークを提案した話。単純に学習させるだけではベンチマークより性能が下回ったが、ラベルなしデータを用いて言語モデルの学習を事前に行うことで、質の良い単語ベクトルが性能向上に寄与することを示した。さらに各タスクを解くためのモデル間でパラメタを共有してマルチタスク学習を行うことで性能がより向上することも示した。</p>

<p>日本語によるまとめスライドです:<br>
<a href="http://www.slideshare.net/alembert2000/deep-learning-6" rel="nofollow noopener" target="_blank">Natural Language Processing (Almost) from Scratch（第 6 回 Deep Learning 勉強会資料; 榊）</a></p>

<h3>
<span id="learning-character-level-representations-for-part-of-speech-tagging201407" class="fragment"></span><a href="#learning-character-level-representations-for-part-of-speech-tagging201407"><i class="fa fa-link"></i></a><a href="http://jmlr.org/proceedings/papers/v32/santos14.pdf" rel="nofollow noopener" target="_blank">Learning Character-level Representations for Part-of-Speech Tagging</a>(2014/07)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/342745ed-a268-8be2-389f-4f26cdb3270a.png" target="_blank" rel="nofollow noopener"><img width="524" alt="スクリーンショット 2017-02-03 5.51.27.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/342745ed-a268-8be2-389f-4f26cdb3270a.png"></a></p>

<p>品詞タグ付けをCNN(CharWNN)を使って行う話。具体的には、単語レベルと文字レベルのembeddingsを統合して単語のベクトル表現を構築し、構築したベクトルを入力することで品詞のスコアを出力するCNNを構築した。英語とポルトガル語に対するデータセット(WSJとMac-Morpho)を用いて実験した結果、SOTAな結果となった。</p>

<h2>
<span id="言語モデル" class="fragment"></span><a href="#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB"><i class="fa fa-link"></i></a>言語モデル</h2>

<h3>
<span id="language-modeling-with-gated-convolutional-networks201612" class="fragment"></span><a href="#language-modeling-with-gated-convolutional-networks201612"><i class="fa fa-link"></i></a><a href="https://arxiv.org/abs/1612.08083" rel="nofollow noopener" target="_blank">Language Modeling with Gated Convolutional Networks</a>(2016/12)</h3>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/77079/54af199b-53da-3d11-c75c-87be2611cf8d.png" target="_blank" rel="nofollow noopener"><img width="459" alt="スクリーンショット 2017-02-03 9.05.21.png" src="https://qiita-image-store.s3.amazonaws.com/0/77079/54af199b-53da-3d11-c75c-87be2611cf8d.png"></a></p>

<p>言語モデルのタスクで、CNNでLSTM同等以上の精度を出したという話。畳み込んだ結果をGRUに近い機構で処理し、過去の情報が消失しないようにしている。Google Billion Wordのデータセットでは、LSTMと同等の精度を出す一方計算効率が20倍程度改善された。</p>

<p>TensorFlowによる実装です:<br>
<a href="https://github.com/anantzoid/Language-Modeling-GatedCNN" rel="nofollow noopener" target="_blank">Language-Modeling-GatedCNN</a></p>

<h1>
<span id="おわりに" class="fragment"></span><a href="#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB"><i class="fa fa-link"></i></a>おわりに</h1>

<p>機械学習・自然言語処理・コンピュータビジョンの最新の論文情報のまとめを以下のTwitterアカウントで配信しています。この記事を読んでくれた方にとって興味深い内容を配信していますので、フォローお待ちしています。<br>
<a href="https://twitter.com/arxivtimes" rel="nofollow noopener" target="_blank">arXivTimes</a><br>
<a href="https://twitter.com/arxivtimes" rel="nofollow noopener" target="_blank"><img width="550" src="https://qiita-image-store.s3.amazonaws.com/0/77079/a8c638c8-16b0-ae92-3844-08ee92e24267.png"></a></p>

<h1>
<span id="参考" class="fragment"></span><a href="#%E5%8F%82%E8%80%83"><i class="fa fa-link"></i></a>参考</h1>

<ul>
<li><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp" rel="nofollow noopener" target="_blank">UNDERSTANDING CONVOLUTIONAL NEURAL NETWORKS FOR NLP</a></li>
</ul>


<div class="hidden"><form class="js-task-list-update" action="/Hironsan/items/63d255fd038acbcdf95b" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="ghUB/eypTgufrc+1YJi2pXEGs+J1q76gbIYqUEx77d0ls0ZBfONjSdDmMepuOtrscS161eRsXHgOumILIzelFA==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1486091723" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
## はじめに
最近、畳み込みニューラルネットワーク(CNN)を用いた自然言語処理が注目を集めています。CNNはRNNと比べて並列化しやすく、またGPUを使うことで畳み込み演算を高速に行えるので、処理速度が圧倒的に速いという利点があります。

この記事は、自然言語処理における畳み込みニューラルネットワークを用いたモデルをまとめたものです。CNNを用いた自然言語処理の研究の進歩を俯瞰するのに役立てば幸いです。



## 文の分類(評判分析・トピック分類・質問タイプ分類)

### [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)(2014/08)
評判分析や質問タイプの分類などの文分類を行うCNNを提案している論文。
&lt;img width=&quot;893&quot; alt=&quot;スクリーンショット 2017-02-03 5.39.48.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/b56a94b5-d54c-207e-98bb-474fe3d22e8b.png&quot;&gt;

具体的には文を単語ベクトルの列として表し、それに対してCNNを用いて特徴抽出・分類を行っている。論文では事前学習済みの単語ベクトル(Google Newsをword2vecで学習したもの)を使うことで性能が向上したことが報告されている。2つのチャンネルそれぞれで単語ベクトルを表し、一方は学習中に更新、もう一方は更新しないようにすることで性能が上がるのが面白いところ。評判分析や質問タイプ分類を含む7つの文書分類タスクで評価したところ、7つ中4つのタスクで今までで最高の結果になった。

著者によるTheano実装とGoogle BrainのDenny BritzによるTensorFlow実装です:
[https://github.com/yoonkim/CNN_sentence](https://github.com/yoonkim/CNN_sentence)
[https://github.com/dennybritz/cnn-text-classification-tf](https://github.com/dennybritz/cnn-text-classification-tf)

[ichiroex](http://qiita.com/ichiroex)さんによる日本語での簡単な解説と実装
[【Chainer】畳み込みニューラルネットワークによる文書分類](http://qiita.com/ichiroex/items/f225f6d8eceb6796cc7e)


### [Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts](http://www.aclweb.org/anthology/C14-1008)(2014/08)
映画レビューやTwitterに対する評判分析を行うCNN(CharSCNN)を提案した論文。
&lt;img width=&quot;735&quot; alt=&quot;スクリーンショット 2017-02-03 5.42.02.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/823f7936-0600-0433-364b-4c316b6a86c9.png&quot;&gt;

Twitter等の短いテキストに対する評判分析は文脈情報が限られているので難しいという問題がある。その問題に対処するために、評判分析をする際に通常使われる単語レベルのベクトル表現だけでなく、文字レベルのベクトル表現を構築し、それらを用いて文のベクトル表現を得ることで性能を向上させるということを行った。映画レビュー(SSTb)とTwitter(STS)に対するデータセットを用いて実験した結果、今までの手法より良い結果となった。

[hogefugabar](https://github.com/hogefugabar)さんによるTheano実装です:
[https://github.com/hogefugabar/CharSCNN-theano](https://github.com/hogefugabar/CharSCNN-theano)

解説記事も書いておられます:
[深層学習でツイートの感情分析](http://qiita.com/hogefugabar/items/93fcb2bc27d7b268cbe6)


### [#TAGSPACE: Semantic Embeddings from Hashtags](http://emnlp2014.org/papers/pdf/EMNLP2014194.pdf)(2014/10)
SNSに使われるハッシュタグを教師として短いテキストの表現を学習するCNNを提案した論文。
&lt;img width=&quot;863&quot; alt=&quot;スクリーンショット 2017-02-03 5.47.14.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/8049b0e7-ea89-f49b-5af1-24b881b72b54.png&quot;&gt;

具体的にはCNNを用いて、入力テキストと対応するハッシュタグのペアに対してスコアを出力し、ハッシュタグのランク付けを行う過程でテキストの表現を学習する。ハッシュタグの予測と文書推薦タスクで評価を行った結果、ベースラインの手法よりも良い結果となった。



### [Effective Use of Word Order for Text Categorization with Convolutional Neural Networks](http://www.anthology.aclweb.org/N/N15/N15-1011.pdf)(2014/12)
語順を考慮したテキスト分類を行うためのCNNを提案した論文。
&lt;img width=&quot;656&quot; alt=&quot;スクリーンショット 2017-02-03 5.59.33.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/46eeb96d-48a1-723e-18e8-56a396b3e620.png&quot;&gt;

文書分類には様々なタスクがあるが、評判分析のようなタスクでは語順を考慮しないと高い性能が出ない。その問題に対処するために語順を考慮した文書分類を行うことのできるCNNを提案している。具体的には、たいていのCNNの手法では入力としてword embeddingを入力するが、この研究では高次元のone-hotベクトルをそのまま入力して、小さなテキスト領域のembeddingを学習する。評判分析(IMDB含む)とトピック分類に関する3つのデータセットでSOTAな手法と比較した結果、提案手法の有効性を示せた。

論文著者による実装です:
[http://riejohnson.com/cnn_download.html](http://riejohnson.com/cnn_download.html)



### [Semi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding](https://arxiv.org/abs/1504.01255)(2015/04)
&lt;img width=&quot;820&quot; alt=&quot;スクリーンショット 2017-02-03 5.48.32.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/f7911571-b91f-5572-4d44-85227869c640.png&quot;&gt;

テキスト分類のためにCNNを使った半教師あり学習のフレームワークを提案した話。従来モデルでは事前学習済みのword embeddingを畳み込み層の入力に使っていたが、本研究では小さいテキストの領域から教師なしでembeddingを学習し、教師ありCNNにおける畳み込み層の入力の一部として使う。評判分析(IMDB, Elec)とトピック分類(RCV1)で実験したところ、先行研究より高い性能を示した。

論文著者による実装です:
[http://riejohnson.com/cnn_download.html](http://riejohnson.com/cnn_download.html)

### [Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626)(2015/09)
&lt;img width=&quot;794&quot; alt=&quot;スクリーンショット 2017-02-03 5.43.09.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/848c6556-ae42-bf67-28c2-c3e8445f2c73.png&quot;&gt;

文字レベルの畳み込みニューラルネットワークをテキスト分類に使った話。シソーラスを使ってテキスト中の単語を同義語で置換することでデータを増やしている。比較は、伝統的な手法としてbow、bag-of-ngram、bag-of-means、Deep Learning手法として、単語ベースのCNN、LSTMを対象に行っている。8つのデータセットを作成してベースの手法と比較した結果、いくつかのデータセットでは有効性を示せた。

論文著者によるLua実装です:
[https://github.com/zhangxiangxiao/Crepe](https://github.com/zhangxiangxiao/Crepe)


### [A Sensitivity Analysis of (and Practitioners&#39; Guide to) Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1510.03820)(2015/10)
&lt;img width=&quot;792&quot; alt=&quot;スクリーンショット 2017-02-03 5.46.08.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/94361fc0-a88a-98a7-2cd7-0d6925edbf22.png&quot;&gt;

CNNのモデルは文分類でいい結果を残しているけど、熟練者がアーキテクチャ決めたりハイパーパラメータを設定する必要がある。これらの変更がどのような結果を及ぼすのかよくわからないので、一層のCNNを使って検証した話。最後に、CNNで文分類するときにモデルのアーキテクチャやハイパーパラメータをどう設定すべきか実践的なアドバイスをしている。





## 系列ラベリング(品詞タグ付け・固有表現認識・チャンキング)
### [Natural Language Processing (almost) from Scratch](https://arxiv.org/abs/1103.0398)(2011/03)
&lt;img width=&quot;654&quot; alt=&quot;スクリーンショット 2017-02-03 5.52.26.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/7c0bca50-c300-35d3-21d8-ed3bbe8e6bcf.png&quot;&gt;

品詞タグ付け、チャンキング、固有表現抽出、意味役割付与を学習できるニューラルネットワークを提案した話。単純に学習させるだけではベンチマークより性能が下回ったが、ラベルなしデータを用いて言語モデルの学習を事前に行うことで、質の良い単語ベクトルが性能向上に寄与することを示した。さらに各タスクを解くためのモデル間でパラメタを共有してマルチタスク学習を行うことで性能がより向上することも示した。

日本語によるまとめスライドです:
[Natural Language Processing (Almost) from Scratch（第 6 回 Deep Learning 勉強会資料; 榊）](http://www.slideshare.net/alembert2000/deep-learning-6)



### [Learning Character-level Representations for Part-of-Speech Tagging](http://jmlr.org/proceedings/papers/v32/santos14.pdf)(2014/07)
&lt;img width=&quot;524&quot; alt=&quot;スクリーンショット 2017-02-03 5.51.27.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/342745ed-a268-8be2-389f-4f26cdb3270a.png&quot;&gt;

品詞タグ付けをCNN(CharWNN)を使って行う話。具体的には、単語レベルと文字レベルのembeddingsを統合して単語のベクトル表現を構築し、構築したベクトルを入力することで品詞のスコアを出力するCNNを構築した。英語とポルトガル語に対するデータセット(WSJとMac-Morpho)を用いて実験した結果、SOTAな結果となった。


## 言語モデル
### [Language Modeling with Gated Convolutional Networks](https://arxiv.org/abs/1612.08083)(2016/12)
&lt;img width=&quot;459&quot; alt=&quot;スクリーンショット 2017-02-03 9.05.21.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/54af199b-53da-3d11-c75c-87be2611cf8d.png&quot;&gt;

言語モデルのタスクで、CNNでLSTM同等以上の精度を出したという話。畳み込んだ結果をGRUに近い機構で処理し、過去の情報が消失しないようにしている。Google Billion Wordのデータセットでは、LSTMと同等の精度を出す一方計算効率が20倍程度改善された。

TensorFlowによる実装です:
[Language-Modeling-GatedCNN](https://github.com/anantzoid/Language-Modeling-GatedCNN)


# おわりに
機械学習・自然言語処理・コンピュータビジョンの最新の論文情報のまとめを以下のTwitterアカウントで配信しています。この記事を読んでくれた方にとって興味深い内容を配信していますので、フォローお待ちしています。
[arXivTimes](https://twitter.com/arxivtimes)
&lt;a href=&quot;https://twitter.com/arxivtimes&quot;&gt;&lt;img width=&quot;550&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/a8c638c8-16b0-ae92-3844-08ee92e24267.png&quot; /&gt;&lt;/a&gt;


# 参考
* [UNDERSTANDING CONVOLUTIONAL NEURAL NETWORKS FOR NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp)

&lt;!--
* [CNNを利用した自然言語処理技術まとめ（2017年1月）](http://ksksksks2.hatenadiary.jp/entry/20170122/1485082800)
* [CNNを利用したセンチメント分析](http://catindog.hatenablog.com/entry/2016/08/22/102406)
--&gt;
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="自然言語処理における畳み込みニューラルネットワークを用いたモデル by @Hironsan13 on @Qiita" data-url="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="自然言語処理における畳み込みニューラルネットワークを用いたモデル" href="http://b.hatena.ne.jp/entry/http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/Hironsan"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/Hironsan">Hironsan</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">4804</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;Hironsan&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-0f49d31f-6a47-446c-a1d2-9e848f27715b"></div>
    <div id="UserFollowButton-react-component-0f49d31f-6a47-446c-a1d2-9e848f27715b"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/6425787ccbee75dfae36">機械学習を使って作る対話システム</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/8ad9b11bcc0c618ec5e2">DeepLearningで上司を認識して画面を隠す</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/20951116a2650f45fea5">学年ビリのアホが1年半でTOEICスコアを300点から840点に上げた英語勉強法の話</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/326b66711eb4196aa9d4">【チュートリアル】機械学習を使って30分で固有表現抽出器を作る</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Hironsan/items/ca0b176fd859490dde08">まだ機械学習の論文を追うのに消耗してるの？それBotで解決したよ</a></li></ul></section><section class="itemsShowAuthorInfo_organization"><h5 class="itemsShowAuthorInfo_organizationTitle">ORGANIZATION</h5><span itemprop="memberOf" itemscope="" itemtype="http://schema.org/Organization"><a itemprop="url" href="/organizations/tis"><img alt="TIS株式会社" class="itemsShowAuthorInfo_organizationLogo" itemprop="image" src="https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/5710e4c30854dd4ab3658e7f585930ab0d81a12c/original.jpg?1484790468" /></a></span></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\&quot;\u003eはじめに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%96%87%E3%81%AE%E5%88%86%E9%A1%9E%E8%A9%95%E5%88%A4%E5%88%86%E6%9E%90%E3%83%88%E3%83%94%E3%83%83%E3%82%AF%E5%88%86%E9%A1%9E%E8%B3%AA%E5%95%8F%E3%82%BF%E3%82%A4%E3%83%97%E5%88%86%E9%A1%9E\&quot;\u003e文の分類(評判分析・トピック分類・質問タイプ分類)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#convolutional-neural-networks-for-sentence-classification201408\&quot;\u003eConvolutional Neural Networks for Sentence Classification(2014/08)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-convolutional-neural-networks-for-sentiment-analysis-of-short-texts201408\&quot;\u003eDeep Convolutional Neural Networks for Sentiment Analysis of Short Texts(2014/08)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#tagspace-semantic-embeddings-from-hashtags201410\&quot;\u003e#TAGSPACE: Semantic Embeddings from Hashtags(2014/10)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#effective-use-of-word-order-for-text-categorization-with-convolutional-neural-networks201412\&quot;\u003eEffective Use of Word Order for Text Categorization with Convolutional Neural Networks(2014/12)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#semi-supervised-convolutional-neural-networks-for-text-categorization-via-region-embedding201504\&quot;\u003eSemi-supervised Convolutional Neural Networks for Text Categorization via Region Embedding(2015/04)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#character-level-convolutional-networks-for-text-classification201509\&quot;\u003eCharacter-level Convolutional Networks for Text Classification(2015/09)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#a-sensitivity-analysis-of-and-practitioners-guide-to-convolutional-neural-networks-for-sentence-classification201510\&quot;\u003eA Sensitivity Analysis of (and Practitioners&#39; Guide to) Convolutional Neural Networks for Sentence Classification(2015/10)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%B3%BB%E5%88%97%E3%83%A9%E3%83%99%E3%83%AA%E3%83%B3%E3%82%B0%E5%93%81%E8%A9%9E%E3%82%BF%E3%82%B0%E4%BB%98%E3%81%91%E5%9B%BA%E6%9C%89%E8%A1%A8%E7%8F%BE%E8%AA%8D%E8%AD%98%E3%83%81%E3%83%A3%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0\&quot;\u003e系列ラベリング(品詞タグ付け・固有表現認識・チャンキング)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#natural-language-processing-almost-from-scratch201103\&quot;\u003eNatural Language Processing (almost) from Scratch(2011/03)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#learning-character-level-representations-for-part-of-speech-tagging201407\&quot;\u003eLearning Character-level Representations for Part-of-Speech Tagging(2014/07)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB\&quot;\u003e言語モデル\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#language-modeling-with-gated-convolutional-networks201612\&quot;\u003eLanguage Modeling with Gated Convolutional Networks(2016/12)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\&quot;\u003eおわりに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8F%82%E8%80%83\&quot;\u003e参考\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-696d0a1c-f1bd-4065-8b68-d0e033b0651b"></div>
    <div id="Toc-react-component-696d0a1c-f1bd-4065-8b68-d0e033b0651b"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:128,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;63d255fd038acbcdf95b&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="TokyoMickey"><a itemprop="url" href="/TokyoMickey"><img alt="TokyoMickey" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/141216/profile-images/1483272512" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hogefugabar"><a itemprop="url" href="/hogefugabar"><img alt="hogefugabar" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31899/profile-images/1473685791" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ryu0322"><a itemprop="url" href="/ryu0322"><img alt="ryu0322" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/123224/profile-images/1473715494" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="tashirovii"><a itemprop="url" href="/tashirovii"><img alt="tashirovii" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/29825/profile-images/1473760346" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hiki_neet_p"><a itemprop="url" href="/hiki_neet_p"><img alt="hiki_neet_p" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/53668/profile-images/1473693131" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kubor"><a itemprop="url" href="/kubor"><img alt="kubor" class="thumb thumb--xs" src="https://pbs.twimg.com/profile_images/798903621358219264/y1bDatvh_normal.jpg" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="tukkyr"><a itemprop="url" href="/tukkyr"><img alt="tukkyr" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/92230/profile-images/1473705563" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kiichi54321"><a itemprop="url" href="/kiichi54321"><img alt="kiichi54321" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/43633/profile-images/1473689593" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="saicologic"><a itemprop="url" href="/saicologic"><img alt="saicologic" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/2432/profile-images/1473681518" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="naotaka1128"><a itemprop="url" href="/naotaka1128"><img alt="naotaka1128" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/99986/profile-images/1484449518" /></a></div></div><div class="ArticleFooter__user"><a href="/Hironsan/items/63d255fd038acbcdf95b/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/63d255fd038acbcdf95b/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/Hironsan/items/63d255fd038acbcdf95b.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/ichiroex/items/7ff1cff3840520cf2410#_reference-584549b52d638a2e0bec"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/92685/profile-images/1473705706" />【テキスト分類】Convolutional Neural Networks for Sentence ClassificationをChainerで実装してみた</a><time class="references_datetime js-dateTimeView" datetime="2017-02-12T16:39:10+00:00">about 1 month ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/HirofumiYashima/items/91b3aade0d7c5b2d403b#_reference-22f3f2c540a8d6b99a07"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" />CNN で 時系列データ の 特徴量（特徴マップ） を 畳み込み で 抽出して、 + プーリング で 情報圧縮 する 方法 いろいろ</a><time class="references_datetime js-dateTimeView" datetime="2017-02-17T12:15:17+00:00">27 days ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="自然言語処理における畳み込みニューラルネットワークを用いたモデル by @Hironsan13 on @Qiita" data-url="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="自然言語処理における畳み込みニューラルネットワークを用いたモデル" href="http://b.hatena.ne.jp/entry/http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Hironsan/items/63d255fd038acbcdf95b" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:465458,&quot;uuid&quot;:&quot;63d255fd038acbcdf95b&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;Hironsan&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:77079,&quot;url_name&quot;:&quot;Hironsan&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-dcbeebd5-2c59-44bc-8541-38c35c2b8bb0"></div>
    <div id="CommentListContainer-react-component-dcbeebd5-2c59-44bc-8541-38c35c2b8bb0"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="Nz8lOit5w5kZPAHtdlj9zGgySLeSAQ6YNbAoEMLkxw2QmWKGuzPu21Z3/7J4+pGFaBmBgAPG7EBXjGBLraiPxA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Hironsan/items/63d255fd038acbcdf95b" /><input type="hidden" name="item_uuid" id="item_uuid" value="63d255fd038acbcdf95b" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/Hironsan/items/63d255fd038acbcdf95b", "id": 465458, "uuid": "63d255fd038acbcdf95b" }</script><script class="js-user" type="application/json">{&quot;id&quot;:77079,&quot;url_name&quot;:&quot;Hironsan&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="TGoq09HO0qVFGeAVP9gcsx8ESxx8DePyPHtD19vprvXrzG1vQYT/5wpSHkoxenD6Hy+CK+3KASpeRwuMtKXmPA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Hironsan/items/63d255fd038acbcdf95b" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-60064a257a213198cd13df56dcca54cf.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>