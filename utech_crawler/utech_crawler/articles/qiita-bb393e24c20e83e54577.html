<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>倒立振子で学ぶ DQN (Deep Q Network) - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="強化学習の一手法であるQ-learning とディープニューラルネットを組み合わせた Deep Q Network、通称DQNを使って倒立振子の振り上げ問題を解決してみます。


問題設定

「倒立振子の振り上げ問題」というのは、今回はこういう問題設定です。

まず空中に静止したモータがあって、モータ軸に棒の一端がつながっています。棒は中心に質量が集中していて剛性$\infty$で太さ0の、よくある棒です。初期状態では棒は重力にしたがって下向きにぶら下がっています。この..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="ashipong" name="twitter:creator" /><meta content="倒立振子で学ぶ DQN (Deep Q Network) - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/ashitani/items/bb393e24c20e83e54577" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="
強化学習の一手法であるQ-learning とディープニューラルネットを組み合わせた Deep Q Network、通称DQNを使って倒立振子の振り上げ問題を解決してみます。

## 問題設定

「倒立振子の振り上げ問題」というのは..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="cz1r3a6fBcqxSO4vkgvea4miv1ux3s9SlS4Vk6ajcAJtDCxBAECJtCXup0QTqltKu0JYUgsh4GiTqXYUBcwmLA==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"ashitani","type":"items","id":"bb393e24c20e83e54577"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;News&quot;,&quot;content&quot;:&quot;ストックの他に「いいね」が追加されました&quot;,&quot;url&quot;:&quot;http://blog.qiita.com/post/153200849029/qiita-like-button&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-1a4ab509-480d-4a38-acf7-f186cb9bf305"></div>
    <div id="HeaderContainer-react-component-1a4ab509-480d-4a38-acf7-f186cb9bf305"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92",        "name": "機械学習"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">倒立振子で学ぶ DQN (Deep Q Network)</h1><ul class="TagList"><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="126"><a class="u-link-unstyled TagList__label" href="/tags/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0"><img alt="ディープラーニング" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>ディープラーニング</span></a></li><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="1075"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="358"><a class="u-link-unstyled TagList__label" href="/tags/Chainer"><img alt="Chainer" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/755fdcf477b1d3db5946dad4f779ba11a5954c18/medium.jpg?1434432587" /><span>Chainer</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">195</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="0 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>0</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:195,&quot;uuid&quot;:&quot;bb393e24c20e83e54577&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="EqualL2"><a itemprop="url" href="/EqualL2"><img alt="EqualL2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/71548/profile-images/1473698862" /></a></li><li class="js-hovercard" data-hovercard-target-name="ytakky"><a itemprop="url" href="/ytakky"><img alt="ytakky" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/62018/profile-images/1473695825" /></a></li><li class="js-hovercard" data-hovercard-target-name="polikeiji"><a itemprop="url" href="/polikeiji"><img alt="polikeiji" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/8895/profile-images/1473681187" /></a></li><li class="js-hovercard" data-hovercard-target-name="msaito3"><a itemprop="url" href="/msaito3"><img alt="msaito3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/6140/profile-images/1473682612" /></a></li><li class="js-hovercard" data-hovercard-target-name="potix2"><a itemprop="url" href="/potix2"><img alt="potix2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5458/profile-images/1473682002" /></a></li><li class="js-hovercard" data-hovercard-target-name="khf11404"><a itemprop="url" href="/khf11404"><img alt="khf11404" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/85464/profile-images/1473703437" /></a></li><li class="js-hovercard" data-hovercard-target-name="kijibato"><a itemprop="url" href="/kijibato"><img alt="kijibato" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/95528/profile-images/1473706597" /></a></li><li class="js-hovercard" data-hovercard-target-name="fantm21"><a itemprop="url" href="/fantm21"><img alt="fantm21" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/32946/profile-images/1473686015" /></a></li><li class="js-hovercard" data-hovercard-target-name="yorksyo"><a itemprop="url" href="/yorksyo"><img alt="yorksyo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/44317/profile-images/1473689856" /></a></li><li class="js-hovercard" data-hovercard-target-name="kazuto1011"><a itemprop="url" href="/kazuto1011"><img alt="kazuto1011" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/81839/profile-images/1473702248" /></a></li><li><a href="/ashitani/items/bb393e24c20e83e54577/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/ashitani"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/30340/profile-images/1473685480" alt="1473685480" /></a> <a class="u-link-unstyled" href="/ashitani">ashitani</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-03-27T10:17:03+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-03-27">Edited at <time datetime="2016-09-03T10:18:25+09:00" itemprop="dateModified">2016-09-03</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/ashitani/items/bb393e24c20e83e54577/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">4</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/ashitani/items/bb393e24c20e83e54577/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(4)</span></a></li><li><a href="/ashitani/items/bb393e24c20e83e54577.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-bb393e24c20e83e54577" itemprop="articleBody"><p>強化学習の一手法であるQ-learning とディープニューラルネットを組み合わせた Deep Q Network、通称DQNを使って倒立振子の振り上げ問題を解決してみます。</p>

<h2>
<span id="問題設定" class="fragment"></span><a href="#%E5%95%8F%E9%A1%8C%E8%A8%AD%E5%AE%9A"><i class="fa fa-link"></i></a>問題設定</h2>

<p>「倒立振子の振り上げ問題」というのは、今回はこういう問題設定です。</p>

<p>まず空中に静止したモータがあって、モータ軸に棒の一端がつながっています。棒は中心に質量が集中していて剛性$\infty$で太さ0の、よくある棒です。初期状態では棒は重力にしたがって下向きにぶら下がっています。この状態から振り子を振り上げて倒立状態で静止させてください、という問題です。古きよき制御工学では、振り上げ用と静止用に別設計されたコントローラを2つ用意して切り替えるなど、非線形要素を含むコントローラを用いて対処することになります。いや、やったことないですけど、そうらしいです。</p>

<p>今回は、モータは右か左に一定トルクの回転しかできない、とします。また、ちょっとしたいじわるですが、モータのトルクはさほど大きくなく、初期状態から一方向に回し続けても重力に勝てず振り上げはできない、という条件にしました。下記はその罠にハマっているときのアニメーションです。ずっと右にトルクをかけている状態ですが、水平に向かうほど重力加速度の角度方向への寄与が大きくなるので押し戻され、振動します。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/62165b65-7c08-23c8-f6f0-083f88a3455a.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/62165b65-7c08-23c8-f6f0-083f88a3455a.gif" alt="initial.gif" title="initial.gif"></a></p>

<p>DQNそのものについては<a href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" id="reference-ce560049f69bbf954ef2">こちら</a>の素晴らしい記事が詳しいので、主に結果と、実装まわりの工夫を今回の記事では説明します。</p>

<h2>
<span id="まず結果から" class="fragment"></span><a href="#%E3%81%BE%E3%81%9A%E7%B5%90%E6%9E%9C%E3%81%8B%E3%82%89"><i class="fa fa-link"></i></a>まず結果から</h2>

<p>エージェント（今回はモータのコントローラ）は環境（モータと棒）に対してアクション（モータの回転方向の指示）を行い、報酬と何らかの観測結果を得る、という条件下でエージェントに最適方策を学ばせます。</p>

<p>報酬は、モータから見た棒の先端の高さを$h$としたとき、高いほどうれしい、という下記のような関数$r(h)$を用いました。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
r(h)= \Biggl\{\quad 
\begin{eqnarray}
5h　&amp; \mathrm{if}　h\ge 0\\
 h　&amp; \mathrm{if}　h&lt; 0
\end{eqnarray}
</pre></div></div>

<p>プラス側にバイアスをかけたのですが、余計なお世話だったかもしれません。観測は、ATARIの例などでは画面の画像を直接入力していますが、今回は振り子の角度そのものを入力してみました。シミュレーションの４ステップぶんの角度列をシーケンスとして得られることとします。</p>

<p>下記が成長の様子をプロットしたものです。横軸が試行回数、縦軸が試行で得られた総得点です。青点が各世代の結果、赤線がハイスコアです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/b091a801-0dd8-c961-9576-c544aa04e01b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/b091a801-0dd8-c961-9576-c544aa04e01b.png" alt="bestER.png" title="bestER.png"></a></p>

<p>非線形性と多峰性を持った系のせいか、結果はとっても振動的で、収束後にも正負の成績で振動していますが、ハイスコア結果は確実に成長しています。以下で成長の過程を見てみます。</p>

<p>初回、罠にはまってますね。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/30340/2d5a7e6d-b285-53f6-6af7-b6751da71bb8.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/2d5a7e6d-b285-53f6-6af7-b6751da71bb8.gif" alt="000000.gif" title="000000.gif"></a></p>

<p>120回目、往復すれば振り上げが可能だということに気づきますが、その後止めることができません。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/30340/472cd12a-91fb-061f-09d4-c276486d8321.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/472cd12a-91fb-061f-09d4-c276486d8321.gif" alt="000120.gif" title="000120.gif"></a></p>

<p>6950回目、振り上げ後の静止のコツをつかんできたようです。もうちょっとだ！がんばれー！<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/30340/bee4bb60-42a3-0b75-1ebb-adb5e782435e.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/bee4bb60-42a3-0b75-1ebb-adb5e782435e.gif" alt="006950.gif" title="006950.gif"></a></p>

<p>7640回目、ほぼ目的は達成できました。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/30340/89cc3e8f-aa75-5ad8-4307-e485e31a9330.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/89cc3e8f-aa75-5ad8-4307-e485e31a9330.gif" alt="007640.gif" title="007640.gif"></a></p>

<p>30000イテレーション内のベスト結果がこれです。ち<br>
ょっと行き過ぎてますが、最初の振り上げを最短時間でやるほうが有利と気づいたようです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/c292ed92-ef24-807b-a2e3-123fabdbbdf9.gif" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/c292ed92-ef24-807b-a2e3-123fabdbbdf9.gif" alt="024410.gif" title="024410.gif"></a></p>

<p>思ってたよりうまくいってちょっと驚いています。最後の例について、時系列で高さとモータへの制御入力のプロファイルをプロットしたのが以下です。振り上げ動作と保持では考え方が全く違うのですが、そのことを学べている様子がわかります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/cb75b60d-8dc2-76c3-1a10-be8adb1d6d6b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/cb75b60d-8dc2-76c3-1a10-be8adb1d6d6b.png" alt="profile.png" title="profile.png"></a></p>

<h2>
<span id="dqnの実装について" class="fragment"></span><a href="#dqn%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6"><i class="fa fa-link"></i></a>DQNの実装について</h2>

<p><a href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5">先ほど紹介したDQNの記事</a>には実装も示されているのですが、今回は理解のために車輪の再実装をしてみました。<a href="http://arxiv.org/pdf/1312.5602.pdf" rel="nofollow noopener" target="_blank">こちらの論文</a>にしたがって実装したものを<a href="https://github.com/ashitani/DQN_pendulum" rel="nofollow noopener" target="_blank">こちら</a>に置きました。</p>

<p>論文を読んだだけでは十分理解できていなかったのは、肝心のディープネットはどのように構成するのか、どのように更新するのか、というところでした。その辺りを解説します。</p>

<p>ディープニューラルネット$Q$は状態観測結果のシーケンスを入力すると、各行動の行動価値を出力するネットです。行動価値は、今回の場合は「入力された角度シーケンスの示す状況では、モータを右、左に回すのはそれぞれ、どれぐらいうれしいか」ということを示すベクタになります。</p>

<p>もちろん学習初期ではこのネットはランダムなので、でたらめな結果を返します。これを今から説明する手順で更新していくと、将来にわたってトータルの報酬がたくさん得られるようないいかんじのネットに成長していきます。</p>

<p>状態$s_t$に対してあるアクション$a_t$ を行って報酬$r_t$が得られ、$s_{t+1}$に変わったとします。このとき、$s_t$を入力したときの$Q$の出力である行動価値ベクタ$Q(s_t)$のうち、$a_t$に相当する行動価値だけを下記の式にしたがって書き換えた$y_t$を作ります(だいぶ元論文から記号を変えています)。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
y_t = Q(s_t)  \\
y_t[a_t] \leftarrow r_t + γ max Q(s_{t+1})
</pre></div></div>

<p>この$y_t$に$Q(s_t)$が近づくように、ネットの重みを一歩更新します。今回の報酬と、次の一手で得られる最大の行動価値に一定の割引率$\gamma$ をかけたものを足しています。エピソード終端までの行動価値を足した和から、今回の行動の価値を決めるのが理想ですが、演算時間的にも無理があるので、次の一歩だけを行います。この更新手順を無限回繰り返せば、状態から総報酬に基づいた行動価値を得られることになります。ほんとかしら。少なくとも私はそう理解しました、ぐらいの表現にしておきます。</p>

<p>ディープネットそのものの構成はあまり試行錯誤をしていませんが、深めにしておいて間違いはないのかなと思います。ほんとうはDropoutやBatch Normalizationなどを入れて汎化性能をあげたほうがよいのでしょうか。問題によるとは思いますが。</p>

<h2>
<span id="best-experience-replay" class="fragment"></span><a href="#best-experience-replay"><i class="fa fa-link"></i></a>Best Experience Replay</h2>

<p>今回の実装は基本的には論文通りですが、一点だけ工夫した点があるので解説します。</p>

<p>学習に使う「状態・アクション・報酬」のセットは、お互いに相関のないものを使ったほうがよいそうです。このためにER(Experience Replay)という手法が重要になります。DQNの最大のポイントの一つのようですね。過去の経験を覚えておいて、そこからランダムで先ほどのセットを取り出したものに対して学習を行う、という手法です。</p>

<p>一回の試行のことをエピソードと呼びますが、元論文ではすべてのエピソードをまるごと記憶します。あたらしい経験を得るために、各エピソードでは $\epsilon$-greedyと呼ばれる方法でいろいろと試します。一定確率$\epsilon$でランダム動作とネットにしたがった動作（greedy動作）を選択するのが$\epsilon$-greedyです。学習初期は$\epsilon$が大きく、ほとんどランダム動作だけから学ぶことになります。</p>

<p>学習が十分に進んだあとは、greedy動作だけで結果を出すことができます。このため、ときどき完全greedy動作をしてみて様子を見ます。</p>

<p>DQNの論文で常々疑問だったのですが試してみるとやはりその通りで、初期は特に、あきらかに覚える価値のないエピソードがどんどん溜まっていき、数少ない良い経験はメモリから消えていきます。なにせ完全ランダムですから。もちろん失敗例から学べることもあるでしょうが、どうせならちょっとでもいい経験を手本にしたほうがよいのではないかと思います。というわけで今回は、$\epsilon$-greedyエピソードもgreedyエピソードも区別せず、良い点数（生涯報酬）を得たエピソードを優先して記憶に残すことにしました。歴代ベスト100エピソード、を保持することにして、ランキング内に入る点数のエピソードだけ記憶入りします。ただし、ランキングに入れなくても1%の確率で記憶入りできます。</p>

<p>この方法を仮にBest ERと呼ぶことにします。同じシード値で初期化した状態から、Best ERとシンプルなERとの収束状況の違いをプロットしてみました。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/93b57449-7e4e-8f6c-03d3-8f5040b6de41.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/93b57449-7e4e-8f6c-03d3-8f5040b6de41.png" alt="compare.png" title="compare.png"></a></p>

<p>だいぶ効果あるように見えます。計算機資源の都合からいろんなケースでは試してませんが。。</p>

<p>新しい経験をまったく取り入れない方式をしばらく試したのですが、収束後、しばらく黄金時代を迎えたあとに衰退していくことがわかりました。下記のようになりました。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/30340/1a208ef5-cbe4-855b-7846-2b5f474aa75f.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/30340/1a208ef5-cbe4-855b-7846-2b5f474aa75f.png" alt="bestER.png" title="bestER.png"></a></p>

<p>乱数のseedを変えても同じようなことが起きました。原因はよく調べていませんが想像するに、成長したgreedyエピソードの高得点な実績がランキングを埋め尽くしたあとの出来事なので、データの相関がありすぎたり多様性を失ったりしているためではないかと思います。ただ、改善版は黄金時代にも至っていないだけにも見えますのでまだ改良の余地があるかなとも思います。</p>

<p>また、Best ERは小さな成功にとらわれすぎてハマる場合があると思います。あとは環境の変化にも十分対応できませんね。過去の栄光にとらわれてしまうので。なんだか先ほどの黄金時代の件といい、つい人生論に重ねたくなってしまいますが、まあ、タスクによる、ということですね。</p>

<p>DQNというものの傾向なのかもしれませんが、学習が進んでるかどうかがわかりにくいので、良さそうな候補はセーブ、というのは必須ですね。また、多峰性関数の数値最適化にありがちなことですが、成果が出るまでの時間が初期値にすごく依存するなあというのが印象です。とはいえ、今回のケースを何回か試した感じだと1万イテレーション付近でほぼ最高スコアを叩き出せます。手元のPC(MacBook Pro Core i5 2.5GHz x2Core)で1時間ぐらいでした。</p>

<h2>
<span id="まとめ" class="fragment"></span><a href="#%E3%81%BE%E3%81%A8%E3%82%81"><i class="fa fa-link"></i></a>まとめ</h2>

<p>常々やってみたかったDQNを試すことができました。また、収束を早める方法としてBest ERという方法を提案してみました。自分で実装すると理解が深まって良いですね。</p>

<p>コントローラ側のチューニングも切り替えも行わず、こういう結果が出るのは驚きです。モータ制御量を連続量にする、二重振り子化、観測を角度ではなく画像でやる、実機とウェブカメラでやる、などなど、やりたいことはいろいろ出てきますが時間の都合でここまでとします。</p>

<p>今回はCPUでやりましたが、やはりこの分野で試行錯誤するにはCPUでは限界ですね。先日発表された<a href="https://cloud.google.com/products/machine-learning/" rel="nofollow noopener" target="_blank">Cloud Machine Learning</a>が楽しみです。おとなしくGPUを買えよという声も聞こえますが。。。</p>
<div class="hidden"><form class="js-task-list-update" action="/ashitani/items/bb393e24c20e83e54577" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="OiuF9qVBgEZf8J5tGYLK3BxFvsYD1qGzE20cfr3om5QkGsJqC54MOMtW1waYI0/9LqVZz7kpjokV6n/5HofNug==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1472865505" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">

強化学習の一手法であるQ-learning とディープニューラルネットを組み合わせた Deep Q Network、通称DQNを使って倒立振子の振り上げ問題を解決してみます。

## 問題設定

「倒立振子の振り上げ問題」というのは、今回はこういう問題設定です。

まず空中に静止したモータがあって、モータ軸に棒の一端がつながっています。棒は中心に質量が集中していて剛性$\infty$で太さ0の、よくある棒です。初期状態では棒は重力にしたがって下向きにぶら下がっています。この状態から振り子を振り上げて倒立状態で静止させてください、という問題です。古きよき制御工学では、振り上げ用と静止用に別設計されたコントローラを2つ用意して切り替えるなど、非線形要素を含むコントローラを用いて対処することになります。いや、やったことないですけど、そうらしいです。

今回は、モータは右か左に一定トルクの回転しかできない、とします。また、ちょっとしたいじわるですが、モータのトルクはさほど大きくなく、初期状態から一方向に回し続けても重力に勝てず振り上げはできない、という条件にしました。下記はその罠にハマっているときのアニメーションです。ずっと右にトルクをかけている状態ですが、水平に向かうほど重力加速度の角度方向への寄与が大きくなるので押し戻され、振動します。

![initial.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/62165b65-7c08-23c8-f6f0-083f88a3455a.gif &quot;initial.gif&quot;)

DQNそのものについては[こちら](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)の素晴らしい記事が詳しいので、主に結果と、実装まわりの工夫を今回の記事では説明します。

## まず結果から

エージェント（今回はモータのコントローラ）は環境（モータと棒）に対してアクション（モータの回転方向の指示）を行い、報酬と何らかの観測結果を得る、という条件下でエージェントに最適方策を学ばせます。

報酬は、モータから見た棒の先端の高さを$h$としたとき、高いほどうれしい、という下記のような関数$r(h)$を用いました。

```math
r(h)= \Biggl\{\quad 
\begin{eqnarray}
5h　&amp; \mathrm{if}　h\ge 0\\
 h　&amp; \mathrm{if}　h&lt; 0
\end{eqnarray}
```

プラス側にバイアスをかけたのですが、余計なお世話だったかもしれません。観測は、ATARIの例などでは画面の画像を直接入力していますが、今回は振り子の角度そのものを入力してみました。シミュレーションの４ステップぶんの角度列をシーケンスとして得られることとします。

下記が成長の様子をプロットしたものです。横軸が試行回数、縦軸が試行で得られた総得点です。青点が各世代の結果、赤線がハイスコアです。

![bestER.png](https://qiita-image-store.s3.amazonaws.com/0/30340/b091a801-0dd8-c961-9576-c544aa04e01b.png &quot;bestER.png&quot;)


非線形性と多峰性を持った系のせいか、結果はとっても振動的で、収束後にも正負の成績で振動していますが、ハイスコア結果は確実に成長しています。以下で成長の過程を見てみます。

初回、罠にはまってますね。
![000000.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/2d5a7e6d-b285-53f6-6af7-b6751da71bb8.gif &quot;000000.gif&quot;)


120回目、往復すれば振り上げが可能だということに気づきますが、その後止めることができません。
![000120.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/472cd12a-91fb-061f-09d4-c276486d8321.gif &quot;000120.gif&quot;)

6950回目、振り上げ後の静止のコツをつかんできたようです。もうちょっとだ！がんばれー！
![006950.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/bee4bb60-42a3-0b75-1ebb-adb5e782435e.gif &quot;006950.gif&quot;)

7640回目、ほぼ目的は達成できました。
![007640.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/89cc3e8f-aa75-5ad8-4307-e485e31a9330.gif &quot;007640.gif&quot;)

30000イテレーション内のベスト結果がこれです。ち
ょっと行き過ぎてますが、最初の振り上げを最短時間でやるほうが有利と気づいたようです。

![024410.gif](https://qiita-image-store.s3.amazonaws.com/0/30340/c292ed92-ef24-807b-a2e3-123fabdbbdf9.gif &quot;024410.gif&quot;)


思ってたよりうまくいってちょっと驚いています。最後の例について、時系列で高さとモータへの制御入力のプロファイルをプロットしたのが以下です。振り上げ動作と保持では考え方が全く違うのですが、そのことを学べている様子がわかります。

![profile.png](https://qiita-image-store.s3.amazonaws.com/0/30340/cb75b60d-8dc2-76c3-1a10-be8adb1d6d6b.png &quot;profile.png&quot;)



## DQNの実装について

[先ほど紹介したDQNの記事](http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)には実装も示されているのですが、今回は理解のために車輪の再実装をしてみました。[こちらの論文](http://arxiv.org/pdf/1312.5602.pdf)にしたがって実装したものを[こちら](https://github.com/ashitani/DQN_pendulum)に置きました。

論文を読んだだけでは十分理解できていなかったのは、肝心のディープネットはどのように構成するのか、どのように更新するのか、というところでした。その辺りを解説します。

ディープニューラルネット$Q$は状態観測結果のシーケンスを入力すると、各行動の行動価値を出力するネットです。行動価値は、今回の場合は「入力された角度シーケンスの示す状況では、モータを右、左に回すのはそれぞれ、どれぐらいうれしいか」ということを示すベクタになります。

もちろん学習初期ではこのネットはランダムなので、でたらめな結果を返します。これを今から説明する手順で更新していくと、将来にわたってトータルの報酬がたくさん得られるようないいかんじのネットに成長していきます。

状態$s\_t$に対してあるアクション$a\_t$ を行って報酬$r\_t$が得られ、$s\_{t+1}$に変わったとします。このとき、$s\_t$を入力したときの$Q$の出力である行動価値ベクタ$Q(s_t)$のうち、$a\_t$に相当する行動価値だけを下記の式にしたがって書き換えた$y\_t$を作ります(だいぶ元論文から記号を変えています)。

```math
y_t = Q(s_t)  \\
y_t[a_t] \leftarrow r_t + γ max Q(s_{t+1})
```

この$y\_t$に$Q(s\_t)$が近づくように、ネットの重みを一歩更新します。今回の報酬と、次の一手で得られる最大の行動価値に一定の割引率$\gamma$ をかけたものを足しています。エピソード終端までの行動価値を足した和から、今回の行動の価値を決めるのが理想ですが、演算時間的にも無理があるので、次の一歩だけを行います。この更新手順を無限回繰り返せば、状態から総報酬に基づいた行動価値を得られることになります。ほんとかしら。少なくとも私はそう理解しました、ぐらいの表現にしておきます。

ディープネットそのものの構成はあまり試行錯誤をしていませんが、深めにしておいて間違いはないのかなと思います。ほんとうはDropoutやBatch Normalizationなどを入れて汎化性能をあげたほうがよいのでしょうか。問題によるとは思いますが。

## Best Experience Replay

今回の実装は基本的には論文通りですが、一点だけ工夫した点があるので解説します。

学習に使う「状態・アクション・報酬」のセットは、お互いに相関のないものを使ったほうがよいそうです。このためにER(Experience Replay)という手法が重要になります。DQNの最大のポイントの一つのようですね。過去の経験を覚えておいて、そこからランダムで先ほどのセットを取り出したものに対して学習を行う、という手法です。

一回の試行のことをエピソードと呼びますが、元論文ではすべてのエピソードをまるごと記憶します。あたらしい経験を得るために、各エピソードでは $\epsilon$-greedyと呼ばれる方法でいろいろと試します。一定確率$\epsilon$でランダム動作とネットにしたがった動作（greedy動作）を選択するのが$\epsilon$-greedyです。学習初期は$\epsilon$が大きく、ほとんどランダム動作だけから学ぶことになります。

学習が十分に進んだあとは、greedy動作だけで結果を出すことができます。このため、ときどき完全greedy動作をしてみて様子を見ます。

DQNの論文で常々疑問だったのですが試してみるとやはりその通りで、初期は特に、あきらかに覚える価値のないエピソードがどんどん溜まっていき、数少ない良い経験はメモリから消えていきます。なにせ完全ランダムですから。もちろん失敗例から学べることもあるでしょうが、どうせならちょっとでもいい経験を手本にしたほうがよいのではないかと思います。というわけで今回は、$\epsilon$-greedyエピソードもgreedyエピソードも区別せず、良い点数（生涯報酬）を得たエピソードを優先して記憶に残すことにしました。歴代ベスト100エピソード、を保持することにして、ランキング内に入る点数のエピソードだけ記憶入りします。ただし、ランキングに入れなくても1%の確率で記憶入りできます。

この方法を仮にBest ERと呼ぶことにします。同じシード値で初期化した状態から、Best ERとシンプルなERとの収束状況の違いをプロットしてみました。

![compare.png](https://qiita-image-store.s3.amazonaws.com/0/30340/93b57449-7e4e-8f6c-03d3-8f5040b6de41.png &quot;compare.png&quot;)


だいぶ効果あるように見えます。計算機資源の都合からいろんなケースでは試してませんが。。

新しい経験をまったく取り入れない方式をしばらく試したのですが、収束後、しばらく黄金時代を迎えたあとに衰退していくことがわかりました。下記のようになりました。

![bestER.png](https://qiita-image-store.s3.amazonaws.com/0/30340/1a208ef5-cbe4-855b-7846-2b5f474aa75f.png &quot;bestER.png&quot;)


乱数のseedを変えても同じようなことが起きました。原因はよく調べていませんが想像するに、成長したgreedyエピソードの高得点な実績がランキングを埋め尽くしたあとの出来事なので、データの相関がありすぎたり多様性を失ったりしているためではないかと思います。ただ、改善版は黄金時代にも至っていないだけにも見えますのでまだ改良の余地があるかなとも思います。

また、Best ERは小さな成功にとらわれすぎてハマる場合があると思います。あとは環境の変化にも十分対応できませんね。過去の栄光にとらわれてしまうので。なんだか先ほどの黄金時代の件といい、つい人生論に重ねたくなってしまいますが、まあ、タスクによる、ということですね。

DQNというものの傾向なのかもしれませんが、学習が進んでるかどうかがわかりにくいので、良さそうな候補はセーブ、というのは必須ですね。また、多峰性関数の数値最適化にありがちなことですが、成果が出るまでの時間が初期値にすごく依存するなあというのが印象です。とはいえ、今回のケースを何回か試した感じだと1万イテレーション付近でほぼ最高スコアを叩き出せます。手元のPC(MacBook Pro Core i5 2.5GHz x2Core)で1時間ぐらいでした。

## まとめ

常々やってみたかったDQNを試すことができました。また、収束を早める方法としてBest ERという方法を提案してみました。自分で実装すると理解が深まって良いですね。

コントローラ側のチューニングも切り替えも行わず、こういう結果が出るのは驚きです。モータ制御量を連続量にする、二重振り子化、観測を角度ではなく画像でやる、実機とウェブカメラでやる、などなど、やりたいことはいろいろ出てきますが時間の都合でここまでとします。

今回はCPUでやりましたが、やはりこの分野で試行錯誤するにはCPUでは限界ですね。先日発表された[Cloud Machine Learning](https://cloud.google.com/products/machine-learning/)が楽しみです。おとなしくGPUを買えよという声も聞こえますが。。。
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="倒立振子で学ぶ DQN (Deep Q Network) by @ashipong on @Qiita" data-url="http://qiita.com/ashitani/items/bb393e24c20e83e54577" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="倒立振子で学ぶ DQN (Deep Q Network)" href="http://b.hatena.ne.jp/entry/http://qiita.com/ashitani/items/bb393e24c20e83e54577" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/ashitani/items/bb393e24c20e83e54577" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/ashitani/items/bb393e24c20e83e54577" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/ashitani"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/30340/profile-images/1473685480" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/ashitani">ashitani</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">557</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;ashitani&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-e6f4aae3-a922-4acc-b7a5-6b3ad3e03993"></div>
    <div id="UserFollowButton-react-component-e6f4aae3-a922-4acc-b7a5-6b3ad3e03993"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/ashitani/items/bb393e24c20e83e54577">倒立振子で学ぶ DQN (Deep Q Network)</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/ashitani/items/1dc0a54da218ec224ad8">関数近似で学ぶ chainer とディープラーニング</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/ashitani/items/2e48729e78a9f77f9790">Distributed TensorFlowを試してみる</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/ashitani/items/8b52a6b0ca812712a348">Distributed TensorFlow を Google Cloud Platform で動かしてみる</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/ashitani/items/566cf9234682cb5f2d60">ペンパイナッポーとアッポーペンを識別する(ChainerでYOLO ver2)</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%95%8F%E9%A1%8C%E8%A8%AD%E5%AE%9A\&quot;\u003e問題設定\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%BE%E3%81%9A%E7%B5%90%E6%9E%9C%E3%81%8B%E3%82%89\&quot;\u003eまず結果から\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dqn%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\&quot;\u003eDQNの実装について\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#best-experience-replay\&quot;\u003eBest Experience Replay\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%BE%E3%81%A8%E3%82%81\&quot;\u003eまとめ\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-e412ed95-9cf9-42b9-bb42-5580132346cb"></div>
    <div id="Toc-react-component-e412ed95-9cf9-42b9-bb42-5580132346cb"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:195,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;bb393e24c20e83e54577&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="EqualL2"><a itemprop="url" href="/EqualL2"><img alt="EqualL2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/71548/profile-images/1473698862" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ytakky"><a itemprop="url" href="/ytakky"><img alt="ytakky" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/62018/profile-images/1473695825" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="polikeiji"><a itemprop="url" href="/polikeiji"><img alt="polikeiji" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/8895/profile-images/1473681187" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="msaito3"><a itemprop="url" href="/msaito3"><img alt="msaito3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/6140/profile-images/1473682612" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="potix2"><a itemprop="url" href="/potix2"><img alt="potix2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5458/profile-images/1473682002" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="khf11404"><a itemprop="url" href="/khf11404"><img alt="khf11404" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/85464/profile-images/1473703437" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kijibato"><a itemprop="url" href="/kijibato"><img alt="kijibato" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/95528/profile-images/1473706597" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="fantm21"><a itemprop="url" href="/fantm21"><img alt="fantm21" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/32946/profile-images/1473686015" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="yorksyo"><a itemprop="url" href="/yorksyo"><img alt="yorksyo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/44317/profile-images/1473689856" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kazuto1011"><a itemprop="url" href="/kazuto1011"><img alt="kazuto1011" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/81839/profile-images/1473702248" /></a></div></div><div class="ArticleFooter__user"><a href="/ashitani/items/bb393e24c20e83e54577/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/bb393e24c20e83e54577/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/ashitani/items/bb393e24c20e83e54577.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><a class="references_toggleOldReferences js-toggleOldReferences" href="#"><i class="fa fa-expand js-toggleOldReferencesIcon"></i><span class="js-toggleOldReferencesText">Show old 1 links</span></a><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/chachay/items/5fdb7c64af68bcacf7d3#_reference-e48340e1591cdb3041ff"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/117379/profile-images/1476087329" />ChainerでやってみるDeep Q Learning - 立ち上げ編</a><time class="references_datetime js-dateTimeView" datetime="2016-04-19T17:04:43+00:00">11 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/yai/items/f09b681f6a2d263ee6d6#_reference-946d0f045cea2bb42606"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/117826/profile-images/1473713721" />TensorFlowでDQNを実装（したかった・・・）</a><time class="references_datetime js-dateTimeView" datetime="2016-04-25T13:55:21+00:00">11 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/HirofumiYashima/items/16da52eaea74e55d75da#_reference-44c54e484990eaa18151"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" />深層強化学習モデルで、Atari2600ゲームを実行する環境構築とモデル ～データセットDL元URLと、DQN独自実装モデル 事例２件の紹介（muupannさん と mhausknさん）</a><time class="references_datetime js-dateTimeView" datetime="2016-07-28T03:06:27+00:00">8 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/ryo_grid/items/72e1b64050650be3504b#_reference-a8cb4734d21a0a5001b1"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/12325/profile-images/1473682364" />深層強化学習(DQN)でオセロもどきAIを作ってみた</a><time class="references_datetime js-dateTimeView" datetime="2016-11-30T06:56:44+00:00">4 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/ryo_grid/items/94f3a95d1e8d2ccddd6a#_reference-03075cf2cae42ca9144a"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/12325/profile-images/1473682364" />PythonでAIシミュレーションプラットフォームOpen AI Gym を利用して遊ぶ (DQN編)</a><time class="references_datetime js-dateTimeView" datetime="2016-12-12T18:05:34+00:00">3 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/nbh/items/8ec4721cf0f6a7ff30f3#_reference-c7ba93c2b1861c1f1ac3"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/136986/profile-images/1479136007" />DQN(Chainer)しながらマルチスレッド(Threading)で結果表示(PyQt)</a><time class="references_datetime js-dateTimeView" datetime="2017-01-21T16:43:58+00:00">about 2 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="倒立振子で学ぶ DQN (Deep Q Network) by @ashipong on @Qiita" data-url="http://qiita.com/ashitani/items/bb393e24c20e83e54577" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="倒立振子で学ぶ DQN (Deep Q Network)" href="http://b.hatena.ne.jp/entry/http://qiita.com/ashitani/items/bb393e24c20e83e54577" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/ashitani/items/bb393e24c20e83e54577" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/ashitani/items/bb393e24c20e83e54577" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:380670,&quot;uuid&quot;:&quot;bb393e24c20e83e54577&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;ashitani&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:30340,&quot;url_name&quot;:&quot;ashitani&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/30340/profile-images/1473685480&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-9c064e16-a747-41f1-aaff-c6d7044c5e90"></div>
    <div id="CommentListContainer-react-component-9c064e16-a747-41f1-aaff-c6d7044c5e90"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="Alcq3yO5FHUXDuXsPDFa3LZio/Tu5mbjQVuoIiOFjiwcZm1DjWaYC4OorIe9kN/9hIJE/VQZSdlH3MulgOrYAg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/ashitani/items/bb393e24c20e83e54577" /><input type="hidden" name="item_uuid" id="item_uuid" value="bb393e24c20e83e54577" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/ashitani/items/bb393e24c20e83e54577", "id": 380670, "uuid": "bb393e24c20e83e54577" }</script><script class="js-user" type="application/json">{&quot;id&quot;:30340,&quot;url_name&quot;:&quot;ashitani&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/30340/profile-images/1473685480&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="7lasR+kF1xAZkLX2lMa8vUTqnfwg9ebInm47ugEH/EHwZ+vbR9pbbo02/J0VZzmcdgp69ZoKyfKY6Vg9omiqbw==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/ashitani/items/bb393e24c20e83e54577" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>