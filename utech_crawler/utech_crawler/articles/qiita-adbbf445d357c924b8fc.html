<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>画像処理の数式を見て石になった時のための、金の針 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="画像処理は難しい。
Instagramのキレイなフィルタ、GoogleのPhoto Sphere、そうしたサービスを見て画像は面白そうだ！と心躍らせて開いた画像処理の本。そこに山と羅列される数式を前に石化せざるを得なかった俺たちが、耳にささやかれる「難しいことはOpenCVがやってくれるわ。そうでしょ？」という声に身をゆだねる以外に何ができただろう。

本稿は石化せざるを得なかったあの頃を克服し、OpenCVを使いながらも基礎的な理論を理解したいと願う方へ、その道筋(ア..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="icoxfog417" name="twitter:creator" /><meta content="画像処理の数式を見て石になった時のための、金の針 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="画像処理は難しい。
Instagramのキレイなフィルタ、GoogleのPhoto Sphere、そうしたサービスを見て画像は面白そうだ！と心躍らせて開いた画像処理の本。そこに山と羅列される数式を前に石化せざるを得なかった俺たちが、耳..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="NTZ6hKiOO/tRYvxCWRSFZec6iEXanvj8/7vzVz3oaV5bLHmltQkWzo4AtSp4FAmHlCgx77k8PA2es3gd+YePNg==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"icoxfog417","type":"items","id":"adbbf445d357c924b8fc"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;Hot&quot;,&quot;content&quot;:&quot;Markdownによる情報共有サービス、Qiita:Team&quot;,&quot;url&quot;:&quot;https://teams.qiita.com?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-2112c0dc-819f-4f29-a80a-4f4a7ffda2f1"></div>
    <div id="HeaderContainer-react-component-2112c0dc-819f-4f29-a80a-4f4a7ffda2f1"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92",        "name": "機械学習"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">画像処理の数式を見て石になった時のための、金の針</h1><ul class="TagList"><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="651"><a class="u-link-unstyled TagList__label" href="/tags/OpenCV"><img alt="OpenCV" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/52891c930a92eb009cc503c1c2165c866ad9d20a/medium.jpg?1470059263" /><span>OpenCV</span></a></li><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li><li class="TagList__item" data-count="22"><a class="u-link-unstyled TagList__label" href="/tags/ComputerVision"><img alt="ComputerVision" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/6be5034d4fe3518c5f4650d98459067c95189ae5/medium.jpg?1422923465" /><span>ComputerVision</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">1870</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="3 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>3</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:1870,&quot;uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="fumi23"><a itemprop="url" href="/fumi23"><img alt="fumi23" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/110423/profile-images/1473711205" /></a></li><li class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></li><li class="js-hovercard" data-hovercard-target-name="ksomemo"><a itemprop="url" href="/ksomemo"><img alt="ksomemo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/6982/profile-images/1473684010" /></a></li><li class="js-hovercard" data-hovercard-target-name="Yatima"><a itemprop="url" href="/Yatima"><img alt="Yatima" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/17440/profile-images/1473682159" /></a></li><li class="js-hovercard" data-hovercard-target-name="eve_yk"><a itemprop="url" href="/eve_yk"><img alt="eve_yk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" /></a></li><li class="js-hovercard" data-hovercard-target-name="shiraco"><a itemprop="url" href="/shiraco"><img alt="shiraco" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5707/profile-images/1473682202" /></a></li><li class="js-hovercard" data-hovercard-target-name="ueno3"><a itemprop="url" href="/ueno3"><img alt="ueno3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/75359/profile-images/1473700135" /></a></li><li class="js-hovercard" data-hovercard-target-name="paralleltree"><a itemprop="url" href="/paralleltree"><img alt="paralleltree" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/55804/profile-images/1473693796" /></a></li><li class="js-hovercard" data-hovercard-target-name="hnakamur"><a itemprop="url" href="/hnakamur"><img alt="hnakamur" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/3227/profile-images/1473682812" /></a></li><li class="js-hovercard" data-hovercard-target-name="tomohisaota"><a itemprop="url" href="/tomohisaota"><img alt="tomohisaota" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/33513/profile-images/1473686144" /></a></li><li><a href="/icoxfog417/items/adbbf445d357c924b8fc/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/icoxfog417"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516" alt="1484303516" /></a> <a class="u-link-unstyled" href="/icoxfog417">icoxfog417</a> </div><div class="ArticleAsideHeader__date"><meta content="2016-01-31T15:10:23+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2016-01-31">Edited at <time datetime="2016-03-17T10:02:05+09:00" itemprop="dateModified">2016-03-17</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/icoxfog417/items/adbbf445d357c924b8fc/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">16</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/icoxfog417/items/adbbf445d357c924b8fc/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(16)</span></a></li><li><a href="/icoxfog417/items/adbbf445d357c924b8fc.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-adbbf445d357c924b8fc" itemprop="articleBody"><div class="alert alert-warning"><i class="fa fa-clock-o"></i> More than 1 year has passed since last update.</div><p>画像処理は難しい。<br>
Instagramのキレイなフィルタ、GoogleのPhoto Sphere、そうしたサービスを見て画像は面白そうだ！と心躍らせて開いた画像処理の本。そこに山と羅列される数式を前に石化せざるを得なかった俺たちが、耳にささやかれる「難しいことはOpenCVがやってくれるわ。そうでしょ？」という声に身をゆだねる以外に何ができただろう。</p>

<p>本稿は石化せざるを得なかったあの頃を克服し、OpenCVを使いながらも基礎的な理論を理解したいと願う方へ、その道筋(アイテム的には金の針)を示すものになればと思います。<br>
扱う範囲としては、あらゆる処理の基礎となる「画像の特徴点検出」を対象とします(<a href="https://www.oreilly.co.jp/books/9784873116075/" rel="nofollow noopener" target="_blank">実践 コンピュータビジョン</a>の2章に相当)。なお、本記事自体、初心者である私が理解しながら書いているため、上級画像処理冒険者の方は誤りなどあれば指摘していただければ幸いです。</p>

<h1>
<span id="画像の特徴点とは" class="fragment"></span><a href="#%E7%94%BB%E5%83%8F%E3%81%AE%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%A8%E3%81%AF"><i class="fa fa-link"></i></a>画像の特徴点とは</h1>

<p>人間がジグソーパズルを組み立てられるのはなぜか？と考えると、私たちはパズルの各ピースの特徴を把握して、それと似た、また連続する特徴を見つけ出して繋ぎあわせているのだ、と考えることができます。同様に、複数の写真をつなげてパノラマ写真にすることができるのは、写真間で共通する特徴を見つけ出して繋ぎあわせているからです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/91432815-aafb-45dc-0d01-738078f7fbbd.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/91432815-aafb-45dc-0d01-738078f7fbbd.png" alt="image"></a><br>
<a href="http://web.stanford.edu/class/cs231m/lectures/lecture-5-stitching-blending.pdf" rel="nofollow noopener" target="_blank">CS231M Mobile Computer Vision Class,Lecture5, Stitching + Blending, p4~</a></p>

<p>この「特徴点」をどんどんシンプルに考えていくと、最終的には以下3つに集約できます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/f34aff28-dd09-bdd3-d14b-05a4efe094dc.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/f34aff28-dd09-bdd3-d14b-05a4efe094dc.png" alt="image"></a></p>

<ul>
<li>edge: 差異が認識できる境界がある</li>
<li>corner: edgeが集中する点</li>
<li>flat: edgeでもcornerでもない、特徴が何も認識できない点</li>
</ul>

<p>そして、上記のようなパノラマ写真の合成などを行うことを考えると、この「特徴点」を見つける際には以下のルールにのっとる必要があります。</p>

<ul>
<li>再現性: ある特徴点は常に特徴点として認識される<br>
</li>
<li>識別性: ある特徴点は、その他の特徴点と明確に異なると識別できる</li>
</ul>

<p>再現性については、例えば写真をとる角度を変えた場合に認識される特徴点がガラッと変わる、となると特徴点同士をつなぎ合わせるという処理そのものが破たんしてしまいます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/6cf0a56d-1721-423e-42d4-0c7602309e0b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/6cf0a56d-1721-423e-42d4-0c7602309e0b.png" alt="image"></a></p>

<p>そのため、<strong>画像の角度や拡大率などで変化しない、頑健な特徴点</strong>を認識できる方がいいことになります。</p>

<p>識別性については、認識した特徴点が一意に識別できないとどれとどれを一致させていいかわからなくなってしまいます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/c54e09db-1e28-d39f-e1ce-8a13a3d4d2b4.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/c54e09db-1e28-d39f-e1ce-8a13a3d4d2b4.png" alt="image"></a></p>

<p>そのため、<strong>各特徴点が一意に識別できる表現方法</strong>が重要になります。</p>

<p>これはそのまま、特徴点の検出(Feature Detection)、特徴点の表現方法(Feature Description) 、それぞれに求められることと一致します。つまり、「画像の角度や拡大率などで変化しない、頑健な特徴点」を検出し(Feature Detection)、それをなるべく「一意に識別できる表現方法」で表現すること(Feature Description)が、画像における特徴点の認識の目標になります。</p>

<p>では、特徴点の検出、特徴点の表現方法を順番に見ていきます。</p>

<h1>
<span id="特徴点の検出feature-detection" class="fragment"></span><a href="#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E6%A4%9C%E5%87%BAfeature-detection"><i class="fa fa-link"></i></a>特徴点の検出(Feature Detection)</h1>

<p>特徴点を検出する手順としては、概ね「edgeを検出」し、次いでedgeが集中する「cornerを検出」する、という流れになります。</p>

<h2>
<span id="edgeの検出" class="fragment"></span><a href="#edge%E3%81%AE%E6%A4%9C%E5%87%BA"><i class="fa fa-link"></i></a>edgeの検出</h2>

<p>edgeとは、具体的には「輝度が大きく変化している点」になります。<br>
簡単な例として、真っ黒なタイルに白い円が描かれた画像を考えます。下図の通り真横から引いた赤い線上の輝度を座標軸に沿ってプロットすると、真ん中の白い円の中で輝度が跳ね上がっているようなグラフが得られるはずです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/94a92a6e-2cd1-8328-3513-db1bc28c9bb3.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/94a92a6e-2cd1-8328-3513-db1bc28c9bb3.png" alt="image"></a></p>

<p>ここで、私たちが検出したいのはedge、つまりグラフ上で暗い-&gt;明るい、明るい-&gt;暗いと大きく変化しているポイントになります。ある点における変化の度合いを知るために、その点を中心とした前後の点の値の値から変化率を計算してみます。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/011b4a04-3255-4720-972d-f08fd158fb2b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/011b4a04-3255-4720-972d-f08fd158fb2b.png" alt="image"></a></p>

<p>すると、上図のように変化が大きな点、つまりedgeに該当する箇所で変化率が上がっているグラフが得られます(変化量には+-がありますが、ここでは絶対値をプロットしていると思ってください)。今は図の水平方向で変化量を考えていますが、垂直方向の場合にも同様に考えられます。</p>

<p>最終的には、水平方向、垂直方向双方の変化量を合計し(通常は二乗和平方根)、その値があるしきい値(threshold)より大きい点を収集すればedgeの検出ができそうです。</p>

<p>これがedge検出の基本的な考え方で、一応これだけでもedgeの検出は可能ですが、より精度の高い検出を行うためのいくつかのテクニックがあるのでそれを見ていきます。</p>

<h3>
<span id="スムージング" class="fragment"></span><a href="#%E3%82%B9%E3%83%A0%E3%83%BC%E3%82%B8%E3%83%B3%E3%82%B0"><i class="fa fa-link"></i></a>スムージング</h3>

<p>変化量を計算する際に、周辺部分も考慮する方法です。ざっくり言ってしまえば、平均をとるようなイメージです。平均をとれば値の変化をならすことができ(スムージング)、この結果滑らかにつながるedgeを検出することができます。</p>

<p>以下は、水平方向の変化量について、隣接する点の変化量も加味して計算する様子を図で表したものです。計3つの変化量の和をとることで、スムージングが実現されています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/7775cc17-6fbb-2303-f79e-89933d96ea87.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/7775cc17-6fbb-2303-f79e-89933d96ea87.png" alt="image"></a></p>

<p>上図では変化量を計算するために3*3の、-1/0/1の値が入った行列を使って計算しています。こうした変化量を計算するための行列や処理を<strong>フィルタ</strong>と呼びます。</p>

<p>このフィルタには様々な種類があり、上図で使っていたのはこのうちのPrewittフィルタになります。SobelフィルタはPrewittより中心に隣接するものを重視したものになり、Gaussianは正規分布の関数を利用し、中心を頂点とし、端に向かってなだらかに係数をかけることができます。edgeの検出でよく利用されるCanny法は、このGaussianフィルタを使った方法になります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/90848fd5-cd79-4e58-5fb8-131320037fc9.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/90848fd5-cd79-4e58-5fb8-131320037fc9.png" alt="image"></a></p>

<p>なお、画像の端の部分については、隣接する点がないため補間を行う必要があります。この補間の方法には以下のような手法があります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/249e6c62-a0f9-503e-9331-fe4fc6d11f88.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/249e6c62-a0f9-503e-9331-fe4fc6d11f88.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture03.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture3, p26~</a></p>

<p>edgeの検出については、以上になります。ここまでの内容をまとめておきます。</p>

<ul>
<li>edgeを検出するには、画像における輝度の変化量を手掛かりにする</li>
<li>変化量は水平方向と垂直方向の2方向でとることができる。この値の二乗和平方根を変化量とし、これをMagnitudeと呼ぶ</li>
<li>Magnitudeが一定のしきい値を超えた場合にedgeと判定することで、edgeの検出が可能になる</li>
<li>変化量を計算する際には、一般的にはスムージングを行う。これは周辺の変化量を加味することで、検出精度を上げるためのものである</li>
<li>この「周辺」の対象範囲と、それに対する重みづけを定義するのがフィルタであり、様々な種類がある</li>
</ul>

<h2>
<span id="cornerの検出" class="fragment"></span><a href="#corner%E3%81%AE%E6%A4%9C%E5%87%BA"><i class="fa fa-link"></i></a>cornerの検出</h2>

<p>続いては、cornerの検出です。ここでは、cornerの検出によく利用されるHarris Corner Detectorについて説明していきます。これは行列の特性を非常にうまく利用した手法で、直観的には主成分分析に近いイメージになります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/051f6ac0-9859-b926-fda6-e051dbc70994.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/051f6ac0-9859-b926-fda6-e051dbc70994.png" alt="image"></a></p>

<p>主成分分析の細かい説明は他の記事に譲りますが、重要なポイントは以下2点です。</p>

<ul>
<li>データの広がる方向をよく説明できる指標を見つける。これは行列の固有ベクトルに相当する。</li>
<li>計算の結果得られる固有値は、その指標の説明能力を表す</li>
</ul>

<p>これをHarrisに当てはめると、「データ」は当然ある点における水平方向・垂直方向それぞれの変化量をまとめたものになります。そうすると、上記の主成分分析の説明から以下のように類推できます。</p>

<ul>
<li>固有ベクトルは「変化量の広がる方向」、つまりedgeの向きを表している</li>
<li>固有値が大きい場合は、「変化量の説明能力が高い」、つまりedgeの強さを表している</li>
</ul>

<p>これによりまずedgeの検出が可能になり、そして「固有値が大きい複数の固有ベクトル」が存在する場合、それは即ち複数のedgeがある、つまりcornerであるということになります。<br>
固有値をそれぞれ$\lambda1$、$\lambda2$とすると、その値の大きさにより以下のように分類が可能です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/6a6bfb47-3667-746b-1916-6eab27a87a75.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/6a6bfb47-3667-746b-1916-6eab27a87a75.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture06.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 06, Corner Detection, p19</a></p>

<p>数式の説明もしておきます。まず、画像$I$上のある点$I(x, y)$と、$x$方向に$u$、$y$方向に$v$移動した点$I(x+u, y+v)$との間の変化量を$E(u, v)$とすると、式で以下のように表せます。</p>

<p>$$<br>
E(u, v) = \sum_{x, y} w(x, y) [I(x+u, y+v) - I(x, y)]^2<br>
$$</p>

<p>$w(x, y)$はウィンドウ関数で、フィルタになります(Gaussian)。変化量が$[I(x+u, y+v) - I(x, y)]^2$で、これを$w(x, y)$でスムージングして変化量を算出する、というイメージです。<br>
この式をテイラー展開を使って近似すると以下のようになります(展開式は省略。興味ある方は、上図のリンクの講義資料をご参照ください)。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
E(u, v) \simeq [u, v] M \begin{bmatrix} u \\ v \end{bmatrix}
</pre></div></div>

<p>ここで、$M$は以下になります。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
M = \sum_{x, y} w(x, y) \begin{bmatrix} I_X^2 &amp; I_x I_y \\ I_x I_y &amp; I_y ^2 \end{bmatrix}
</pre></div></div>

<p>$I_x$、$I_y$はそれぞれx軸、y軸での差異で$[I_x, I_y]$を二乗すると上記の行列部分になります。やっていることは上記の式の$[I(x+u, y+v) - I(x, y)]^2$と変わりません。そして、これこそが「変化量を記述した行列」で、これを特異値分解することで冒頭で述べたようなedge、cornerの判定が可能になります。</p>

<p>ただ、固有値の計算は結構手間なので、必要ないところではなるべく計算しないようにします。そのために利用されるのが、以下の指標です。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
R = det M -k(trace M)^2
</pre></div></div>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
det M = \lambda1 \lambda2
</pre></div></div>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
trace M = \lambda1 + \lambda2
</pre></div></div>

<p>$k$は定数で、だいたい0.04~0.06くらいです。Rの値によって以下のように分類できます。</p>

<ul>
<li>Rが大きい: corner</li>
<li>Rが小さい: flat</li>
<li>R &lt; 0: edge</li>
</ul>

<p>図にすると、以下のようになります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/2f99ccc4-563c-bca7-5393-50e7ea0f0a45.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/2f99ccc4-563c-bca7-5393-50e7ea0f0a45.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture06.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 06, Corner Detection, p22</a></p>

<p>これで手早くcornerを検出できるようになりました。ここで、corner検出についてまとめておきます。</p>

<ul>
<li>cornerは複数のedgeが集まる箇所と定義できる</li>
<li>変化量をまとめた行列の固有ベクトルからedgeの向き、固有値の大きさから変化量の大きさ(edgeらしさ)がわかる</li>
<li>2つの固有値の値を基に、edge、corner、flatを判定できる</li>
<li>固有値の計算は手間であるため、判定式を利用し計算を簡略化する</li>
</ul>

<p>なお、Harrisはedgeの向きである固有ベクトルを考慮するため、画像の傾きなどに対し頑健です。ただ、スケール(拡大)については頑健ではありません。これは、拡大するにつれcornerが緩やかになり、edgeが区別しにくくなるためです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/973040e9-8fb2-72ba-13ef-4cf45a3b9eed.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/973040e9-8fb2-72ba-13ef-4cf45a3b9eed.png" alt="image"></a></p>

<p>これを克服するための編み出された手法がFASTになります。詳細は割愛しますが、中心点を基準としてそれより暗いor明るいn点の連なりを認識する方法です。これは文字通りFASTな手法でかつ回転・拡大に対し共に頑健です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/459f5ad2-934d-87d1-ce91-6b45b40f9c3d.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/459f5ad2-934d-87d1-ce91-6b45b40f9c3d.png" alt="image"></a><br>
<a href="http://web.stanford.edu/class/cs231m/lectures/lecture-5-stitching-blending.pdf" rel="nofollow noopener" target="_blank">CS231M Mobile Computer Vision Class,Lecture5, Stitching + Blending, p24</a></p>

<h1>
<span id="特徴点の表現方法feature-description" class="fragment"></span><a href="#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E8%A1%A8%E7%8F%BE%E6%96%B9%E6%B3%95feature-description"><i class="fa fa-link"></i></a>特徴点の表現方法(Feature Description)</h1>

<p>これで特徴点は検出できるようになりました。今度は、その特徴点を利用してなるべく一意な特徴表現、Feature Descriptionを作成することで画像のマッチングなどが行えるようにしたいです。</p>

<p>今までも少し触れましたが、このFeature Descriptionにとって望ましい特性は以下3点です。</p>

<ul>
<li>Translation: 画像のスライドに対して頑健</li>
<li>Rotation: 画像の回転に対して頑健</li>
<li>Scaling: 画像の拡大/縮小に対して頑健</li>
</ul>

<p>Translationは単に位置が変わるだけなので対応は割と簡単で、前項のHarrisでも上げた通りRotationも何とかなります。ただ、Scalingへの対応、これは難しいです。下図から明らかなとおり、画像内の情報は拡大率によって著しく変わってしまうためです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/6deb384f-5e7a-4ada-7321-1eba62098e92.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/6deb384f-5e7a-4ada-7321-1eba62098e92.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture31.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 31, Object Recognition : SIFT Keys, p31</a></p>

<p>そもそも、拡大をしたのならマッチングさせる範囲もそれに合わせて拡大しなければ、情報が落ちてしまうのは自明です。なぜなら同じ範囲でも収まる領域が変わってくるためです。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/8d40f069-982e-10c2-4b77-3c40e587bccf.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/8d40f069-982e-10c2-4b77-3c40e587bccf.png" alt="image"></a></p>

<p>つまり、上述の3点の特性を満たすにはスケールを考慮する必要があります。Harrisでは固有ベクトルと固有値から「向き」と「強さ」だけでしたが、ここに「スケール」、つまりどの拡大値で得られるものなのか、を加味するということです。イメージ的には以下のような感じになります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/c1a5513c-24fb-19b7-3506-01f1cada5653.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/c1a5513c-24fb-19b7-3506-01f1cada5653.png" alt="image"></a></p>

<p>そして、このスケールを加味した特徴点の検出・記述を可能にするのがSIFTという手法になります。</p>

<h2>
<span id="sift-scale-invariant-feature-transform" class="fragment"></span><a href="#sift-scale-invariant-feature-transform"><i class="fa fa-link"></i></a>SIFT (Scale Invariant Feature Transform)</h2>

<p>SIFTの肝になっているのが、スケールごとの特徴点抽出になっています。これに使われているのがLoG/DoGという手法です。LogはLaplacian of Gaussianの略で、Gaussianフィルタでスムージングした画像の中の、変化量が最大の点を2回微分(=Laplacian)で求めるというものです。<br>
Gaussianについてはスムージングで述べた通りなので、2次微分(=Laplacian)でなぜ変化量が最大の点が求められるのかについて簡単に説明します。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/95ff9f1c-15f4-bb5a-030b-7474f12df363.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/95ff9f1c-15f4-bb5a-030b-7474f12df363.png" alt="image"></a></p>

<p>微分とは、端的にはある点における変化量を求めるものだと思ってください。そうすると、以下のようなことがわかります。</p>

<ul>
<li>1次微分: 元の関数に対する変化量を表す=頂点が、変化量が最大(最小)の点を表す</li>
<li>2次微分: 変化量の変化を表す=1次微分の頂点付近では、変化の方向の切り替わりが発生する=1次微分の頂点は、軸と交わる点(zero-crossing point)と等しくなる</li>
</ul>

<p>つまり、$I(x)''=0$となる点が変化量が最大の点、特徴点として上げられるということです。この2次微分をかけるのに相当するフィルタを「ラプラシアン・フィルタ」と呼び、値が0に近いほど特徴点の可能性が高くなります。<br>
※ただ、2次微分だけでは、1次微分の段階で0(=変化なし)な点と頂点のものとが区別できないので、1次微分の値が十分に大きいかを同時に見る必要あります。このことからもわかる通り、ノイズにとても弱いです。</p>

<p>そして、LoGに戻るとこれはその名の通り、Gaussianフィルタでスムージングをしてから、ラプラシアンフィルタで変化量が最大の点=特徴点を求めるという手法になります。<br>
そして、LoGではGaussianフィルタを使うため、その$\sigma$(分散)を調整することができます。$\sigma$は大きいほどスムージングが強力にかかるため、変化量が大きい点が集まっているところしか検出できなくなります。これは逆もしかりで、感の良い方は気づいたかもしれませんが、これはちょうど拡大率を操作してみているのと同じ役割を果たしています。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/7e8cf0e3-45b6-c6c6-0c58-b8bd895a06cb.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/7e8cf0e3-45b6-c6c6-0c58-b8bd895a06cb.png" alt="image"></a></p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/97065b27-9568-db2f-b8bf-bd5c86390a8f.png" target="_blank" rel="nofollow noopener"><img width="826" alt="スクリーンショット 2016-02-04 13.37.57.png" src="https://qiita-image-store.s3.amazonaws.com/0/25990/97065b27-9568-db2f-b8bf-bd5c86390a8f.png"></a></p>

<p><a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture11.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 11, LoG Edge and Blob Finding, p19</a></p>

<p>SIFTでは、この$\sigma$を調整した複数の階層を用意して特徴点の検出を行います(スケールスペース)。以下は、その図になります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/76a61fc0-c366-87b8-9777-fc2beb4d2259.png" target="_blank" rel="nofollow noopener"><img width="829" alt="スクリーンショット 2016-02-04 13.45.53.png" src="https://qiita-image-store.s3.amazonaws.com/0/25990/76a61fc0-c366-87b8-9777-fc2beb4d2259.png"></a></p>

<p>ここでは、LoGの計算を簡略化するために、$\sigma$を変えた2つの層の差分で近似を行っています。これがDoG(Difference of Gaussian)になります。発見された特徴点は、スケールに対し頑健かを見るため前後のスケールと比較します(周辺8点と、前後のスケールにおける各9点、計26点と比較)。</p>

<p>これで、スケールに頑健な特徴点がわかるようになりました。後はこの特徴点における変化量の「強さ」と「向き」ですが、これは変化量とその角度から取得します。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/e7d1048c-ebb6-e455-1b6d-052e132e67c9.png" target="_blank" rel="nofollow noopener"><img width="380" alt="スクリーンショット 2016-02-04 13.30.42.png" src="https://qiita-image-store.s3.amazonaws.com/0/25990/e7d1048c-ebb6-e455-1b6d-052e132e67c9.png"></a></p>

<p>ここで、SIFTでは単一の特徴点だけでなく、それを中心とした16*16のマスを考慮します(この「マス」の大きさを<a href="http://www.vlfeat.org/api/sift.html" rel="nofollow noopener" target="_blank">bin sizeと呼びます</a>)。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/bb98c8d4-be5a-55dd-8655-6debe299d2f1.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/bb98c8d4-be5a-55dd-8655-6debe299d2f1.png" alt="image"></a><br>
<a href="http://www.kki.yamanashi.ac.jp/%7Eohbuchi/courses/2013/sm2013/pdf/sm13_lect03_20131028all.pdf" rel="nofollow noopener" target="_blank">意味的マルチメディア処理 第A2回, p4</a></p>

<p>それをさらに4x4の領域に区切っていき、そこで縦軸に変化量(変化量のMagnitude)、横軸に向き(=変化量の勾配の角度)を取ったヒストグラムを作成します。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/4fcf463a-b0ed-abe7-5538-2de1c76a388b.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/4fcf463a-b0ed-abe7-5538-2de1c76a388b.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture31.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 31, Object Recognition : SIFT Keys, p24</a></p>

<p>上図は0〜36(360度に対応)でまとめていますが、最終的には8方向にまとめます。そうすると、4*4の各窓が8方向の要素を持つベクトルが出来上がります。この全128次元のベクトルがSIFT Vectorであり、これは「スケール」に対し頑健であり、「強さ」と「向き」がわかるという、3つの特性を備える特徴点記述になります。</p>

<p>このSIFT Vectorを利用することで、非常に精度の高い特徴点のマッチングなどが行えるようになります。</p>

<h1>
<span id="特徴点検出方法の進化" class="fragment"></span><a href="#%E7%89%B9%E5%BE%B4%E7%82%B9%E6%A4%9C%E5%87%BA%E6%96%B9%E6%B3%95%E3%81%AE%E9%80%B2%E5%8C%96"><i class="fa fa-link"></i></a>特徴点検出方法の進化</h1>

<p>上記で挙げた特徴点の検出や記述の手法は年々進化しています。以下はSIFT登場前後までの手法の進化です。回転・スケールに対し頑健な特徴量を得ていく過程がわかるかと思います。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/cdeefdd5-f092-f48a-3dc8-b6c195814072.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/cdeefdd5-f092-f48a-3dc8-b6c195814072.png" alt="image"></a><br>
<a href="http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf" rel="nofollow noopener" target="_blank">MIRU2013チュートリアル：SIFTとそれ以降のアプローチ, p5</a></p>

<p>こちらはSIFT以降の進化になります。SIFTを高速化されたSURFという手法もよく使われます。なお、<strong>SIFTもSURFも特許が取られており、利用にあたっては特許料が発生します</strong>。そのため、ここまで説明しておいてなんですが、実際利用するにあたってはORBや、OpenCV 3.0で追加されたAKAZEなどが良いようです。何もSIFT/SURFより高速です。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/cc448669-3d09-10be-91bd-88e0b639392c.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/cc448669-3d09-10be-91bd-88e0b639392c.png" alt="image"></a><br>
<a href="http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf" rel="nofollow noopener" target="_blank">MIRU2013チュートリアル：SIFTとそれ以降のアプローチ, p93</a></p>

<p>今回ご紹介した基礎的な手法を理解しておけば、今後新しい手法が登場したときも、何が改良されてきたのかがわかり易くなると思います。</p>

<h1>
<span id="特徴点のマッチング" class="fragment"></span><a href="#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0"><i class="fa fa-link"></i></a>特徴点のマッチング</h1>

<p>検出した特徴点は、様々な用途に使えます。代表的な例として、画像のマッチングに使用する方法を紹介したいと思います。</p>

<p>まず、画像のマッチングに際しては比較対象とする特徴点の周辺領域を切り出し(この切り出したものをパッチといいます)、それがもう片方の画像にもあるか調べる、というのが基本的な流れになります。このパッチをどう使うかについて、以下の2種類があります。</p>

<ul>
<li>Templateベース: もう片方の画像にパッチと一致するものがないか確認する</li>
<li>Featureベース: パッチから特徴を抽出し、その特徴と一致するものがないか確認する</li>
</ul>

<p>イメージ的には下図のようになります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/ab6fe875-7516-6d0d-49a2-b23ccf51fdc6.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/ab6fe875-7516-6d0d-49a2-b23ccf51fdc6.png" alt="image"></a></p>

<p>いずれにせよ、比較を行う際には類似度(similarity)を測るための関数が必要になります。※画像の全範囲について類似度を計算していると大変なので、探索範囲を絞る方法も必要になってきますが、ここでは触れません。</p>

<p>類似度を測る代表的な指標としては、以下があります。</p>

<ul>
<li>SSD (sum of squared differences): 差の二乗和</li>
<li>NCC (normalized cross correlation): 正規化した相互相関</li>
</ul>

<p><strong>SSD</strong>は一番単純な指標で、TemplateやFeature同士の差を見て0に近ければ類似しているとする方法です。どれくらいの距離であればよいか、つまり閾値については、最も一致する特徴点との比率が用いられることが多いです。<br>
<strong>NCC</strong>、というか相互相関は類似性を示す尺度で、内積を計算することで得られます。例えば、全く関係ない(=互いに独立している)ベクトル同士だと直行しているはずで、直行しているベクトルの内積は0になります。逆に0以外であれば正・あるいは負の相関があることになります。そして、正規化(平均0・分散1)した上で相互相関を取ったものが正規化相互相関です。</p>

<p>一応式も書いておきます</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
SSD(I_1, I_2) = \sum_{[x, y] \in R} (I_1(x, y) - I_2(x, y))^2 \\
C(I_1, I_2) = \sum_{[x, y] \in R} I_1(x, y) I_2(x, y) \\
NCC(I_1, I_2) = \frac{1}{n - 1}\sum_{[x, y] \in R} \frac{(I_1(x, y) - \mu_1)}{\sigma_1} \cdot \frac{(I_2(x, y) - \mu_2)}{\sigma_2}
</pre></div></div>

<p>NCCは、内積を取るという性質上Templateでフィルタをかけているような処理になります。以下は実際に処理してみた例で、Templateに一致する箇所が反応しているのが分かります。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/57206334-2b6b-a8a9-081d-7085a8c64956.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/57206334-2b6b-a8a9-081d-7085a8c64956.png" alt="image"></a><br>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/lecture07.pdf" rel="nofollow noopener" target="_blank">CSE/EE486 Computer Vision I, Lecture 7, Template Matching, p7</a></p>

<p>より厳密なマッチングを行いたい場合は、A-&gt;BだけでなくB-&gt;Aからもマッチングを行い双方で一致していると判断されたもののみ使うという方法もあります。<br>
この特徴点のマッチングを使用することで、パノラマのような写真を作ったり画像の分類や検索などを行うことが可能になります。</p>

<h1>
<span id="opencvでの実装" class="fragment"></span><a href="#opencv%E3%81%A7%E3%81%AE%E5%AE%9F%E8%A3%85"><i class="fa fa-link"></i></a>OpenCVでの実装</h1>

<p>最後に、OpenCVでの実装方法を紹介しておきます。以下のリポジトリに、Jupyter notebookで書いたサンプルをおいています。</p>

<p><a href="https://github.com/icoxfog417/cv_tutorial_feature" rel="nofollow noopener" target="_blank">icoxfog417/cv_tutorial_feature</a></p>

<p>なおOpenCVのインストールですが、Windowsの方はAnaconda(conda)がお勧めです。numpyやmatplotlibはもちろんですが、OpenCV本体も以下から一発でインストール可能です。</p>

<p><a href="https://anaconda.org/menpo/opencv3" rel="nofollow noopener" target="_blank">menpo/Packages/opencv3</a></p>

<p>実装に当たっては、以下のOpenCV公式のチュートリアルを参考にしました。</p>

<ul>
<li><a href="http://docs.opencv.org/master/dc/d0d/tutorial_py_features_harris.html#gsc.tab=0" rel="nofollow noopener" target="_blank">Harris Corner Detection</a></li>
<li><a href="http://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html#gsc.tab=0" rel="nofollow noopener" target="_blank">SIFT</a></li>
<li><a href="http://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html#gsc.tab=0" rel="nofollow noopener" target="_blank">Feature Matching</a></li>
</ul>

<p>書くコード自体はほんの数行ですが、パラメーターの調整を行わないとうまく検出できないことが多く(特にSIFT)、そしてパラメーターの調整にはやはり理論的な理解が必要です。そういう意味では、本当の意味でOpenCVを使いこなすには理論的な理解は不可欠だと思います。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/25990/9de898de-0c21-26bd-6fdf-9f9500c0869e.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/25990/9de898de-0c21-26bd-6fdf-9f9500c0869e.png" alt="image"></a></p>

<p>今回ご紹介した解説が、石化を解き真の画像処理マスターとなるための手助けとなれば幸いです。</p>

<h1>
<span id="参考文献" class="fragment"></span><a href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><i class="fa fa-link"></i></a>参考文献</h1>

<ul>
<li>
<a href="https://www.oreilly.co.jp/books/9784873116075/" rel="nofollow noopener" target="_blank">実践 コンピュータビジョン</a><br>
説明は正直かなりぶっきらぼうな感じで、初学者がこれ単体で理解するのは難しいと思う。</li>
<li>
<a href="http://www.cse.psu.edu/%7Ertc12/CSE486/" rel="nofollow noopener" target="_blank">Computer Vision I</a><br>
ペンシルベニア州立大学のComputer Visionのコース。とても丁寧でわかりやすい</li>
<li>
<a href="http://web.stanford.edu/class/cs231m/syllabus.html" rel="nofollow noopener" target="_blank">Mobile Computer Vision</a><br>
StanfordのモバイルでComputerVisionを行うコース。IntroductionからしばらくはAndroidの開発の仕方などの説明で、Stitching + BlendingからComputer Visionの説明。モバイルに組み込みたい人には特におすすめ。</li>
<li>
<a href="https://t.co/mnlQPR84Xi" rel="nofollow noopener" target="_blank">OpenCV公式</a><br>
かなりわかりやすくまとまっており、かつOpenCVベースのコードサンプルがある。おすすめ。</li>
<li>
<a href="http://apple.ee.uec.ac.jp/COMPROG/" rel="nofollow noopener" target="_blank">電気通信大学大学院 情報理工学研究科 の資料</a><br>
第9回から画像処理の話。図解が豊富でわかりやすい。最も基礎となる画像の微分の基礎を理解するならここ。</li>
<li>
<a href="http://www.kki.yamanashi.ac.jp/%7Eohbuchi/courses/2013/sm2013/pdf/sm13_lect03_20131028all.pdf" rel="nofollow noopener" target="_blank">意味的マルチメディア処理 第A2回（2013年10月15日）</a><br>
SIFTについて理解するならこちら。非常にわかり易い</li>
<li>
<a href="http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf" rel="nofollow noopener" target="_blank">MIRU2013チュートリアル：SIFTとそれ以降のアプローチ</a><br>
SIFTの解説中心だが、後半(92～)に手法の体系的・歴史的な背景が整理されている</li>
<li>
<a href="http://www.vlfeat.org/api/index.html" rel="nofollow noopener" target="_blank">VLFeat Documentation</a><br>
Cなどで使える画像の特徴点抽出のライブラリのドキュメント。英語だがかなりわかりやすい。</li>
<li><a href="https://www.researchgate.net/post/Can_any_one_help_me_understand_Deeply_SIFT" rel="nofollow noopener" target="_blank">Can any one help me understand Deeply SIFT ?</a></li>
<li><a href="http://postd.cc/a-beginners-guide-to-eigenvectors-pca-covariance-and-entropy/" rel="nofollow noopener" target="_blank">固有ベクトル、主成分分析、共分散、エントロピー入門</a></li>
</ul>
<div class="hidden"><form class="js-task-list-update" action="/icoxfog417/items/adbbf445d357c924b8fc" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="Tg7/qeW6Ec0D1FPKTAhsGy3P+T7qUQfpmZfo/17zxDsgFPyI+D08+Ny2GqJtCOD5Xt1AlInzwxj4n2O1mpwiUw==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1458176525" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
画像処理は難しい。
Instagramのキレイなフィルタ、GoogleのPhoto Sphere、そうしたサービスを見て画像は面白そうだ！と心躍らせて開いた画像処理の本。そこに山と羅列される数式を前に石化せざるを得なかった俺たちが、耳にささやかれる「難しいことはOpenCVがやってくれるわ。そうでしょ？」という声に身をゆだねる以外に何ができただろう。

本稿は石化せざるを得なかったあの頃を克服し、OpenCVを使いながらも基礎的な理論を理解したいと願う方へ、その道筋(アイテム的には金の針)を示すものになればと思います。
扱う範囲としては、あらゆる処理の基礎となる「画像の特徴点検出」を対象とします([実践 コンピュータビジョン](https://www.oreilly.co.jp/books/9784873116075/)の2章に相当)。なお、本記事自体、初心者である私が理解しながら書いているため、上級画像処理冒険者の方は誤りなどあれば指摘していただければ幸いです。

# 画像の特徴点とは

人間がジグソーパズルを組み立てられるのはなぜか？と考えると、私たちはパズルの各ピースの特徴を把握して、それと似た、また連続する特徴を見つけ出して繋ぎあわせているのだ、と考えることができます。同様に、複数の写真をつなげてパノラマ写真にすることができるのは、写真間で共通する特徴を見つけ出して繋ぎあわせているからです。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/91432815-aafb-45dc-0d01-738078f7fbbd.png)
[CS231M Mobile Computer Vision Class,Lecture5, Stitching + Blending, p4~](http://web.stanford.edu/class/cs231m/lectures/lecture-5-stitching-blending.pdf)

この「特徴点」をどんどんシンプルに考えていくと、最終的には以下3つに集約できます。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/f34aff28-dd09-bdd3-d14b-05a4efe094dc.png)

* edge: 差異が認識できる境界がある
* corner: edgeが集中する点
* flat: edgeでもcornerでもない、特徴が何も認識できない点

そして、上記のようなパノラマ写真の合成などを行うことを考えると、この「特徴点」を見つける際には以下のルールにのっとる必要があります。

* 再現性: ある特徴点は常に特徴点として認識される  
* 識別性: ある特徴点は、その他の特徴点と明確に異なると識別できる

再現性については、例えば写真をとる角度を変えた場合に認識される特徴点がガラッと変わる、となると特徴点同士をつなぎ合わせるという処理そのものが破たんしてしまいます。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/6cf0a56d-1721-423e-42d4-0c7602309e0b.png)

そのため、**画像の角度や拡大率などで変化しない、頑健な特徴点**を認識できる方がいいことになります。


識別性については、認識した特徴点が一意に識別できないとどれとどれを一致させていいかわからなくなってしまいます。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/c54e09db-1e28-d39f-e1ce-8a13a3d4d2b4.png)

そのため、**各特徴点が一意に識別できる表現方法**が重要になります。


これはそのまま、特徴点の検出(Feature Detection)、特徴点の表現方法(Feature Description) 、それぞれに求められることと一致します。つまり、「画像の角度や拡大率などで変化しない、頑健な特徴点」を検出し(Feature Detection)、それをなるべく「一意に識別できる表現方法」で表現すること(Feature Description)が、画像における特徴点の認識の目標になります。

では、特徴点の検出、特徴点の表現方法を順番に見ていきます。

# 特徴点の検出(Feature Detection)

特徴点を検出する手順としては、概ね「edgeを検出」し、次いでedgeが集中する「cornerを検出」する、という流れになります。

## edgeの検出

edgeとは、具体的には「輝度が大きく変化している点」になります。
簡単な例として、真っ黒なタイルに白い円が描かれた画像を考えます。下図の通り真横から引いた赤い線上の輝度を座標軸に沿ってプロットすると、真ん中の白い円の中で輝度が跳ね上がっているようなグラフが得られるはずです。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/94a92a6e-2cd1-8328-3513-db1bc28c9bb3.png)

ここで、私たちが検出したいのはedge、つまりグラフ上で暗い-&gt;明るい、明るい-&gt;暗いと大きく変化しているポイントになります。ある点における変化の度合いを知るために、その点を中心とした前後の点の値の値から変化率を計算してみます。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/011b4a04-3255-4720-972d-f08fd158fb2b.png)

すると、上図のように変化が大きな点、つまりedgeに該当する箇所で変化率が上がっているグラフが得られます(変化量には+-がありますが、ここでは絶対値をプロットしていると思ってください)。今は図の水平方向で変化量を考えていますが、垂直方向の場合にも同様に考えられます。

最終的には、水平方向、垂直方向双方の変化量を合計し(通常は二乗和平方根)、その値があるしきい値(threshold)より大きい点を収集すればedgeの検出ができそうです。

これがedge検出の基本的な考え方で、一応これだけでもedgeの検出は可能ですが、より精度の高い検出を行うためのいくつかのテクニックがあるのでそれを見ていきます。

### スムージング

変化量を計算する際に、周辺部分も考慮する方法です。ざっくり言ってしまえば、平均をとるようなイメージです。平均をとれば値の変化をならすことができ(スムージング)、この結果滑らかにつながるedgeを検出することができます。

以下は、水平方向の変化量について、隣接する点の変化量も加味して計算する様子を図で表したものです。計3つの変化量の和をとることで、スムージングが実現されています。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/7775cc17-6fbb-2303-f79e-89933d96ea87.png)

上図では変化量を計算するために3*3の、-1/0/1の値が入った行列を使って計算しています。こうした変化量を計算するための行列や処理を**フィルタ**と呼びます。

このフィルタには様々な種類があり、上図で使っていたのはこのうちのPrewittフィルタになります。SobelフィルタはPrewittより中心に隣接するものを重視したものになり、Gaussianは正規分布の関数を利用し、中心を頂点とし、端に向かってなだらかに係数をかけることができます。edgeの検出でよく利用されるCanny法は、このGaussianフィルタを使った方法になります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/90848fd5-cd79-4e58-5fb8-131320037fc9.png)

なお、画像の端の部分については、隣接する点がないため補間を行う必要があります。この補間の方法には以下のような手法があります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/249e6c62-a0f9-503e-9331-fe4fc6d11f88.png)
[CSE/EE486 Computer Vision I, Lecture3, p26~](http://www.cse.psu.edu/~rtc12/CSE486/lecture03.pdf)


edgeの検出については、以上になります。ここまでの内容をまとめておきます。

* edgeを検出するには、画像における輝度の変化量を手掛かりにする
* 変化量は水平方向と垂直方向の2方向でとることができる。この値の二乗和平方根を変化量とし、これをMagnitudeと呼ぶ
* Magnitudeが一定のしきい値を超えた場合にedgeと判定することで、edgeの検出が可能になる
* 変化量を計算する際には、一般的にはスムージングを行う。これは周辺の変化量を加味することで、検出精度を上げるためのものである
* この「周辺」の対象範囲と、それに対する重みづけを定義するのがフィルタであり、様々な種類がある

## cornerの検出

続いては、cornerの検出です。ここでは、cornerの検出によく利用されるHarris Corner Detectorについて説明していきます。これは行列の特性を非常にうまく利用した手法で、直観的には主成分分析に近いイメージになります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/051f6ac0-9859-b926-fda6-e051dbc70994.png)

主成分分析の細かい説明は他の記事に譲りますが、重要なポイントは以下2点です。

* データの広がる方向をよく説明できる指標を見つける。これは行列の固有ベクトルに相当する。
* 計算の結果得られる固有値は、その指標の説明能力を表す

これをHarrisに当てはめると、「データ」は当然ある点における水平方向・垂直方向それぞれの変化量をまとめたものになります。そうすると、上記の主成分分析の説明から以下のように類推できます。

* 固有ベクトルは「変化量の広がる方向」、つまりedgeの向きを表している
* 固有値が大きい場合は、「変化量の説明能力が高い」、つまりedgeの強さを表している

これによりまずedgeの検出が可能になり、そして「固有値が大きい複数の固有ベクトル」が存在する場合、それは即ち複数のedgeがある、つまりcornerであるということになります。
固有値をそれぞれ$\lambda1$、$\lambda2$とすると、その値の大きさにより以下のように分類が可能です。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/6a6bfb47-3667-746b-1916-6eab27a87a75.png)
[CSE/EE486 Computer Vision I, Lecture 06, Corner Detection, p19](http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf)

数式の説明もしておきます。まず、画像$I$上のある点$I(x, y)$と、$x$方向に$u$、$y$方向に$v$移動した点$I(x+u, y+v)$との間の変化量を$E(u, v)$とすると、式で以下のように表せます。

$$
E(u, v) = \sum_{x, y} w(x, y) [I(x+u, y+v) - I(x, y)]^2
$$

$w(x, y)$はウィンドウ関数で、フィルタになります(Gaussian)。変化量が$[I(x+u, y+v) - I(x, y)]^2$で、これを$w(x, y)$でスムージングして変化量を算出する、というイメージです。
この式をテイラー展開を使って近似すると以下のようになります(展開式は省略。興味ある方は、上図のリンクの講義資料をご参照ください)。

```math
E(u, v) \simeq [u, v] M \begin{bmatrix} u \\ v \end{bmatrix}
```

ここで、$M$は以下になります。

```math
M = \sum_{x, y} w(x, y) \begin{bmatrix} I_X^2 &amp; I_x I_y \\ I_x I_y &amp; I_y ^2 \end{bmatrix}
```

$I_x$、$I_y$はそれぞれx軸、y軸での差異で$[I_x, I_y]$を二乗すると上記の行列部分になります。やっていることは上記の式の$[I(x+u, y+v) - I(x, y)]^2$と変わりません。そして、これこそが「変化量を記述した行列」で、これを特異値分解することで冒頭で述べたようなedge、cornerの判定が可能になります。

ただ、固有値の計算は結構手間なので、必要ないところではなるべく計算しないようにします。そのために利用されるのが、以下の指標です。

```math
R = det M -k(trace M)^2
```

```math
det M = \lambda1 \lambda2
```

```math
trace M = \lambda1 + \lambda2
```

$k$は定数で、だいたい0.04~0.06くらいです。Rの値によって以下のように分類できます。

* Rが大きい: corner
* Rが小さい: flat
* R &lt; 0: edge

図にすると、以下のようになります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/2f99ccc4-563c-bca7-5393-50e7ea0f0a45.png)
[CSE/EE486 Computer Vision I, Lecture 06, Corner Detection, p22](http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf)

これで手早くcornerを検出できるようになりました。ここで、corner検出についてまとめておきます。

* cornerは複数のedgeが集まる箇所と定義できる
* 変化量をまとめた行列の固有ベクトルからedgeの向き、固有値の大きさから変化量の大きさ(edgeらしさ)がわかる
* 2つの固有値の値を基に、edge、corner、flatを判定できる
* 固有値の計算は手間であるため、判定式を利用し計算を簡略化する

なお、Harrisはedgeの向きである固有ベクトルを考慮するため、画像の傾きなどに対し頑健です。ただ、スケール(拡大)については頑健ではありません。これは、拡大するにつれcornerが緩やかになり、edgeが区別しにくくなるためです。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/973040e9-8fb2-72ba-13ef-4cf45a3b9eed.png)

これを克服するための編み出された手法がFASTになります。詳細は割愛しますが、中心点を基準としてそれより暗いor明るいn点の連なりを認識する方法です。これは文字通りFASTな手法でかつ回転・拡大に対し共に頑健です。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/459f5ad2-934d-87d1-ce91-6b45b40f9c3d.png)
[CS231M Mobile Computer Vision Class,Lecture5, Stitching + Blending, p24](http://web.stanford.edu/class/cs231m/lectures/lecture-5-stitching-blending.pdf)

# 特徴点の表現方法(Feature Description) 

これで特徴点は検出できるようになりました。今度は、その特徴点を利用してなるべく一意な特徴表現、Feature Descriptionを作成することで画像のマッチングなどが行えるようにしたいです。

今までも少し触れましたが、このFeature Descriptionにとって望ましい特性は以下3点です。

* Translation: 画像のスライドに対して頑健
* Rotation: 画像の回転に対して頑健
* Scaling: 画像の拡大/縮小に対して頑健

Translationは単に位置が変わるだけなので対応は割と簡単で、前項のHarrisでも上げた通りRotationも何とかなります。ただ、Scalingへの対応、これは難しいです。下図から明らかなとおり、画像内の情報は拡大率によって著しく変わってしまうためです。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/6deb384f-5e7a-4ada-7321-1eba62098e92.png)
[CSE/EE486 Computer Vision I, Lecture 31, Object Recognition : SIFT Keys, p31](http://www.cse.psu.edu/~rtc12/CSE486/lecture31.pdf)

そもそも、拡大をしたのならマッチングさせる範囲もそれに合わせて拡大しなければ、情報が落ちてしまうのは自明です。なぜなら同じ範囲でも収まる領域が変わってくるためです。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/8d40f069-982e-10c2-4b77-3c40e587bccf.png)

つまり、上述の3点の特性を満たすにはスケールを考慮する必要があります。Harrisでは固有ベクトルと固有値から「向き」と「強さ」だけでしたが、ここに「スケール」、つまりどの拡大値で得られるものなのか、を加味するということです。イメージ的には以下のような感じになります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/c1a5513c-24fb-19b7-3506-01f1cada5653.png)

そして、このスケールを加味した特徴点の検出・記述を可能にするのがSIFTという手法になります。

## SIFT (Scale Invariant Feature Transform)

SIFTの肝になっているのが、スケールごとの特徴点抽出になっています。これに使われているのがLoG/DoGという手法です。LogはLaplacian of Gaussianの略で、Gaussianフィルタでスムージングした画像の中の、変化量が最大の点を2回微分(=Laplacian)で求めるというものです。
Gaussianについてはスムージングで述べた通りなので、2次微分(=Laplacian)でなぜ変化量が最大の点が求められるのかについて簡単に説明します。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/95ff9f1c-15f4-bb5a-030b-7474f12df363.png)

微分とは、端的にはある点における変化量を求めるものだと思ってください。そうすると、以下のようなことがわかります。

* 1次微分: 元の関数に対する変化量を表す=頂点が、変化量が最大(最小)の点を表す
* 2次微分: 変化量の変化を表す=1次微分の頂点付近では、変化の方向の切り替わりが発生する=1次微分の頂点は、軸と交わる点(zero-crossing point)と等しくなる

つまり、$I(x)&#39;&#39;=0$となる点が変化量が最大の点、特徴点として上げられるということです。この2次微分をかけるのに相当するフィルタを「ラプラシアン・フィルタ」と呼び、値が0に近いほど特徴点の可能性が高くなります。
※ただ、2次微分だけでは、1次微分の段階で0(=変化なし)な点と頂点のものとが区別できないので、1次微分の値が十分に大きいかを同時に見る必要あります。このことからもわかる通り、ノイズにとても弱いです。

そして、LoGに戻るとこれはその名の通り、Gaussianフィルタでスムージングをしてから、ラプラシアンフィルタで変化量が最大の点=特徴点を求めるという手法になります。
そして、LoGではGaussianフィルタを使うため、その$\sigma$(分散)を調整することができます。$\sigma$は大きいほどスムージングが強力にかかるため、変化量が大きい点が集まっているところしか検出できなくなります。これは逆もしかりで、感の良い方は気づいたかもしれませんが、これはちょうど拡大率を操作してみているのと同じ役割を果たしています。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/7e8cf0e3-45b6-c6c6-0c58-b8bd895a06cb.png)

&lt;img width=&quot;826&quot; alt=&quot;スクリーンショット 2016-02-04 13.37.57.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/97065b27-9568-db2f-b8bf-bd5c86390a8f.png&quot;&gt;

[CSE/EE486 Computer Vision I, Lecture 11, LoG Edge and Blob Finding, p19](http://www.cse.psu.edu/~rtc12/CSE486/lecture11.pdf)

SIFTでは、この$\sigma$を調整した複数の階層を用意して特徴点の検出を行います(スケールスペース)。以下は、その図になります。

&lt;img width=&quot;829&quot; alt=&quot;スクリーンショット 2016-02-04 13.45.53.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/76a61fc0-c366-87b8-9777-fc2beb4d2259.png&quot;&gt;

ここでは、LoGの計算を簡略化するために、$\sigma$を変えた2つの層の差分で近似を行っています。これがDoG(Difference of Gaussian)になります。発見された特徴点は、スケールに対し頑健かを見るため前後のスケールと比較します(周辺8点と、前後のスケールにおける各9点、計26点と比較)。

これで、スケールに頑健な特徴点がわかるようになりました。後はこの特徴点における変化量の「強さ」と「向き」ですが、これは変化量とその角度から取得します。

&lt;img width=&quot;380&quot; alt=&quot;スクリーンショット 2016-02-04 13.30.42.png&quot; src=&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/e7d1048c-ebb6-e455-1b6d-052e132e67c9.png&quot;&gt;

ここで、SIFTでは単一の特徴点だけでなく、それを中心とした16*16のマスを考慮します(この「マス」の大きさを[bin sizeと呼びます](http://www.vlfeat.org/api/sift.html))。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/bb98c8d4-be5a-55dd-8655-6debe299d2f1.png)
[意味的マルチメディア処理 第A2回, p4](http://www.kki.yamanashi.ac.jp/~ohbuchi/courses/2013/sm2013/pdf/sm13_lect03_20131028all.pdf)

それをさらに4x4の領域に区切っていき、そこで縦軸に変化量(変化量のMagnitude)、横軸に向き(=変化量の勾配の角度)を取ったヒストグラムを作成します。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/4fcf463a-b0ed-abe7-5538-2de1c76a388b.png)
[CSE/EE486 Computer Vision I, Lecture 31, Object Recognition : SIFT Keys, p24](http://www.cse.psu.edu/~rtc12/CSE486/lecture31.pdf)

上図は0〜36(360度に対応)でまとめていますが、最終的には8方向にまとめます。そうすると、4*4の各窓が8方向の要素を持つベクトルが出来上がります。この全128次元のベクトルがSIFT Vectorであり、これは「スケール」に対し頑健であり、「強さ」と「向き」がわかるという、3つの特性を備える特徴点記述になります。

このSIFT Vectorを利用することで、非常に精度の高い特徴点のマッチングなどが行えるようになります。

# 特徴点検出方法の進化

上記で挙げた特徴点の検出や記述の手法は年々進化しています。以下はSIFT登場前後までの手法の進化です。回転・スケールに対し頑健な特徴量を得ていく過程がわかるかと思います。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/cdeefdd5-f092-f48a-3dc8-b6c195814072.png)
[MIRU2013チュートリアル：SIFTとそれ以降のアプローチ, p5](http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf)

こちらはSIFT以降の進化になります。SIFTを高速化されたSURFという手法もよく使われます。なお、**SIFTもSURFも特許が取られており、利用にあたっては特許料が発生します**。そのため、ここまで説明しておいてなんですが、実際利用するにあたってはORBや、OpenCV 3.0で追加されたAKAZEなどが良いようです。何もSIFT/SURFより高速です。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/cc448669-3d09-10be-91bd-88e0b639392c.png)
[MIRU2013チュートリアル：SIFTとそれ以降のアプローチ, p93](http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf)

今回ご紹介した基礎的な手法を理解しておけば、今後新しい手法が登場したときも、何が改良されてきたのかがわかり易くなると思います。

# 特徴点のマッチング

検出した特徴点は、様々な用途に使えます。代表的な例として、画像のマッチングに使用する方法を紹介したいと思います。

まず、画像のマッチングに際しては比較対象とする特徴点の周辺領域を切り出し(この切り出したものをパッチといいます)、それがもう片方の画像にもあるか調べる、というのが基本的な流れになります。このパッチをどう使うかについて、以下の2種類があります。

* Templateベース: もう片方の画像にパッチと一致するものがないか確認する
* Featureベース: パッチから特徴を抽出し、その特徴と一致するものがないか確認する

イメージ的には下図のようになります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/ab6fe875-7516-6d0d-49a2-b23ccf51fdc6.png)

いずれにせよ、比較を行う際には類似度(similarity)を測るための関数が必要になります。※画像の全範囲について類似度を計算していると大変なので、探索範囲を絞る方法も必要になってきますが、ここでは触れません。

類似度を測る代表的な指標としては、以下があります。

* SSD (sum of squared differences): 差の二乗和
* NCC (normalized cross correlation): 正規化した相互相関

**SSD**は一番単純な指標で、TemplateやFeature同士の差を見て0に近ければ類似しているとする方法です。どれくらいの距離であればよいか、つまり閾値については、最も一致する特徴点との比率が用いられることが多いです。
**NCC**、というか相互相関は類似性を示す尺度で、内積を計算することで得られます。例えば、全く関係ない(=互いに独立している)ベクトル同士だと直行しているはずで、直行しているベクトルの内積は0になります。逆に0以外であれば正・あるいは負の相関があることになります。そして、正規化(平均0・分散1)した上で相互相関を取ったものが正規化相互相関です。

一応式も書いておきます

```math
SSD(I_1, I_2) = \sum_{[x, y] \in R} (I_1(x, y) - I_2(x, y))^2 \\
C(I_1, I_2) = \sum_{[x, y] \in R} I_1(x, y) I_2(x, y) \\
NCC(I_1, I_2) = \frac{1}{n - 1}\sum_{[x, y] \in R} \frac{(I_1(x, y) - \mu_1)}{\sigma_1} \cdot \frac{(I_2(x, y) - \mu_2)}{\sigma_2}
```
NCCは、内積を取るという性質上Templateでフィルタをかけているような処理になります。以下は実際に処理してみた例で、Templateに一致する箇所が反応しているのが分かります。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/57206334-2b6b-a8a9-081d-7085a8c64956.png)
[CSE/EE486 Computer Vision I, Lecture 7, Template Matching, p7](http://www.cse.psu.edu/~rtc12/CSE486/lecture07.pdf)

より厳密なマッチングを行いたい場合は、A-&gt;BだけでなくB-&gt;Aからもマッチングを行い双方で一致していると判断されたもののみ使うという方法もあります。
この特徴点のマッチングを使用することで、パノラマのような写真を作ったり画像の分類や検索などを行うことが可能になります。

# OpenCVでの実装

最後に、OpenCVでの実装方法を紹介しておきます。以下のリポジトリに、Jupyter notebookで書いたサンプルをおいています。

[icoxfog417/cv_tutorial_feature](https://github.com/icoxfog417/cv_tutorial_feature)

なおOpenCVのインストールですが、Windowsの方はAnaconda(conda)がお勧めです。numpyやmatplotlibはもちろんですが、OpenCV本体も以下から一発でインストール可能です。

[menpo/Packages/opencv3](https://anaconda.org/menpo/opencv3)

実装に当たっては、以下のOpenCV公式のチュートリアルを参考にしました。

* [Harris Corner Detection](http://docs.opencv.org/master/dc/d0d/tutorial_py_features_harris.html#gsc.tab=0)
* [SIFT](http://docs.opencv.org/master/da/df5/tutorial_py_sift_intro.html#gsc.tab=0)
* [Feature Matching](http://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html#gsc.tab=0)

書くコード自体はほんの数行ですが、パラメーターの調整を行わないとうまく検出できないことが多く(特にSIFT)、そしてパラメーターの調整にはやはり理論的な理解が必要です。そういう意味では、本当の意味でOpenCVを使いこなすには理論的な理解は不可欠だと思います。

![image](https://qiita-image-store.s3.amazonaws.com/0/25990/9de898de-0c21-26bd-6fdf-9f9500c0869e.png)

今回ご紹介した解説が、石化を解き真の画像処理マスターとなるための手助けとなれば幸いです。


# 参考文献

* [実践 コンピュータビジョン](https://www.oreilly.co.jp/books/9784873116075/)  
 説明は正直かなりぶっきらぼうな感じで、初学者がこれ単体で理解するのは難しいと思う。
* [Computer Vision I](http://www.cse.psu.edu/~rtc12/CSE486/)  
ペンシルベニア州立大学のComputer Visionのコース。とても丁寧でわかりやすい
* [Mobile Computer Vision](http://web.stanford.edu/class/cs231m/syllabus.html)  
StanfordのモバイルでComputerVisionを行うコース。IntroductionからしばらくはAndroidの開発の仕方などの説明で、Stitching + BlendingからComputer Visionの説明。モバイルに組み込みたい人には特におすすめ。
* [OpenCV公式](https://t.co/mnlQPR84Xi)  
 かなりわかりやすくまとまっており、かつOpenCVベースのコードサンプルがある。おすすめ。
* [電気通信大学大学院 情報理工学研究科 の資料](http://apple.ee.uec.ac.jp/COMPROG/)  
第9回から画像処理の話。図解が豊富でわかりやすい。最も基礎となる画像の微分の基礎を理解するならここ。
* [意味的マルチメディア処理 第A2回（2013年10月15日）](http://www.kki.yamanashi.ac.jp/~ohbuchi/courses/2013/sm2013/pdf/sm13_lect03_20131028all.pdf)  
SIFTについて理解するならこちら。非常にわかり易い
* [MIRU2013チュートリアル：SIFTとそれ以降のアプローチ](http://www.vision.cs.chubu.ac.jp/cvtutorial/PDF/02SIFTandMore.pdf)  
SIFTの解説中心だが、後半(92～)に手法の体系的・歴史的な背景が整理されている
* [VLFeat Documentation](http://www.vlfeat.org/api/index.html)  
Cなどで使える画像の特徴点抽出のライブラリのドキュメント。英語だがかなりわかりやすい。
* [Can any one help me understand Deeply SIFT ?](https://www.researchgate.net/post/Can_any_one_help_me_understand_Deeply_SIFT)
* [固有ベクトル、主成分分析、共分散、エントロピー入門](http://postd.cc/a-beginners-guide-to-eigenvectors-pca-covariance-and-entropy/)
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="画像処理の数式を見て石になった時のための、金の針 by @icoxfog417 on @Qiita" data-url="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="画像処理の数式を見て石になった時のための、金の針" href="http://b.hatena.ne.jp/entry/http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/icoxfog417"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/icoxfog417">icoxfog417</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">20387</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;icoxfog417&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-54d21b3c-b218-4d43-ad60-5812b1264d49"></div>
    <div id="UserFollowButton-react-component-54d21b3c-b218-4d43-ad60-5812b1264d49"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/icoxfog417/items/e8f97a6acad07903b5b0">Pythonを書き始める前に見るべきTips</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/icoxfog417/items/adbbf445d357c924b8fc">画像処理の数式を見て石になった時のための、金の針</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/icoxfog417/items/242439ecd1a477ece312">ゼロからDeepまで学ぶ強化学習</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/icoxfog417/items/65e800c3a2094457c3a0">はじめるDeep learning</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/icoxfog417/items/5d79b3336226aa51e30d">React.js 実戦投入への道</a></li></ul></section><section class="itemsShowAuthorInfo_organization"><h5 class="itemsShowAuthorInfo_organizationTitle">ORGANIZATION</h5><span itemprop="memberOf" itemscope="" itemtype="http://schema.org/Organization"><a itemprop="url" href="/organizations/tis"><img alt="TIS株式会社" class="itemsShowAuthorInfo_organizationLogo" itemprop="image" src="https://s3-ap-northeast-1.amazonaws.com/qiita-organization-image/5710e4c30854dd4ab3658e7f585930ab0d81a12c/original.jpg?1484790468" /></a></span></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%94%BB%E5%83%8F%E3%81%AE%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%A8%E3%81%AF\&quot;\u003e画像の特徴点とは\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E6%A4%9C%E5%87%BAfeature-detection\&quot;\u003e特徴点の検出(Feature Detection)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#edge%E3%81%AE%E6%A4%9C%E5%87%BA\&quot;\u003eedgeの検出\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%82%B9%E3%83%A0%E3%83%BC%E3%82%B8%E3%83%B3%E3%82%B0\&quot;\u003eスムージング\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#corner%E3%81%AE%E6%A4%9C%E5%87%BA\&quot;\u003ecornerの検出\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E8%A1%A8%E7%8F%BE%E6%96%B9%E6%B3%95feature-description\&quot;\u003e特徴点の表現方法(Feature Description)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#sift-scale-invariant-feature-transform\&quot;\u003eSIFT (Scale Invariant Feature Transform)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%89%B9%E5%BE%B4%E7%82%B9%E6%A4%9C%E5%87%BA%E6%96%B9%E6%B3%95%E3%81%AE%E9%80%B2%E5%8C%96\&quot;\u003e特徴点検出方法の進化\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%89%B9%E5%BE%B4%E7%82%B9%E3%81%AE%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0\&quot;\u003e特徴点のマッチング\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#opencv%E3%81%A7%E3%81%AE%E5%AE%9F%E8%A3%85\&quot;\u003eOpenCVでの実装\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\&quot;\u003e参考文献\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-aad119e4-5dec-4bda-9c96-aa3ed4961c32"></div>
    <div id="Toc-react-component-aad119e4-5dec-4bda-9c96-aa3ed4961c32"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:1870,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="fumi23"><a itemprop="url" href="/fumi23"><img alt="fumi23" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/110423/profile-images/1473711205" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Hironsan"><a itemprop="url" href="/Hironsan"><img alt="Hironsan" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/77079/profile-images/1473700709" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ksomemo"><a itemprop="url" href="/ksomemo"><img alt="ksomemo" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/6982/profile-images/1473684010" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="Yatima"><a itemprop="url" href="/Yatima"><img alt="Yatima" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/17440/profile-images/1473682159" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="eve_yk"><a itemprop="url" href="/eve_yk"><img alt="eve_yk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="shiraco"><a itemprop="url" href="/shiraco"><img alt="shiraco" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/5707/profile-images/1473682202" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ueno3"><a itemprop="url" href="/ueno3"><img alt="ueno3" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/75359/profile-images/1473700135" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="paralleltree"><a itemprop="url" href="/paralleltree"><img alt="paralleltree" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/55804/profile-images/1473693796" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hnakamur"><a itemprop="url" href="/hnakamur"><img alt="hnakamur" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/3227/profile-images/1473682812" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="tomohisaota"><a itemprop="url" href="/tomohisaota"><img alt="tomohisaota" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/33513/profile-images/1473686144" /></a></div></div><div class="ArticleFooter__user"><a href="/icoxfog417/items/adbbf445d357c924b8fc/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/adbbf445d357c924b8fc/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/icoxfog417/items/adbbf445d357c924b8fc.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><li class="references_reference js-reference"><span>Linked from </span><a href="/GushiSnow/items/1b855f94bd43001e85e3#_reference-84369268b556e01cb955"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/10496/profile-images/1473757289" />結婚式で余興を頼まれた時の画像処理をPythonでやってみる</a><time class="references_datetime js-dateTimeView" datetime="2016-02-09T03:25:14+00:00">about 1 year ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/howdy39/items/a1aef86fef1ce1b6d778#_reference-d580569449ff5321813e"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/35595/profile-images/1473686778" />Googleクラウド自然言語APIを使ってみた</a><time class="references_datetime js-dateTimeView" datetime="2016-07-21T13:27:43+00:00">8 months ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="画像処理の数式を見て石になった時のための、金の針 by @icoxfog417 on @Qiita" data-url="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="画像処理の数式を見て石になった時のための、金の針" href="http://b.hatena.ne.jp/entry/http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eとてもいい記事でした。ありがとうございます。\u003cbr\u003e\nひとつ質問ですが、「cornerの検出」の項でプロットしているのは何ですか?\u003cbr\u003e\nいきなりベクトルが出てきたけれど、これって一体何のベクトルだろうと、そこが理解できませんでした。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-28T22:43:33+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:601861,&quot;is_team&quot;:false,&quot;item_id&quot;:365504,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;とてもいい記事でした。ありがとうございます。\nひとつ質問ですが、「cornerの検出」の項でプロットしているのは何ですか?\nいきなりベクトルが出てきたけれど、これって一体何のベクトルだろうと、そこが理解できませんでした。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc#comment-948664cfb81ebd287dae&quot;,&quot;user&quot;:{&quot;contribution&quot;:1,&quot;created_at&quot;:&quot;2016-06-05T22:08:10+09:00&quot;,&quot;id&quot;:128476,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/128476/profile-images/1473717230&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;arene&quot;},&quot;uuid&quot;:&quot;948664cfb81ebd287dae&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eHarris Corner Detectionの方でプロットされているのは、文中にもあるとおり「ある点(周辺)における水平方向・垂直方向それぞれの変化量の分布」になります。この変化量の「方向性」を示すのが、固有ベクトルになります。\u003cbr\u003e\n左の主成分分析の方は、似ていることを示すためのものなので特に何をプロットしているということはないです。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-08-03T12:45:12+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:605854,&quot;is_team&quot;:false,&quot;item_id&quot;:365504,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;Harris Corner Detectionの方でプロットされているのは、文中にもあるとおり「ある点(周辺)における水平方向・垂直方向それぞれの変化量の分布」になります。この変化量の「方向性」を示すのが、固有ベクトルになります。\n左の主成分分析の方は、似ていることを示すためのものなので特に何をプロットしているということはないです。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc#comment-4e61244ebaa0745eaee7&quot;,&quot;user&quot;:{&quot;contribution&quot;:20387,&quot;created_at&quot;:&quot;2013-06-19T22:28:11+09:00&quot;,&quot;id&quot;:25990,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;icoxfog417&quot;},&quot;uuid&quot;:&quot;4e61244ebaa0745eaee7&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eご回答ありがとうございます。\u003cbr\u003e\nおかげさまで理解できたと思います。\u003c/p\u003e\n\n\u003cp\u003e注目画素と、その周辺画素に関して水平(x)方向、垂直(y)方向の輝度値変化量をプロットし、そのデータの広がり方を見てコーナーかどうかを判定するのがHarris Corner Ditectionであり、\u003cbr\u003e\nデータの広がり方(方向性と量＝エッジの方向と強さ)を分析するための道具が固有ベクトル。分析指標が本文中のR……と理解しました。\u003c/p\u003e\n\n\u003cp\u003eとても勉強になる記事をありがとうございます。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-08-15T19:55:17+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:613546,&quot;is_team&quot;:false,&quot;item_id&quot;:365504,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;ご回答ありがとうございます。\nおかげさまで理解できたと思います。\n\n注目画素と、その周辺画素に関して水平(x)方向、垂直(y)方向の輝度値変化量をプロットし、そのデータの広がり方を見てコーナーかどうかを判定するのがHarris Corner Ditectionであり、\nデータの広がり方(方向性と量＝エッジの方向と強さ)を分析するための道具が固有ベクトル。分析指標が本文中のR……と理解しました。\n\nとても勉強になる記事をありがとうございます。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc#comment-5064fc60400b7dcbabb4&quot;,&quot;user&quot;:{&quot;contribution&quot;:1,&quot;created_at&quot;:&quot;2016-06-05T22:08:10+09:00&quot;,&quot;id&quot;:128476,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/128476/profile-images/1473717230&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;arene&quot;},&quot;uuid&quot;:&quot;5064fc60400b7dcbabb4&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:365504,&quot;uuid&quot;:&quot;adbbf445d357c924b8fc&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;icoxfog417&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:25990,&quot;url_name&quot;:&quot;icoxfog417&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516&quot;},{&quot;id&quot;:128476,&quot;url_name&quot;:&quot;arene&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/128476/profile-images/1473717230&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-b7cb275f-7222-47e1-9bdd-c53bfa3d4ded"></div>
    <div id="CommentListContainer-react-component-b7cb275f-7222-47e1-9bdd-c53bfa3d4ded"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="JPr76d+L0TM04JRJh/2mUvPXgN6R+9QOrQRTugYHi2lK4PjIwgz8BuuC3SGm/SqwgMU5dPJZEP/MDNjwwmhtAQ==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/icoxfog417/items/adbbf445d357c924b8fc" /><input type="hidden" name="item_uuid" id="item_uuid" value="adbbf445d357c924b8fc" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/icoxfog417/items/adbbf445d357c924b8fc", "id": 365504, "uuid": "adbbf445d357c924b8fc" }</script><script class="js-user" type="application/json">{&quot;id&quot;:25990,&quot;url_name&quot;:&quot;icoxfog417&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="Du0MJbvGJQkvpHLSYngAHekwRFPAJPeyohARSw/HBdRg9w8EpkEIPPDGO7pDeIz/miL9+aOGM0PDGJoBy6jjvA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/icoxfog417/items/adbbf445d357c924b8fc" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>