<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>Autogradという野郎が乗り込んできたのでガクブルな件 - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="Autogradという野郎が乗り込んできました。はい、そりゃもういきなり。複雑な確率モデルや損失関数だとしても、パラメータに関する勾配をこれでもかというぐらい簡単に計算できちゃうので、機械学習の世界に大きな影響を与えそうです。現時点では、PythonとTorchでの実装が公開されているようですが、これからJuliaなど他の言語でも実装されていきそうですね。

（補足：この記事を書いたすぐ後にGoogleがTensorFlowなるものを出してきまして、そちらでも自動微分が..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="Autogradという野郎が乗り込んできたのでガクブルな件 - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="
Autogradという野郎が乗り込んできました。はい、そりゃもういきなり。複雑な確率モデルや損失関数だとしても、パラメータに関する勾配をこれでもかというぐらい簡単に計算できちゃうので、機械学習の世界に大きな影響を与えそうです。現時点..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-81dd63f4385f99b52aeab91266068ebd.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="PqkdnTJ9nbGfRuEH8FosFvLWVnWKpT/0qBmeJwF0rRApj9aKs/ZO42Pzc+bj8nZdj3V1H9jAY7mFVbvRMQotsA==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"tanemaki","type":"items","id":"8dfb3c81a3af6feebdfe"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;News&quot;,&quot;content&quot;:&quot;ストックの他に「いいね」が追加されました&quot;,&quot;url&quot;:&quot;http://blog.qiita.com/post/153200849029/qiita-like-button&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-c56c3ce2-2bb5-427d-9302-e7ca341ab774"></div>
    <div id="HeaderContainer-react-component-c56c3ce2-2bb5-427d-9302-e7ca341ab774"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92",        "name": "機械学習"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">Autogradという野郎が乗り込んできたのでガクブルな件</h1><ul class="TagList"><li class="TagList__item" data-count="1835"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="814"><a class="u-link-unstyled TagList__label" href="/tags/MachineLearning"><img alt="MachineLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/b85c97772bddbfbb48a8b116669349c7ec92e4bf/medium.jpg?1395227038" /><span>MachineLearning</span></a></li><li class="TagList__item" data-count="9910"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">487</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="1 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>1</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:487,&quot;uuid&quot;:&quot;8dfb3c81a3af6feebdfe&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></li><li class="js-hovercard" data-hovercard-target-name="godhand00@github"><a itemprop="url" href="/godhand00@github"><img alt="godhand00@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/22915/profile-images/1473683681" /></a></li><li class="js-hovercard" data-hovercard-target-name="heliac2000"><a itemprop="url" href="/heliac2000"><img alt="heliac2000" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/44994/profile-images/1473690106" /></a></li><li class="js-hovercard" data-hovercard-target-name="sharow"><a itemprop="url" href="/sharow"><img alt="sharow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/11641/profile-images/1473682092" /></a></li><li class="js-hovercard" data-hovercard-target-name="kochory"><a itemprop="url" href="/kochory"><img alt="kochory" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/68258/profile-images/1473697785" /></a></li><li class="js-hovercard" data-hovercard-target-name="shimo_t"><a itemprop="url" href="/shimo_t"><img alt="shimo_t" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64100/profile-images/1473696475" /></a></li><li class="js-hovercard" data-hovercard-target-name="hogefugabar"><a itemprop="url" href="/hogefugabar"><img alt="hogefugabar" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31899/profile-images/1473685791" /></a></li><li class="js-hovercard" data-hovercard-target-name="atsaki"><a itemprop="url" href="/atsaki"><img alt="atsaki" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/15114/profile-images/1473683668" /></a></li><li class="js-hovercard" data-hovercard-target-name="josenaka"><a itemprop="url" href="/josenaka"><img alt="josenaka" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/79562/profile-images/1480749853" /></a></li><li class="js-hovercard" data-hovercard-target-name="EqualL2"><a itemprop="url" href="/EqualL2"><img alt="EqualL2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/71548/profile-images/1473698862" /></a></li><li><a href="/tanemaki/items/8dfb3c81a3af6feebdfe/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/tanemaki"><img class="thumb thumb--xs" src="https://secure.gravatar.com/avatar/19040888fb3dc4487f2ed3beff8dccd6?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png" alt="Assets.github.com%2fimages%2fgravatars%2fgravatar user 420" /></a> <a class="u-link-unstyled" href="/tanemaki">tanemaki</a> </div><div class="ArticleAsideHeader__date"><meta content="2015-11-07T16:29:03+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2015-11-07">Edited at <time datetime="2015-11-10T15:50:57+09:00" itemprop="dateModified">2015-11-10</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/tanemaki/items/8dfb3c81a3af6feebdfe/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">7</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/tanemaki/items/8dfb3c81a3af6feebdfe/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(7)</span></a></li><li><a href="/tanemaki/items/8dfb3c81a3af6feebdfe.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-8dfb3c81a3af6feebdfe" itemprop="articleBody"><div class="alert alert-warning"><i class="fa fa-clock-o"></i> More than 1 year has passed since last update.</div><p>Autogradという野郎が乗り込んできました。はい、そりゃもういきなり。複雑な確率モデルや損失関数だとしても、パラメータに関する勾配をこれでもかというぐらい簡単に計算できちゃうので、機械学習の世界に大きな影響を与えそうです。現時点では、<a href="https://github.com/HIPS/autograd" rel="nofollow noopener" target="_blank">Python</a>と<a href="https://github.com/twitter/torch-autograd/" rel="nofollow noopener" target="_blank">Torch</a>での実装が公開されているようですが、これからJuliaなど他の言語でも実装されていきそうですね。</p>

<p>（補足：この記事を書いたすぐ後にGoogleが<a href="http://tensorflow.org/" rel="nofollow noopener" target="_blank">TensorFlow</a>なるものを出してきまして、そちらでも自動微分がしっかり実装されてるみたいです〜。機械学習関連のフレームワークは移り変わりが激しいですねー ^^; ）</p>

<p>ちなみに始まりはこんな感じでした。</p>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/16310/3eedd716-9f82-9082-0c1d-4a82b48d9912.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/16310/3eedd716-9f82-9082-0c1d-4a82b48d9912.png" alt="スクリーンショット 2015-11-07 12.06.06.png" title="スクリーンショット 2015-11-07 12.06.06.png"></a></p>

<p>ゆるいですね。</p>

<p>とりあえずチュートリアルやりながら、Python版チュートリアルの前半部分にテキトーな日本語訳をつけたので、ここでシェアしておきます。英語が読める方は、僕のヘンテコな日本語訳を読むより、英語の原文（<a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#autograd-tutorial" rel="nofollow noopener" target="_blank">Autograd tutorial</a>）を読むことをオススメします。</p>

<p>以下、Python版チュートリアルの前半部分のテキトーな日本語翻訳です。</p>

<h2>
<span id="モチベーション" class="fragment"></span><a href="#%E3%83%A2%E3%83%81%E3%83%99%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3"><i class="fa fa-link"></i></a>モチベーション</h2>

<p>手元にあるデータを使って、ある確率モデルのトレーニングしてみたい場面を想像してみましょう。この場合、まずは確率モデルとそのモデルの良さ（確率モデルがどれだけデータをよく表しているか）を測るための損失関数を用意しますね。次に、このモデルと損失関数を基に、モデルのパラメータを最適化します。いわゆる学習ですね。簡単なモデルであればバッチ最適化手法を用いて一発で最適なパタメータが求められたりしますが、ニューラルネットワークのようにパラメータ数が多く複雑なモデルだと、逐次最適化手法に頼らざるを得なくなり、勾配を計算する必要がでてきます。</p>

<p>こうなると現時点では２つの方法が残ります。</p>

<ul>
<li>１つ目の方法は、紙とペンで勾配を計算し、出てきた答えをガリガリコーディングするというやり方です。</li>
<li>２つ目の方法は、Theanoのようなライブラリの構文に従って確率モデルをコーディングするというやり方です。</li>
</ul>

<p>Autogradはこれに対し、３つ目の方法を提供します。</p>

<ul>
<li>３つ目の方法は、損失関数をnumpyのような数値計算ライブラリで実装してAutogradに渡すというやり方です。</li>
</ul>

<p>後はAutogradが自動微分（Automatic Differentiation）のリバースモード（いわゆるbackpropagation）を使って自動的に勾配を求めます。</p>

<h2>
<span id="autogradの使い方" class="fragment"></span><a href="#autograd%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9"><i class="fa fa-link"></i></a>Autogradの使い方</h2>

<p>Autogradが提供する<code>grad</code>関数に損失関数を渡すだけです。そうすると、損失関数の勾配を計算する関数が返ってきます。</p>

<p>ちなみに<code>grad</code>関数に渡す関数はスカラ関数（返り値が<code>float</code>などのスカラ値）である必要があります。</p>

<p>Autogradは、Pythonの<code>while</code>ループや<code>if</code>文やクロージャなどの制御構文やnumpyのデータ型に対して働きかけ最終的な勾配関数を導きだすので、<code>grad</code>関数に渡す損失関数はPythonとnumpyを使ってかなり自由に記述できます。</p>

<h2>
<span id="簡単な例" class="fragment"></span><a href="#%E7%B0%A1%E5%8D%98%E3%81%AA%E4%BE%8B"><i class="fa fa-link"></i></a>簡単な例</h2>

<p>まず<code>autograd</code>というパッケージを<code>pip isntall autograd</code>でインストールし、必要なモジュールを読み込みます。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="kn">as</span> <span class="nn">np</span> <span class="c"># Numpy用の薄いラッパ</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>
</pre></div></div>

<p>関数（機械学習したければ損失関数。以下、「損失関数」という言葉を使用）を定義します。<code>sin(x)</code>を定義する際に<code>for</code>ループをぶん回している点に注目して下さい。（コメント欄に以下のコードの意味のざっくりとした解説を乗せておきましたので、コードがチンプンカンプンの場合はどうぞー。）</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="k">def</span> <span class="nf">taylor_sine</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c"># サイン関数のテイラー近似（マクローリン展開）</span>
    <span class="n">ans</span> <span class="o">=</span> <span class="n">currterm</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">currterm</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.001</span><span class="p">:</span>
        <span class="n">currterm</span> <span class="o">=</span> <span class="o">-</span><span class="n">currterm</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">ans</span> <span class="o">=</span> <span class="n">ans</span> <span class="o">+</span> <span class="n">currterm</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">ans</span>
</pre></div></div>

<p>描画用ライブラリを読み込んで、</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div></div>

<p>まずは損失関数自体を描画します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c"># 損失関数</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">taylor_sine</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]);</span>
</pre></div></div>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/16310/98adac4c-c453-4e42-b05b-bcfb40cfe7c2.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/16310/98adac4c-c453-4e42-b05b-bcfb40cfe7c2.png" alt="w+DXqDw73Wj9gAAAABJRU5ErkJggg==.png" title="w+DXqDw73Wj9gAAAABJRU5ErkJggg==.png"></a></p>

<p>ここでおもむろに、<code>grad</code>関数に損失関数を渡します。すると勾配関数が帰ってくるので受け取ります。ただこれだけです。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c"># grad関数に損失関数を渡して、勾配関数を受け取る</span>
<span class="n">grad_sine</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">taylor_sine</span><span class="p">)</span>
</pre></div></div>

<p>描画してみると、$\frac{d\sin(x)}{dx} = \cos(x)$がちゃんと返ってきていることがわかります。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c">#  パラメータの各点における勾配を計算して描画</span>
<span class="n">dydxs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">grad_sine</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">dydxs</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]);</span>
</pre></div></div>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/16310/19d62b77-7089-65bf-66ee-9f7421b3fe0e.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/16310/19d62b77-7089-65bf-66ee-9f7421b3fe0e.png" alt="oyyWv03mRiAAAAAElFTkSuQmCC.png" title="oyyWv03mRiAAAAAElFTkSuQmCC.png"></a></p>

<h2>
<span id="実際の学習に用いた例ロジスティック回帰" class="fragment"></span><a href="#%E5%AE%9F%E9%9A%9B%E3%81%AE%E5%AD%A6%E7%BF%92%E3%81%AB%E7%94%A8%E3%81%84%E3%81%9F%E4%BE%8B%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0"><i class="fa fa-link"></i></a>実際の学習に用いた例：ロジスティック回帰</h2>

<p>次に、Autogradで求めた勾配を使って、実際に学習してみます。モデルにはロジスティック回帰モデルを使ってみます。</p>

<p>まず先ほどと同様、<code>autograd</code>から必要なモジュールを読みこみ、確率モデルと損失関数を定義します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="kn">import</span> <span class="nn">autograd.numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">autograd</span> <span class="kn">import</span> <span class="n">grad</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># typo in the original</span>

<span class="k">def</span> <span class="nf">logistic_predictions</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="c"># ロジスティック回帰モデルで予測されたy=1となる確率。</span>
    <span class="c"># バイアス項は無し。</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>    

<span class="k">def</span> <span class="nf">training_loss</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
    <span class="c"># トレーニング用損失関数には負の対数尤度（negative log-likelihood）を用いる</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">logistic_predictions</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">label_probabilities</span> <span class="o">=</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">label_probabilities</span><span class="p">))</span>
</pre></div></div>

<p>次に、データセットを用意します。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c"># 小さなデータセットを作成</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">1.12</span><span class="p">,</span>  <span class="mf">0.77</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.88</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.30</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.74</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.49</span><span class="p">,</span> <span class="mf">1.39</span><span class="p">]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="bp">True</span><span class="p">,</span> <span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">])</span>
</pre></div></div>

<p>ここまで来たら、おもむろに<code>grad</code>関数を呼び出し、勾配関数をゲットします。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c"># autogradを用いて、トレーニング用損失関数のモデルパラメータに対する勾配を求める</span>
<span class="n">training_gradient_fun</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">training_loss</span><span class="p">)</span>
</pre></div></div>

<p>勾配を計算できるので、勾配法を用いて逐次的にパラメータをアップデートしていくと、損失関数の値が下がります。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="c"># モデルパラメータを勾配法を用いて逐次最適化</span>

<span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">])</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">training_loss</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
<span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"Initial loss:"</span><span class="p">,</span> <span class="n">loss</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">weights</span> <span class="o">-=</span> <span class="n">training_gradient_fun</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">training_loss</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
    <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"Trained loss:"</span><span class="p">,</span> <span class="n">loss</span>    
<span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</pre></div></div>

<p>Initial loss: 2.77258872224<br>
Trained loss: 1.06727067579</p>

<p>途中経過の値をプロットするとこんな感じです。しっかり学習できてますね。</p>

<div class="code-frame" data-lang="python"><div class="highlight"><pre>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Training loss"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Iterations"</span><span class="p">)</span>
</pre></div></div>

<p><a href="https://qiita-image-store.s3.amazonaws.com/0/16310/0c75765c-78af-d016-5612-bdcebe8a1452.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/16310/0c75765c-78af-d016-5612-bdcebe8a1452.png" alt="download.png" title="download.png"></a></p>

<p>Pythonを使うと確率モデルを簡単に記述できます。Autogradによってもたらされる最大の恩恵は、モデルの変更しても面倒な手順を踏まずにサクッと学習が始められる点です。</p>

<p><a href="https://github.com/HIPS/autograd/tree/master/examples" rel="nofollow noopener" target="_blank">examples</a>ディレクトリには以下の例を含むいろいろなサンプルコードがあります。</p>

<ul>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/neural_net.py" rel="nofollow noopener" target="_blank">Simple neural net</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/convnet.py" rel="nofollow noopener" target="_blank">Convolutional neural net</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/rnn.py" rel="nofollow noopener" target="_blank">Recurrent neural net</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/lstm.py" rel="nofollow noopener" target="_blank">Long short-term memory (LSTM)</a></li>
<li><a href="https://github.com/DoctorTeeth/diffmem/blob/512aadeefd6dbafc1bdd253a64b6be192a435dc3/ntm/ntm.py" rel="nofollow noopener" target="_blank">Neural Turing Machine</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/fluidsim/fluidsim.py" rel="nofollow noopener" target="_blank">Backpropagating through a fluid simulation</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/bayesian_neural_net.py" rel="nofollow noopener" target="_blank">Variational inference in Bayesian neural network</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/examples/gaussian_process.py" rel="nofollow noopener" target="_blank">Gaussian process regression</a></li>
</ul>

<p>以上、Autogradの紹介でした。内部的にどうやってやっているのかという点については、気が向いたら書くかもですー。それでは、また〜。</p>

<h2>
<span id="リンク集" class="fragment"></span><a href="#%E3%83%AA%E3%83%B3%E3%82%AF%E9%9B%86"><i class="fa fa-link"></i></a>リンク集</h2>

<p>Python系実装</p>

<ul>
<li><a href="https://github.com/HIPS/autograd" rel="nofollow noopener" target="_blank">Autograd for Python @ GitHub</a></li>
<li><a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#autograd-tutorial" rel="nofollow noopener" target="_blank">チュートリアル</a></li>
<li><a href="https://github.com/HIPS/autograd/tree/master/examples" rel="nofollow noopener" target="_blank">用例集</a></li>
</ul>

<p>Torch系実装</p>

<ul>
<li><a href="https://github.com/twitter/torch-autograd/" rel="nofollow noopener" target="_blank">Autograd for Torch @ GitHub</a></li>
<li><a href="https://blog.twitter.com/2015/autograd-for-torch" rel="nofollow noopener" target="_blank">チュートリアル</a></li>
</ul>
<div class="hidden"><form class="js-task-list-update" action="/tanemaki/items/8dfb3c81a3af6feebdfe" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="hjYCpkR8rqbnC0ZgEW/QCri8LZgwEVBjGr4Hx3jmjJyREMmxxfd99Bu+1IECx4pBxR8O8mJ0DC438iIxSJgMPA==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1447138257" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">

Autogradという野郎が乗り込んできました。はい、そりゃもういきなり。複雑な確率モデルや損失関数だとしても、パラメータに関する勾配をこれでもかというぐらい簡単に計算できちゃうので、機械学習の世界に大きな影響を与えそうです。現時点では、[Python](https://github.com/HIPS/autograd)と[Torch](https://github.com/twitter/torch-autograd/)での実装が公開されているようですが、これからJuliaなど他の言語でも実装されていきそうですね。

（補足：この記事を書いたすぐ後にGoogleが[TensorFlow](http://tensorflow.org/)なるものを出してきまして、そちらでも自動微分がしっかり実装されてるみたいです〜。機械学習関連のフレームワークは移り変わりが激しいですねー ^^; ）

ちなみに始まりはこんな感じでした。

![スクリーンショット 2015-11-07 12.06.06.png](https://qiita-image-store.s3.amazonaws.com/0/16310/3eedd716-9f82-9082-0c1d-4a82b48d9912.png &quot;スクリーンショット 2015-11-07 12.06.06.png&quot;)

ゆるいですね。

とりあえずチュートリアルやりながら、Python版チュートリアルの前半部分にテキトーな日本語訳をつけたので、ここでシェアしておきます。英語が読める方は、僕のヘンテコな日本語訳を読むより、英語の原文（[Autograd tutorial](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#autograd-tutorial)）を読むことをオススメします。

以下、Python版チュートリアルの前半部分のテキトーな日本語翻訳です。

## モチベーション

手元にあるデータを使って、ある確率モデルのトレーニングしてみたい場面を想像してみましょう。この場合、まずは確率モデルとそのモデルの良さ（確率モデルがどれだけデータをよく表しているか）を測るための損失関数を用意しますね。次に、このモデルと損失関数を基に、モデルのパラメータを最適化します。いわゆる学習ですね。簡単なモデルであればバッチ最適化手法を用いて一発で最適なパタメータが求められたりしますが、ニューラルネットワークのようにパラメータ数が多く複雑なモデルだと、逐次最適化手法に頼らざるを得なくなり、勾配を計算する必要がでてきます。

こうなると現時点では２つの方法が残ります。

- １つ目の方法は、紙とペンで勾配を計算し、出てきた答えをガリガリコーディングするというやり方です。
- ２つ目の方法は、Theanoのようなライブラリの構文に従って確率モデルをコーディングするというやり方です。

Autogradはこれに対し、３つ目の方法を提供します。

- ３つ目の方法は、損失関数をnumpyのような数値計算ライブラリで実装してAutogradに渡すというやり方です。

後はAutogradが自動微分（Automatic Differentiation）のリバースモード（いわゆるbackpropagation）を使って自動的に勾配を求めます。

## Autogradの使い方

Autogradが提供する`grad`関数に損失関数を渡すだけです。そうすると、損失関数の勾配を計算する関数が返ってきます。

ちなみに`grad`関数に渡す関数はスカラ関数（返り値が`float`などのスカラ値）である必要があります。

Autogradは、Pythonの`while`ループや`if`文やクロージャなどの制御構文やnumpyのデータ型に対して働きかけ最終的な勾配関数を導きだすので、`grad`関数に渡す損失関数はPythonとnumpyを使ってかなり自由に記述できます。

## 簡単な例

まず`autograd`というパッケージを`pip isntall autograd`でインストールし、必要なモジュールを読み込みます。

```python:
import autograd.numpy as np # Numpy用の薄いラッパ
from autograd import grad
```

関数（機械学習したければ損失関数。以下、「損失関数」という言葉を使用）を定義します。`sin(x)`を定義する際に`for`ループをぶん回している点に注目して下さい。（コメント欄に以下のコードの意味のざっくりとした解説を乗せておきましたので、コードがチンプンカンプンの場合はどうぞー。）

```python:
def taylor_sine(x):   # サイン関数のテイラー近似（マクローリン展開）
    ans = currterm = x
    i = 0
    while np.abs(currterm) &gt; 0.001:
        currterm = -currterm * x**2 / ((2 * i + 3) * (2 * i + 2))
        ans = ans + currterm
        i += 1
    return ans
```

描画用ライブラリを読み込んで、

```python:
import matplotlib.pyplot as plt
%matplotlib inline
```

まずは損失関数自体を描画します。

```python:
# 損失関数
xs = np.arange(0, 2 * np.pi, 0.01)
ys = np.asarray([taylor_sine(x) for x in xs])
plt.plot(xs, ys);
plt.ylim([-1.0, 1.0]);
```
![w+DXqDw73Wj9gAAAABJRU5ErkJggg==.png](https://qiita-image-store.s3.amazonaws.com/0/16310/98adac4c-c453-4e42-b05b-bcfb40cfe7c2.png &quot;w+DXqDw73Wj9gAAAABJRU5ErkJggg==.png&quot;)

ここでおもむろに、`grad`関数に損失関数を渡します。すると勾配関数が帰ってくるので受け取ります。ただこれだけです。

```python:
# grad関数に損失関数を渡して、勾配関数を受け取る
grad_sine = grad(taylor_sine)
```

描画してみると、$\frac{d\sin(x)}{dx} = \cos(x)$がちゃんと返ってきていることがわかります。

```python:
#  パラメータの各点における勾配を計算して描画
dydxs = np.asarray([grad_sine(x) for x in xs])
plt.plot(xs, dydxs);
plt.ylim([-1.0, 1.0]);
```
![oyyWv03mRiAAAAAElFTkSuQmCC.png](https://qiita-image-store.s3.amazonaws.com/0/16310/19d62b77-7089-65bf-66ee-9f7421b3fe0e.png &quot;oyyWv03mRiAAAAAElFTkSuQmCC.png&quot;)


## 実際の学習に用いた例：ロジスティック回帰

次に、Autogradで求めた勾配を使って、実際に学習してみます。モデルにはロジスティック回帰モデルを使ってみます。

まず先ほどと同様、`autograd`から必要なモジュールを読みこみ、確率モデルと損失関数を定義します。

```python:
import autograd.numpy as np
from autograd import grad

def sigmoid(x):
    return 0.5 * (np.tanh(0.5 * x) + 1) # typo in the original
    
def logistic_predictions(weights, inputs):
    # ロジスティック回帰モデルで予測されたy=1となる確率。
    # バイアス項は無し。
    return sigmoid(np.dot(inputs, weights))    

def training_loss(weights):
    # トレーニング用損失関数には負の対数尤度（negative log-likelihood）を用いる
    preds = logistic_predictions(weights, inputs)
    label_probabilities = preds * targets + (1 - preds) * (1 - targets)
    return -np.sum(np.log(label_probabilities))
```

次に、データセットを用意します。

```python:
# 小さなデータセットを作成
inputs = np.array([[0.52, 1.12,  0.77],
                   [0.88, -1.08, 0.15],
                   [0.52, 0.06, -1.30],
                   [0.74, -2.49, 1.39]])
targets = np.array([True, True, False, True])
```

ここまで来たら、おもむろに`grad`関数を呼び出し、勾配関数をゲットします。

```python:
# autogradを用いて、トレーニング用損失関数のモデルパラメータに対する勾配を求める
training_gradient_fun = grad(training_loss)
```

勾配を計算できるので、勾配法を用いて逐次的にパラメータをアップデートしていくと、損失関数の値が下がります。

```python:
# モデルパラメータを勾配法を用いて逐次最適化

losses = []

weights = np.array([0.0, 0.0, 0.0])
loss = training_loss(weights)
losses.append(loss)
print &quot;Initial loss:&quot;, loss

for i in xrange(100):
    weights -= training_gradient_fun(weights) * 0.01
    loss = training_loss(weights)
    losses.append(loss)

print &quot;Trained loss:&quot;, loss    
losses = np.asarray(losses)
```

Initial loss: 2.77258872224
Trained loss: 1.06727067579

途中経過の値をプロットするとこんな感じです。しっかり学習できてますね。

```python:
plt.plot(losses)
plt.ylabel(&quot;Training loss&quot;)
plt.xlabel(&quot;Iterations&quot;)
```
![download.png](https://qiita-image-store.s3.amazonaws.com/0/16310/0c75765c-78af-d016-5612-bdcebe8a1452.png &quot;download.png&quot;)


Pythonを使うと確率モデルを簡単に記述できます。Autogradによってもたらされる最大の恩恵は、モデルの変更しても面倒な手順を踏まずにサクッと学習が始められる点です。

[examples](https://github.com/HIPS/autograd/tree/master/examples)ディレクトリには以下の例を含むいろいろなサンプルコードがあります。

- [Simple neural net](https://github.com/HIPS/autograd/blob/master/examples/neural_net.py)
- [Convolutional neural net](https://github.com/HIPS/autograd/blob/master/examples/convnet.py)
- [Recurrent neural net](https://github.com/HIPS/autograd/blob/master/examples/rnn.py)
- [Long short-term memory (LSTM)](https://github.com/HIPS/autograd/blob/master/examples/lstm.py)
- [Neural Turing Machine](https://github.com/DoctorTeeth/diffmem/blob/512aadeefd6dbafc1bdd253a64b6be192a435dc3/ntm/ntm.py)
- [Backpropagating through a fluid simulation](https://github.com/HIPS/autograd/blob/master/examples/fluidsim/fluidsim.py)
- [Variational inference in Bayesian neural network](https://github.com/HIPS/autograd/blob/master/examples/bayesian_neural_net.py)
- [Gaussian process regression](https://github.com/HIPS/autograd/blob/master/examples/gaussian_process.py)

以上、Autogradの紹介でした。内部的にどうやってやっているのかという点については、気が向いたら書くかもですー。それでは、また〜。

## リンク集

Python系実装

- [Autograd for Python @ GitHub](https://github.com/HIPS/autograd)
- [チュートリアル](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#autograd-tutorial)
- [用例集](https://github.com/HIPS/autograd/tree/master/examples)

Torch系実装

- [Autograd for Torch @ GitHub](https://github.com/twitter/torch-autograd/)
- [チュートリアル](https://blog.twitter.com/2015/autograd-for-torch)
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Autogradという野郎が乗り込んできたのでガクブルな件 on @Qiita" data-url="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Autogradという野郎が乗り込んできたのでガクブルな件" href="http://b.hatena.ne.jp/entry/http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/tanemaki"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://secure.gravatar.com/avatar/19040888fb3dc4487f2ed3beff8dccd6?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/tanemaki">tanemaki</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">955</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;tanemaki&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-33543c77-9957-415b-8bc6-546273c91542"></div>
    <div id="UserFollowButton-react-component-33543c77-9957-415b-8bc6-546273c91542"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">Popular Posts</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/tanemaki/items/8dfb3c81a3af6feebdfe">Autogradという野郎が乗り込んできたのでガクブルな件</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/tanemaki/items/7c74b35505dc372bc9c6">Jupyterがすごい勢いでやってくるからお前ら備えとけ（IPython Notebook + R）</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/tanemaki/items/e965cd9c25acbeed860c">IPythonでクリクリできる図を作ろう</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/tanemaki/items/2ed05e258ef4c9e6caac">ゆるふわPandasチートシート</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/tanemaki/items/23006f47dbb7c5cd9f16">Estimote iOS SDKのメジャーアップデートに関するまとめ</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%A2%E3%83%81%E3%83%99%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3\&quot;\u003eモチベーション\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#autograd%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9\&quot;\u003eAutogradの使い方\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E7%B0%A1%E5%8D%98%E3%81%AA%E4%BE%8B\&quot;\u003e簡単な例\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%AE%9F%E9%9A%9B%E3%81%AE%E5%AD%A6%E7%BF%92%E3%81%AB%E7%94%A8%E3%81%84%E3%81%9F%E4%BE%8B%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0\&quot;\u003e実際の学習に用いた例：ロジスティック回帰\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%83%AA%E3%83%B3%E3%82%AF%E9%9B%86\&quot;\u003eリンク集\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-4acdf842-f181-4d3f-9ee6-5c0874962295"></div>
    <div id="Toc-react-component-4acdf842-f181-4d3f-9ee6-5c0874962295"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:487,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;8dfb3c81a3af6feebdfe&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="godhand00@github"><a itemprop="url" href="/godhand00@github"><img alt="godhand00@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/22915/profile-images/1473683681" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="heliac2000"><a itemprop="url" href="/heliac2000"><img alt="heliac2000" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/44994/profile-images/1473690106" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="sharow"><a itemprop="url" href="/sharow"><img alt="sharow" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/11641/profile-images/1473682092" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="kochory"><a itemprop="url" href="/kochory"><img alt="kochory" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/68258/profile-images/1473697785" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="shimo_t"><a itemprop="url" href="/shimo_t"><img alt="shimo_t" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/64100/profile-images/1473696475" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hogefugabar"><a itemprop="url" href="/hogefugabar"><img alt="hogefugabar" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/31899/profile-images/1473685791" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="atsaki"><a itemprop="url" href="/atsaki"><img alt="atsaki" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/15114/profile-images/1473683668" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="josenaka"><a itemprop="url" href="/josenaka"><img alt="josenaka" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/79562/profile-images/1480749853" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="EqualL2"><a itemprop="url" href="/EqualL2"><img alt="EqualL2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/71548/profile-images/1473698862" /></a></div></div><div class="ArticleFooter__user"><a href="/tanemaki/items/8dfb3c81a3af6feebdfe/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/8dfb3c81a3af6feebdfe/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/tanemaki/items/8dfb3c81a3af6feebdfe.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="Autogradという野郎が乗り込んできたのでガクブルな件 on @Qiita" data-url="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="Autogradという野郎が乗り込んできたのでガクブルな件" href="http://b.hatena.ne.jp/entry/http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e本筋とは関係ありませんが、forループをぶん回して$sin(x)$を定義するってとこでつまってしまい、Autogradの凄さがいまいち伝わらないという方のために、$sin(x)$のテイラー近似（正しくはマクローリン展開）に関する自分用手書きノートここに晒しておきます。変数を使い回しながら、逐次的に足し込んでいくという感じですね。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;https://qiita-image-store.s3.amazonaws.com/0/16310/f02a88d2-0524-1e8d-3e38-2bd639d8c5ec.jpeg\&quot; target=\&quot;_blank\&quot; rel=\&quot;nofollow noopener\&quot;\u003e\u003cimg src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/16310/f02a88d2-0524-1e8d-3e38-2bd639d8c5ec.jpeg\&quot; alt=\&quot;IMG_6226.JPG\&quot;\u003e\u003c/a\u003e\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-11-08T14:45:10+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:378198,&quot;is_team&quot;:false,&quot;item_id&quot;:340985,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;8dfb3c81a3af6feebdfe&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;本筋とは関係ありませんが、forループをぶん回して$sin(x)$を定義するってとこでつまってしまい、Autogradの凄さがいまいち伝わらないという方のために、$sin(x)$のテイラー近似（正しくはマクローリン展開）に関する自分用手書きノートここに晒しておきます。変数を使い回しながら、逐次的に足し込んでいくという感じですね。\n\n![IMG_6226.JPG](https://qiita-image-store.s3.amazonaws.com/0/16310/f02a88d2-0524-1e8d-3e38-2bd639d8c5ec.jpeg)\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe#comment-97727d38eedd026ad6ae&quot;,&quot;user&quot;:{&quot;contribution&quot;:955,&quot;created_at&quot;:&quot;2013-02-08T23:39:23+09:00&quot;,&quot;id&quot;:16310,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://secure.gravatar.com/avatar/19040888fb3dc4487f2ed3beff8dccd6?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;tanemaki&quot;},&quot;uuid&quot;:&quot;97727d38eedd026ad6ae&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:340985,&quot;uuid&quot;:&quot;8dfb3c81a3af6feebdfe&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;tanemaki&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:16310,&quot;url_name&quot;:&quot;tanemaki&quot;,&quot;profile_image_url&quot;:&quot;https://secure.gravatar.com/avatar/19040888fb3dc4487f2ed3beff8dccd6?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-ddee7625-835d-4a91-8013-7c9b9b056487"></div>
    <div id="CommentListContainer-react-component-ddee7625-835d-4a91-8013-7c9b9b056487"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="KaeQarYezMLtWWbVqB6Q7AFbZVz5cEkHb6Do3TL5HBY+gVt9N5UfkBHs9DS7tsqnfPhGNqsVFUpC7M0rAoectg==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/tanemaki/items/8dfb3c81a3af6feebdfe" /><input type="hidden" name="item_uuid" id="item_uuid" value="8dfb3c81a3af6feebdfe" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/tanemaki/items/8dfb3c81a3af6feebdfe", "id": 340985, "uuid": "8dfb3c81a3af6feebdfe" }</script><script class="js-user" type="application/json">{&quot;id&quot;:16310,&quot;url_name&quot;:&quot;tanemaki&quot;,&quot;profile_image_url&quot;:&quot;https://secure.gravatar.com/avatar/19040888fb3dc4487f2ed3beff8dccd6?d=https://a248.e.akamai.net/assets.github.com%2Fimages%2Fgravatars%2Fgravatar-user-420.png&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="lBEW16SxSJNmRwUVp3+McVB8nmbee0Q5TYxhsBm99KGDN93AJTqbwZryl/S019Y6Ld+9DIweGHRgwERGKcN0AQ==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/tanemaki/items/8dfb3c81a3af6feebdfe" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-3af9c1d29a49f3320bb796fb5e75304b.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>