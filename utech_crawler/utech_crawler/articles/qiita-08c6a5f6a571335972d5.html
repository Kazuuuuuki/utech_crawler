<!DOCTYPE html><html xmlns:og="http://ogp.me/ns#"><head><meta charset="UTF-8" /><title>DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた - Qiita</title><meta content="width=device-width,initial-scale=1" name="viewport" /><meta content="

はじめに

そもそもDQNが作りたかったわけじゃなくて、他の目的でChainerを使いたかったのでその練習にDQNを書いたんですが、せっかくだし公開しようと思いました 公開しました 。またどうせ公開するなら、この機会にこれ（Q学習+関数近似）関連で持っている知識をついでに整理しようと思ってまとめました。

　ニュース記事とかNatureとかNIPSの論文だけ読むと、DQN作ったDeepmind/Googleすげー！！！って感覚になりそうですが、強化学習的な歴史的経緯..." name="description" /><meta content="summary" name="twitter:card" /><meta content="@Qiita" name="twitter:site" /><meta content="movingsloth" name="twitter:creator" /><meta content="DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた - Qiita" property="og:title" /><meta content="article" property="og:type" /><meta content="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" property="og:url" /><meta content="http://cdn.qiita.com/assets/qiita-fb-2887e7b4aad86fd8c25cea84846f2236.png" property="og:image" /><meta content="#はじめに
そもそもDQNが作りたかったわけじゃなくて、他の目的でChainerを使いたかったのでその練習にDQNを書いたんですが、せっかくだし~~公開しようと思いました~~ **公開しました** 。またどうせ公開するなら、この機会に..." property="og:description" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><link rel="shortcut icon" type="image/x-icon" href="http://cdn.qiita.com/assets/favicons/public/production-4ff10c1e1e2b5fcb353ff9cafdd56c70.ico" /><link rel="apple-touch-icon" type="image/png" href="http://cdn.qiita.com/assets/favicons/public/apple-touch-icon-f9a6afad761ec2306e10db2736187c8b.png" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link rel="stylesheet" media="all" href="http://cdn.qiita.com/assets/public-e89b2462e454a13b67eaa536fcb0b04a.min.css" /><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="nFwdu2Sciol5hCdtlPahOvaoYHiJsfS5YY7kYTQTIy9g82kHEPWPRE+CnIJUqGiCKbwl2QWlxZhtst9IG/UmZw==" /></head><body class="without-js" id=""><noscript><iframe height="0" src="//www.googletagmanager.com/ns.html?id=GTM-TBQWPN" style="display:none;visibility:hidden" width="0"></iframe></noscript><script>
  document.body.className = document.body.className.replace('without-js', '') + ' with-js';
  window.Qiita = {"asset_host":"cdn.qiita.com","TLD":"com","controller_path":"public/items","controller_action":"public/items#show","controller":"items","action":"show","action_path":"public/items#show","env":"production","flash":{},"is_landing_page":false,"is_team_page":false,"request_parameters":{"controller":"public/items","action":"show","user_id":"Ugo-Nama","type":"items","id":"08c6a5f6a571335972d5"},"root_domain":"qiita.com","variant":null,"config":{"mixpanel":{"career":"dd35af27e959781713d63fd7ca898a8d","per_team":"c0a2116368b33b44b5029ebd2cc9b094","public":"be87616606b0e26a87689099aab2c4e5","team":"b7c0342acba2dbc8742484d98788efb3"},"default_locale":"ja","locale":"en"},"team":null,"user":null,"GIT_BRANCH":null,"DEBUG":false};

</script>
<div class="headerContainer headerContainer-public" role="navigation"><div class="js-react-on-rails-component" data-component-name="HeaderContainer" data-props="{&quot;user&quot;:null,&quot;team&quot;:null,&quot;news&quot;:{&quot;type&quot;:&quot;募集&quot;,&quot;content&quot;:&quot;QiitaやQiita:Teamを良くしたいエンジニア&quot;,&quot;url&quot;:&quot;http://increments.co.jp/jobs/engineers?utm_source=qiita\u0026utm_medium=header_news&quot;},&quot;initial_unread_count&quot;:null,&quot;siteid_image&quot;:&quot;http://cdn.qiita.com/siteid-reverse.png&quot;,&quot;is_team_page&quot;:false,&quot;on_team_setting&quot;:false,&quot;show_post_menu&quot;:true,&quot;show_search_menu&quot;:true,&quot;is_fluid&quot;:false,&quot;locale&quot;:&quot;en&quot;}" data-trace="false" data-dom-id="HeaderContainer-react-component-a88735a0-b31b-488e-862b-b822f05aac7b"></div>
    <div id="HeaderContainer-react-component-a88735a0-b31b-488e-862b-b822f05aac7b"></div>
    
</div><div id="main"><script type="application/ld+json">{  "@context": "http://schema.org",  "@type": "BreadcrumbList",  "itemListElement": [    {      "@type": "ListItem",      "position": 1,      "item": {        "@id": "/",        "name": "Qiita"      }    },    {      "@type": "ListItem",      "position": 2,      "item": {        "@id": "/items",        "name": "Items"      }    },    {      "@type": "ListItem",      "position": 3,      "item": {        "@id": "/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92",        "name": "強化学習"      }    }  ]}</script><article itemscope="" itemtype="http://schema.org/Article"><div class="ArticleMainHeader "><div class="container"></div><div class="container"><div class="row s-flex-align-center"><div class="col-sm-9"><h1 class="ArticleMainHeader__title" itemprop="headline">DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた</h1><ul class="TagList"><li class="TagList__item" data-count="75"><a class="u-link-unstyled TagList__label" href="/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92"><img alt="強化学習" class="TagList__icon" src="//cdn.qiita.com/assets/icons/medium/missing-2e17009a0b32a6423572b0e6dc56727e.png" /><span>強化学習</span></a></li><li class="TagList__item" data-count="1830"><a class="u-link-unstyled TagList__label" href="/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92"><img alt="機械学習" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/a94d4d239b3b0b83723d5b56c050ffc54b8593e7/medium.jpg?1394635775" /><span>機械学習</span></a></li><li class="TagList__item" data-count="1070"><a class="u-link-unstyled TagList__label" href="/tags/DeepLearning"><img alt="DeepLearning" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/eac844d1d880a38fc3be5ebf534cad5182b64ebf/medium.jpg?1453002020" /><span>DeepLearning</span></a></li><li class="TagList__item" data-count="356"><a class="u-link-unstyled TagList__label" href="/tags/Chainer"><img alt="Chainer" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/755fdcf477b1d3db5946dad4f779ba11a5954c18/medium.jpg?1434432587" /><span>Chainer</span></a></li><li class="TagList__item" data-count="9868"><a class="u-link-unstyled TagList__label" href="/tags/Python"><img alt="Python" class="TagList__icon" src="https://s3-ap-northeast-1.amazonaws.com/qiita-tag-image/28fd3d6b220c89e6197fd82c02fd2fcd2bb66d81/medium.jpg?1383884245" /><span>Python</span></a></li></ul></div><div class="col-sm-3"><div class="itemsShowHeaderStock"><ul class="list-unstyled itemsShowHeaderStock_statusList"><li><div class="itemsShowHeaderStock_count stock"><span class="fa fa-thumbs-up"></span><span class="js-likecount">595</span></div><div class="itemsShowHeaderStock_countText">Like</div></li><li><div class="itemsShowHeaderStock_count" content="13 UserComments" itemprop="commentCount"><span class="fa fa-comment"></span>13</div><div class="itemsShowHeaderStock_countText">Comment</div></li></ul></div><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:595,&quot;uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-header&quot;}"></div><ul class="list-inline ArticleMainHeader__users"><li class="js-hovercard" data-hovercard-target-name="neka-nat@github"><a itemprop="url" href="/neka-nat@github"><img alt="neka-nat@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/14516/profile-images/1473683376" /></a></li><li class="js-hovercard" data-hovercard-target-name="menphim"><a itemprop="url" href="/menphim"><img alt="menphim" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/55217/profile-images/1473693620" /></a></li><li class="js-hovercard" data-hovercard-target-name="pyr_revs"><a itemprop="url" href="/pyr_revs"><img alt="pyr_revs" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/69525/profile-images/1473698177" /></a></li><li class="js-hovercard" data-hovercard-target-name="tokuhisa_f"><a itemprop="url" href="/tokuhisa_f"><img alt="tokuhisa_f" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/18292/profile-images/1473682394" /></a></li><li class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></li><li class="js-hovercard" data-hovercard-target-name="sotetsuk"><a itemprop="url" href="/sotetsuk"><img alt="sotetsuk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/35293/profile-images/1473686670" /></a></li><li class="js-hovercard" data-hovercard-target-name="ixixi"><a itemprop="url" href="/ixixi"><img alt="ixixi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/8954/profile-images/1473681204" /></a></li><li class="js-hovercard" data-hovercard-target-name="ssssssk"><a itemprop="url" href="/ssssssk"><img alt="ssssssk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/80548/profile-images/1473701825" /></a></li><li class="js-hovercard" data-hovercard-target-name="mero"><a itemprop="url" href="/mero"><img alt="mero" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63104/profile-images/1473696161" /></a></li><li class="js-hovercard" data-hovercard-target-name="hiro_matsuno2"><a itemprop="url" href="/hiro_matsuno2"><img alt="hiro_matsuno2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/9764/profile-images/1473681543" /></a></li><li><a href="/Ugo-Nama/items/08c6a5f6a571335972d5/likers"><span class="fa fa-ellipsis-h"></span></a></li></ul></div></div></div></div><div class="ArticleAsideHeader"><div class="container"><div class="u-flex u-space-between"><div class="u-flex u-flex-wrap"><div class="u-flex u-align-center s-pdv-5 u-flex-wrap"><div class="ArticleAsideHeader__author"><a href="/Ugo-Nama"><img class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182" alt="1473699182" /></a> <a class="u-link-unstyled" href="/Ugo-Nama">Ugo-Nama</a> </div><div class="ArticleAsideHeader__date"><meta content="2015-07-11T13:43:45+09:00" itemprop="datePublished" /><span data-toggle="tooltip" title="posted at 2015-07-11">Edited at <time datetime="2017-03-13T00:08:08+09:00" itemprop="dateModified">2017-03-13</time></span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"><div class="ArticleAsideHeader__revision"> <a data-toggle="tooltip" title="Revisions" href="/Ugo-Nama/items/08c6a5f6a571335972d5/revisions"><span class="fa fa-history"></span></a><span class="ArticleAsideHeader__revisionCount">26</span></div></div><div class="u-flex u-align-center s-pdv-5 mobile-hidden"></div></div><div class="u-flex u-align-center s-flex-justiry-between s-pdv-5 u-shrink-0"><div class="ArticleAsideHeader__stock"><div class="js-stockbutton" data-position="top" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h fa-lg"></span></a><ul class="dropdown-menu dropdown-menu-right"><li class="dropdown__item--mobile"><a href="/Ugo-Nama/items/08c6a5f6a571335972d5/revisions"><span class="fa fa-fw fa-history"></span> Revisions<span>(26)</span></a></li><li><a href="/Ugo-Nama/items/08c6a5f6a571335972d5.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><span class="fa fa-fw fa-flag"></span> Report article</a></li></ul></div></div></div></div></div><div class="container"><div class="row" id="article-body-wrapper"><div class="col-sm-9"><section class="markdownContent markdownContent-headingEnabled js-task-list-container clearfix position-relative" id="item-08c6a5f6a571335972d5" itemprop="articleBody">
<h1>
<span id="はじめに" class="fragment"></span><a href="#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB"><i class="fa fa-link"></i></a>はじめに</h1>

<p>そもそもDQNが作りたかったわけじゃなくて、他の目的でChainerを使いたかったのでその練習にDQNを書いたんですが、せっかくだし<del>公開しようと思いました</del> <strong>公開しました</strong> 。またどうせ公開するなら、この機会にこれ（Q学習+関数近似）関連で持っている知識をついでに整理しようと思ってまとめました。</p>

<p>　ニュース記事とかNatureとかNIPSの論文だけ読むと、DQN作ったDeepmind/Googleすげー！！！って感覚になりそうですが、強化学習的な歴史的経緯を考えると強化学習+深層学習になった、むしろかなり当然の成り行きで生まれた技術であることがわかります。（ATARIのゲームを人間以上のパフォーマンスでプレイするというのがビジュアル的にわかりやすかった$\leftrightarrow$問題設定が良かったというのもあります。）</p>

<p>　この記事ではNIPSとNatureの以下の２本の論文</p>

<p>・ V. Mnih <em>et al.</em>, "Playing atari with deep reinforcement learning"<br>
　　<a href="http://arxiv.org/pdf/1312.5602.pdf" class="autolink" rel="nofollow noopener" target="_blank">http://arxiv.org/pdf/1312.5602.pdf</a><br>
・ V. Mnih <em>et al.</em>, "Human-level control through deep reinforcement learning"<br>
　　<a href="http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html" class="autolink" rel="nofollow noopener" target="_blank">http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html</a></p>

<p>について、そこにいたるまでの強化学習の経緯をなるべく関係するところだけを取り上げて解説するつもりです。</p>

<p>　オタクな追っかけ的情報ですが、筆頭著者のMnihさんはUCTをはじめ確率的プランニングで有名なSzepesvári先生と、おなじみニューラルネットワークのゴッドファーザー、Hinton先生の元で学んだ人で、DQNが生まれたのは自然な流れといえるでしょう。</p>

<h1>
<span id="ていうか強化学習って" class="fragment"></span><a href="#%E3%81%A6%E3%81%84%E3%81%86%E3%81%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%A3%E3%81%A6"><i class="fa fa-link"></i></a>ていうか、強化学習って？</h1>

<p>強化学習自体、聞きなれない方もいるかもしれません。Bishop先生の"PRML"とか、最近だとMurphy先生の"Machine Learning"とかでも「本書では機械学習の多くの分野を扱う。ただし強化学習は除く。」のような扱いになっています。</p>

<p>　これは、強化学習の特殊な条件設定のためだと思います。というのも、強化学習の理論は制御理論（最適制御理論・動的計画法）と機械学習をシェイカーに入れて力いっぱいかき混ぜたものになっているからです。ここではそういった理論に立ち入ることなく、「強化学習って何するのよ？」という事をできるだけ簡単に説明します（するつもりです）。</p>

<h2>
<span id="強化学習環境とエージェント" class="fragment"></span><a href="#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%92%B0%E5%A2%83%E3%81%A8%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88"><i class="fa fa-link"></i></a>強化学習：環境とエージェント</h2>

<p>　強化学習とは、"環境"中に置かれた"エージェント"が、環境との相互作用を通して最適な方策（行動を決定するきまり）を得るための機械学習の手法のことを言います。</p>

<p>　Wikipediaによると、「強化学習(きょうかがくしゅう, Reinforcement Learning)とは、ある環境内におけるエージェントが、現在の状態を観測し、取るべき行動を決定する問題を扱う機械学習の一種。」 <sup id="fnref1"><a href="#fn1" rel="footnote" title="強化学習 in Wikipediahttps://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92">1</a></sup> だそうです。</p>

<p>　百聞は一見にしかずというやつで、模式図で表すと次の図のようになります。<br>
<a href="https://qiita-image-store.s3.amazonaws.com/0/72529/dc77a4fe-85a1-a69e-e0e3-571e2e04a33f.png" target="_blank" rel="nofollow noopener"><img src="https://qiita-image-store.s3.amazonaws.com/0/72529/dc77a4fe-85a1-a69e-e0e3-571e2e04a33f.png" alt="RL.png"></a></p>

<p>強化学習は次の単純なステップを繰り返すことで時間が進行していきます。</p>

<blockquote>
<ol>
<li>エージェントは環境から受け取った観測 $o$（あるいは直接、環境の状態 $s$ ）を受け取り、方策 $\pi$ に基いて環境に行動 $a$ を返す。</li>
<li>環境はエージェントから受け取った行動 $a$ と現在の状態 $s$ に基いて、次の状態 $s'$ に変化し、その遷移に基いて次の観測 $o'$ と、報酬 $r$ と呼ばれる直前の行動の良し悪しを示す1つの数（スカラー量）をエージェントに返す。</li>
<li>時間の進行：$t \leftarrow t + 1$</li>
</ol>
</blockquote>

<p>ここで $\leftarrow$ は代入操作を表します。報酬は通常、状態と行動、そして次の状態の関数で与えられます $r=r(s,a,s')$。我々は強化学習の条件から、環境を直接操作するといったことはできません。我々が自由に操作できるのは、あくまでエージェントのみです。このような環境とエージェントの相互作用、そして制約が、強化学習の置かれた特殊性を表しています。</p>

<p>　ここでは簡単のため、観測は環境の状態が直接エージェントに渡される状況（ $o=s$ ）についてのみ説明します <sup id="fnref2"><a href="#fn2" rel="footnote" title="このような状況（本当はもう少し色々仮定がありますが）を、「環境はマルコフ決定過程（Markov Decision Process, MDP）で記述できるものと仮定する」などと言います。状態が直接エージェントに渡されない場合は部分観測マルコフ決定過程（Partially Observable Markov Decision Process, POMDP）と呼ばれ、このような環境中での強化学習は近年ではPartially Observable Reinforcement Learning（PORL）などと呼ばれます。ATARIゲームを学習する設定はゲーム画面のみをエージェントの入力としているので当然PORLに含まれますが、4ステップ間のゲーム画面をもって状態が構成されると仮定して学習を行っています。">2</a></sup> 。強化学習の枠組みにおいてエージェントにとって最適な方策とは、その方策に従って行動を決定していくことで、現時点から無限の未来までに得ることのできる報酬の和</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} = r_{t+1} + \gamma\ r_{t+2} + \gamma^2\ r_{t+3} + \cdots
</pre></div></div>

<p>を最大化させるような方策であると考えます。すなわちこれが強化学習における目的関数です。$t$ は現時刻、$r_i$ は時刻 $i$ でエージェントが受け取る報酬を表します。$\gamma$ は割引率と呼ばれる強化学習のパラメータです。この値は通常 $0$ 以上 $1$ 未満の連続値で与えるもので、無限大・無限小を含まないどのような報酬設定に対しても $R_t$ が有限の値になるように導入される定数です。たいてい、$\gamma = 0.99$などといった$1$に近い値に設定します。</p>

<p>　今のように環境の状態がエージェントの観測として直接渡される（ $o=s$ ）場合は<strong><em>必ず、行動を出力とするある決定論的な関数</em></strong> $\pi^* (s)$ <strong><em>で表すことが可能な最適方策が、少なくとも1つ存在する</em></strong>ことが理論的に知られています<sup id="fnref3"><a href="#fn3" rel="footnote" title='この点に関して詳しくは M. Putermanの "Markov decision processes" や O . Sigaud, O. Buffetの "Markov decision processes in artificial intelligence" などを参照してください。'>3</a></sup>。詳細は割愛しますが、以下の節で紹介する学習手法はいずれも上記の目的関数を最大化させる決定論的な方策を探す手法という点で同じものです。</p>

<p>　なおDQNがATARIゲームを学習する場合、4ステップ間のゲーム画面（観測）をもって状態が構成されると仮定して学習を行っています。</p>

<h1>
<span id="deep-q-network出生以前" class="fragment"></span><a href="#deep-q-network%E5%87%BA%E7%94%9F%E4%BB%A5%E5%89%8D"><i class="fa fa-link"></i></a>Deep Q-Network出生以前</h1>

<p>ここではDeep Q-Networkが生まれた背景について紹介していきます。Deep Q-Networkを構成するQ学習、関数近似、Experience Replay（経験再生）、さらにNeural Fitted Q Iterationについて説明します。</p>

<h2>
<span id="q学習" class="fragment"></span><a href="#q%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>Q学習</h2>

<p>Q学習は強化学習の古典的なアルゴリズムで、強化学習のアルゴリズムでは最も広く知られた存在かもしれません。Q学習に基づく手法では、みな <strong>最適行動価値関数</strong> と呼ばれる関数を近似することで最適方策を学習します。</p>

<p>　最適行動価値は各状態と行動の組 $(s,a)$ にそれぞれ１つ存在し、「状態 $s$ で行動 $a$ を取り、残りはずっと最適方策に従ったとした場合に得る報酬の和（即ち目的関数） $R$の期待値」を表す値です。これを全ての各状態と行動の組 $(s,a)$について求めたものを最適行動価値関数と呼び、 $Q^* (s,a)$ で表します。</p>

<p>　ややこしい定義ですが、この値はざっくり言うと、エージェントが置かれた状態 $s$ で行動 $a$ を取る<strong>メリット</strong> を表します。この<strong>最適行動価値関数に関して重要な点は、この関数と最適な決定論的方策（のうちの1つである） $ \pi ^* (s)$ との間には</strong></p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\pi^*(s) = \rm{argmax}_a Q^* (s,a)  
</pre></div></div>

<p><strong>の関係があるということです。</strong>すなわち、Q学習において <sup id="fnref4"><a href="#fn4" rel="footnote" title="より正確には、「マルコフ決定過程で記述できる環境中において」">4</a></sup> 最適行動価値関数を得ることと最適方策を得ることは、ほぼ同じ意味を持つのです。</p>

<p>　実際のQ学習ではすべての状態 $s$ と行動の組 $a$ に対してテーブル関数 $Q(s,a)$ を作成し、すべての要素を任意の値に初期化した後、毎回のデータ ($s$, $a$, $r$, $s'$) に対して次の式で更新します。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \Bigl(r + \gamma \ \max_{a'} Q(s',a')\Bigr)
</pre></div></div>

<p>ここで $r$ は状態 $s$ で行動 $a$ を選択したあと環境から受け取った報酬、$s'$ は同様に状態 $s$ で行動 $a$ を選択した後、次の時刻で受け取った状態を表します。$\alpha$ は学習率を表します。通常は $1$ 未満の $0.1$ や $0.01$ といった小さい値を使います。このアルゴリズムは $Q(s,a)$ を、毎回その関数自身を含む $r + \gamma \ \max_{a'} Q(s',a')$ に毎回少しづつ近づけていく形になっています<sup id="fnref5"><a href="#fn5" rel="footnote" title="アルゴリズムの完全な形は次のサイトを参照してください： http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html">5</a></sup>。</p>

<p>　上の更新式の表記は（特にSutton &amp; Barto本で強化学習を知られた方には）あまり強化学習ではポピュラーではないかもしれませんが、この表記は価値関数 $Q(s,a)$ を何に当てはめようとしているのかを明示的に表す形式になっています。当然、この更新式はよりポピュラーな表記である</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
Q(s,a) \leftarrow Q(s,a) + \alpha \Bigl(r + \gamma\ \max_{a'} Q(s',a') - Q(s,a)\Bigr)
</pre></div></div>

<p>と等価です。</p>

<p>　Q学習の最大の特徴は、<strong>($s$, $a$) の全ての組からサンプル ($s$, $a$, $r$, $s'$) が無限回得られるとするなら、それらをどのような順番で与えたとしても上記のアルゴリズムで必ず最適な価値関数 $Q^*(s,a)$ が得られる</strong>という点にあります<sup id="fnref6"><a href="#fn6" rel="footnote" title="理論的には、学習率 $\alpha$ がアップデートごとに徐々に減衰していく必要があります。ただし強化学習アルゴリズムを長期間回そうという時や動的な環境にも常にある程度適応させたいといった状況には、あまり現実的ではありません。収束条件に関して詳しくはMelo先生の「Convergence of Q-learning: a simple proof」を参照：http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf">6</a></sup>。この特徴は後で紹介するExperience Replayで最大限に利用されます。　</p>

<p>　すべての状態と行動についてテーブル関数 $Q(s,a)$ を作成するとは言いましたが、例えばDQNのように画像列などのむちゃくちゃに高次元なものを状態として与えたりすると、当然テーブル関数ではとてもメモリに乗りませんし、すべてサンプルを取ることなんて出来ません。 $10\times 10$のバイナリ画像でも $2^{100} \approx 10^{30} $ といったとても手に負えない状態数になってしまいます。そこで、こういった非常に高次元の状態を扱う場合には $Q(s,a)$ に関数近似を使います。</p>

<h2>
<span id="deepじゃないq-networkq学習--関数近似" class="fragment"></span><a href="#deep%E3%81%98%E3%82%83%E3%81%AA%E3%81%84q-networkq%E5%AD%A6%E7%BF%92--%E9%96%A2%E6%95%B0%E8%BF%91%E4%BC%BC"><i class="fa fa-link"></i></a>DeepじゃないQ-network：Q学習 ＋ 関数近似</h2>

<p>というわけで、関数 $Q(s,a)$ に関数近似を使うことを考えます。そこで $Q(s,a)$ が何らかのパラメータ $\theta$ を用いて表されていて、近似された関数を $Q_\theta(s,a)$ で表すことにします。</p>

<p>　この関数を用いてQ学習に相当する関数を近似するには、古典的には以下の勾配法によるアルゴリズムを使います。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\theta \leftarrow \theta - \alpha \nabla_\theta L_\theta
</pre></div></div>

<p>ここで $L_\theta$ は以下の誤差関数で表されます。このへんは一般的な、勾配法での更新とおんなじです。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
L_\theta = \frac{1}{2}\Bigl( \rm{target} - Q_\theta(s,a)\Bigr)^2
</pre></div></div>

<p>ところで、真の行動価値 $Q^* (s,a)$ がわかっていれば純粋な教師あり学習として $\rm{target} = Q^* (s,a)$ とすれば良いのですが、強化学習では教師信号にあたる $Q^* (s,a)$ は与えられません。そこでテーブル関数を用いたQ学習と同様に</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
 \rm{target} = r + \gamma \max_{a'} Q_\theta (s',a')
</pre></div></div>

<p>をその時点での教師信号として使います。そして誤差関数の微分は、あくまで $Q_\theta(s,a)$ を教師信号に近づけるためとして、教師信号に対する微分は行いません。つまり</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\begin{align}
\nabla_\theta L_\theta &amp;=-\Bigl(\rm{target} - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)\\
&amp;=-\Bigl(r + \gamma \max_{a'} Q_\theta (s',a') - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)
\end{align}
</pre></div></div>

<p>となります。</p>

<p>　結果、近似関数を用いた場合のQ学習は以下のようになります。</p>

<div class="code-frame" data-lang="math"><div class="highlight"><pre>
\theta \leftarrow \theta + \alpha \Bigl(r + \gamma \max_{a'} Q_\theta (s',a') - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)
</pre></div></div>

<p>テーブル関数を用いたQ学習の２つ目の更新式にとても良く似た式が現れました。</p>

<p>　関数近似を用いたQ学習で気をつけなければいけないことは、<strong>誤差関数の微分の計算において教師信号に含まれる $\max_{a'} Q_\theta (s',a')$ を微分してはならない</strong>ということです。自分で微分してコードに落とすようなときはこのような間違いはまず起こりませんが、最近広く使われる自動微分を利用すると、知らない間に教師信号まで含めて微分してしまっているという事が起こりえます（しかも、学習初期は両者おんなじような感じで動くので気付きにくい）。(自分も一度やりました ﾎﾞｿｯ)</p>

<p>　教師信号まで含めて微分して学習させてしまうと、ここで述べた関数近似器を用いたQ学習とは異なる性質を持つアルゴリズムになってしまいます。</p>

<h3>
<span id="関数近似にニューラルネットワークを使う" class="fragment"></span><a href="#%E9%96%A2%E6%95%B0%E8%BF%91%E4%BC%BC%E3%81%AB%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E4%BD%BF%E3%81%86"><i class="fa fa-link"></i></a>関数近似にニューラルネットワークを使う</h3>

<p>多層のニューラルネットワークのような非線形関数近似手法を用いたQ学習は、誤差逆伝播法が開発された第二次ニューラルネットワークの時代、強化学習が理論的に成熟し始めた時期に、Long-Ji Linさんという方が精力的に行いました。Experience Replayはこの方が論文"Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching " で公開した手法です。</p>

<p>　余談ですが、この論文は複雑でダイナミックなシミュレーション環境中で、なんとか生き残ろうとするエージェントを学習させるというとても面白い論文になっています。</p>

<p>　多層（といっても全結合の3層か4層程度）ニューラルネットワークではどうしても学習が遅く、また学習率を大きくしても学習が発散してしまいます。これをなんとか高速化する方法として、上記で述べたQ学習の特性を利用します。</p>

<p>　Experience Replayとはエージェントが経験したサンプル ($s$, $a$, $r$, $s'$）を全て（あるいは有限の時間数ぶん）記録しておき、一度つかったサンプルを上記の関数近似を用いたQ学習で何度も利用するという手法です。これは、テーブル関数を用いたQ学習ではサンプルの順番をどのように並べ替えても学習は進行するので、関数近似器を使ったQ学習でも同様の性質を期待するという考え方になっています。</p>

<p>　Experience Replayのある種の極限を考えるようなアルゴリズムが、Riedmiller先生のNeural Fitted Q Iteration <sup id="fnref7"><a href="#fn7" rel="footnote" title='Martin Riedmiller, "Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method".http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.1193&amp;rep=rep1&amp;type=pdf'>7</a></sup>です。これはここまでの「自分でデータを集めて、学習する」といったオンラインの強化学習とは異なり、まずサンプル ($s$, $a$, $r$, $s'$）からなる十分な数のデータがあるものとして、「これ以上サンプルを追加せず、与えられたデータから最適方策を学習する」、すなわちバッチ強化学習を行うことを考えるアルゴリズムです。</p>

<p>　Neural Fitted Q Iterationのアルゴリズムの擬似コードは以下のようになります。</p>

<blockquote>
<p>input: 多数のサンプル（$s$, $a$, $r$, $s'$）からなるデータ $D$<br>
output: 学習後のQ関数 $Q_{\theta_N} (s,a)$ <br>
1. $k=0$ <br>
2. ニューラルネットワークのパラメータ $\theta_0$ を初期化する<br>
3. 繰り返し： $N-1$ 回繰り返す<br>
4. 　　　　データ $D$ からサンプルを $M$ 個取り出す<br>
5.　　　　 $M$個のサンプルを元に、<br>
　　　　　　　  教師信号 $ {\rm{target}}^t = r_t + \gamma \max_{a'} Q_{\theta_k} (s_t',a') $ と<br>
　　 　　　　　入力 $ (s_t, a_t) $ を作成する（ $t=1,2,...,M$ ） <br>
6.　　　　 作成した教師信号 ${\rm{target}}^t$ と入力$ (s_t, a_t) $を元に、<br>
　　　　　 $Q_\theta (s, a)$ と教師信号との誤差$L_\theta$ を何らかの勾配法で最小化<br>
　　　　　 する。学習が収束するなり一定の条件を満たしたら、その結果を<br>
 　　　　　 $\theta_{k+1}$とする<br>
7.　　　　 $k \leftarrow k+1$とする<br>
8. $Q_{\theta_N} (s,a)$ を返す</p>
</blockquote>

<p>Neural Fitted Q Iterationで重要な点は、繰り返し中の各回に行われる勾配法を使った最適化において、教師信号は常に同じパラメータ $\theta_k$ に基づく $r+\max_{a'} Q_{\theta_k} (s_t',a') $ から生成されていて、そのためオンライン手法とは異なり、<strong>各最適化中では完全に教師あり学習になっている</strong>ところです。そのため、設定的には非常に学習が安定したものとなると期待されます。</p>

<p>　Neural Fitted Q Iterationはデータ数が一定として、そこから情報を引き出すアルゴリズムですが、さらに発展的な手法で、ある程度学習を行った後にエージェントを環境中で動かし、バッチのデータを後から追加していくGrowing Batchという考え方が提案されています <sup id="fnref8"><a href="#fn8" rel="footnote" title='Sascha Lange, Thomas Gabel, and Martin Riedmiller, "Batch reinforcement learning".http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf'>8</a></sup> 。ここまでくるとほとんど手法的にはNature論文のものと変わらないです。</p>

<p>　Riedmiller先生のチームはこのバッチ強化学習を主な手法として、ロボットの行動学習などに適用しています。先生もDeepmindに籍をおいているようで、驚くほど多くの強化学習研究者がDeepmindに集積されていることが伺えます。</p>

<h1>
<span id="そしてdeep-q-networkq学習--深層学習" class="fragment"></span><a href="#%E3%81%9D%E3%81%97%E3%81%A6deep-q-networkq%E5%AD%A6%E7%BF%92--%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92"><i class="fa fa-link"></i></a>そしてDeep Q-Network：Q学習 ＋ 深層学習</h1>

<p>　「DQNの何が新しい技術なのか？」という問いは、「以上の強化学習の関数近似に深層学習の技術を適用した」という点に尽きます。</p>

<p>　具体的には、NIPSとNatureの両方での論文で畳み込みニューラルネットワークが用いられていますが、この構造を使用したこと。それと近年開発されたRMSPropを始め、局所解にはまらない最適化手法を適用したこと。これらが本質的にDQNにおいて新しい点と言えます。<br>
（追記：ネットワークに対して状態のみを入力として、その状態における全ての行動価値の推定値を一回のforward伝搬で表現するDQNの構造は新しいものだと思うのですが、<del>すみません確信が持てません。</del>）<br>
（追記:$\uparrow$の追記で紹介したDQNの構造ですが，先例がありました。遅くとも2009年の時点で柴田&amp;河野が同様の構造を持った5層の全結合ニューラルネットワークを用いてQ学習を構成し、画像入力を用いた強化学習をAIBOの行動学習タスクに適用しています<sup id="fnref9"><a href="#fn9" rel="footnote" title="Katsunari Shibata and Tomohiko Kawano: Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network, Advances in Neuro-Information Processing, Lecture Notes in Computer Science, Proc. of ICONIP (Int'l Conf. on Neural Information Processing) 08, Vol. 5506, pp. 755--762, 55060755.pdf (CD-ROM), 2009">9</a></sup>。）</p>

<p>　DQNが最初に現れたNIPSでの論文"Playing atari with deep reinforcement learning"ではExperience Replayをミニバッチ手法と組み合わせて大規模に使っています。しかしこれでもなお、オンライン強化学習アルゴリズムはまだ不安定なところがあります。</p>

<p>　一方、Natureでの論文"Human-level control through deep reinforcement learning"ではNeural Fitted Q Iteration ＋ Growing Batchを適用することで、オンライン「ほとんどバッチ」強化学習による安定化が図られています。</p>

<p>　私の知っている、DQNの生い立ちはこんなところです。</p>

<h1>
<span id="dqnをpython--chainerで書いたよコード公開しました" class="fragment"></span><a href="#dqn%E3%82%92python--chainer%E3%81%A7%E6%9B%B8%E3%81%84%E3%81%9F%E3%82%88%E3%82%B3%E3%83%BC%E3%83%89%E5%85%AC%E9%96%8B%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F"><i class="fa fa-link"></i></a>DQNをPython + Chainerで書いたよ（コード公開しました）</h1>

<p>というわけでDQNをChainerの練習に実装しました。githubで<del>公開予定です。</del><br>
$\rightarrow$ <strong>公開しました。</strong> <br>
<a href="https://github.com/ugo-nama-kun/DQN-chainer" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/ugo-nama-kun/DQN-chainer</a></p>

<p>　<strong>追記１</strong>：公開したコードに関してnature/nipsバージョンの両方を公開しました。テストには、強化学習研究でベンチマークなどを行う際によく用いられるRLglueとRLglue Python-codec、ATARIエミュレータとRLglueをつなぐArcade Learning Environment、PythonパッケージにはNumpy、Scipy、そしてモチロンChainerが必要です。GPUでの計算を想定しているので、NVIDIAのGPUが必要です。Ubuntu 14.04LTSでのみ動作確認しています。</p>

<p>　<strong>追記２</strong>：DQNのExperience Replay用に保存する履歴数を、オリジナルの1/10に設定しています（ $ 10^6 \rightarrow 10^5$ ）。これは単純に、私の小規模なマシンでも動くように規模縮小したかったからです。今回の履歴の実装は、Nature誌中に記載されているアルゴリズムを愚直に実装したものなので非効率的な面があると思います。少なくともPongでは問題なく学習が行われることを確認しています。</p>

<p>　<strong>追記3</strong>：動作確認は全てATARIゲームのうち、Pongで行っています。他のゲームで学習させる方法はreadme.txtを参照してください。規模縮小しているため、想定されるパフォーマンスが学習されないかもしれません。</p>

<p>　NIPSバージョンとNatureバージョンの両方を作成しましたが、NIPSバージョンは論文中に不明なパラメータも多く、収束の雰囲気はNatureバージョンのほうがよろしい感じです。NIPSバージョンはおまけ程度と考えてください。</p>

<p>Natureバージョンでの学習結果はyoutubeに載せてます $\downarrow$<br>
<a href="https://www.youtube.com/watch?v=N813o-Xb6S8" class="autolink" rel="nofollow noopener" target="_blank">https://www.youtube.com/watch?v=N813o-Xb6S8</a></p>

<p>追記：NIPSバージョンの学習もうまくいったので載せました（結果の挙動は殆ど変わりませんが） $\downarrow$<br>
<a href="https://www.youtube.com/watch?v=ez_JEHMUpng" class="autolink" rel="nofollow noopener" target="_blank">https://www.youtube.com/watch?v=ez_JEHMUpng</a></p>

<p>　Chainerを使った感想ですが<sup id="fnref10"><a href="#fn10" rel="footnote" title="Chainerは公開後も頻繁にアップデートされて新機能が追加されているので、あくまで現在（2015, 7月）の所感です。">10</a></sup>、一度TheanoでもDQNを実装してyoutubeに動画を公開をしてみた経験から言うと、ChainerはTheanoベースのものに比べてエラーがどこで発生しているか本当にわかりやすくなっています。かなりの自由度もあるので、ニューラルネットワークのスタンダードなコンポーネントを組み合わせて使うような（畳み込みニューラルネットワークを使いたいとか、層を積んでいきたいとか）使い方ならば断然オススメできます。</p>

<p>　一方でTheanoは、なにか新しいアルゴリズムを試したいとき、ほぼそれがどのようなものでも同じような操作でGPUでの高速化が可能な点がやはり魅力です。例えば「LSTMセルのようなでも別の、もっとイイものを新たに開発してやるぜっ！それをガッツリ最適化するぜ！」って時にはTheanoのような一般的なものを使う必要がでてくる感じがします。こうなるとほとんど先端研究とか学術的（でさらに特殊）な場面で使う感じになりそうですね。以上感想でした。</p>

<h1>
<span id="tipsdqnという命名は何故起こったのか" class="fragment"></span><a href="#tipsdqn%E3%81%A8%E3%81%84%E3%81%86%E5%91%BD%E5%90%8D%E3%81%AF%E4%BD%95%E6%95%85%E8%B5%B7%E3%81%93%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%8B"><i class="fa fa-link"></i></a>Tips：DQNという命名は何故起こったのか？</h1>

<p>この話は完全に蛇足になります。</p>

<p>　ヤンキー的な人をDQNと呼ぶほうがどちらかと言うと状況的にまれな現象だと思うのですが、ともかく今回のアルゴリズムがDeep Q-Network = DQNになってマッチングしてしまったのも、結構奇跡的な感じがします。</p>

<p>　それぞれのD-Q-Nのコンポーネントを掘り起こすとDは深層学習（Deep learning）、QはQ学習、Nはニューラルネットワーク（Neural Network）になります。</p>

<p>　まずDeepは昨今の流れ的によしとして（このDeepナントカという呼び方は恐らくHinton先生たちの論文"A fast learning algorithm for deep belief nets"以降から爆発的に増えたという見方がされています。）、Q学習のQは何なんでしょう。強化学習の生みの親のひとり、Barto先生のコメントによると、この"Q"は"Quality"のQ（と思ってるよ）ということらしいです <sup id="fnref11"><a href="#fn11" rel="footnote" title="ちょっと情報源がマユツバですが：http://www.quora.com/How-was-the-term-Q-learning-coined">11</a></sup> 。</p>

<p>　つぎに、Q-Networkという呼び方はあまり強化学習で一般的でない印象です。論文中でも、だいたい「Q学習で関数近似器にニューラルネットワークを使った」といった言い方がなされている気がします。近い表現は先ほどのLinさんが1993年の論文でQ-netという呼び方をしていて、恐らくこれが今回Q-Networkと呼ばれるようになった源流ではないかと思います。</p>

<h1>
<span id="注" class="fragment"></span><a href="#%E6%B3%A8"><i class="fa fa-link"></i></a>注：</h1>

<div class="footnotes">
<hr>
<ol>

<li id="fn1">
<p>強化学習 in Wikipedia<br><a href="https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92" class="autolink" rel="nofollow noopener" target="_blank">https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92</a> <a href="#fnref1">↩</a></p>
</li>

<li id="fn2">
<p>このような状況（本当はもう少し色々仮定がありますが）を、「環境はマルコフ決定過程（Markov Decision Process, MDP）で記述できるものと仮定する」などと言います。状態が直接エージェントに渡されない場合は部分観測マルコフ決定過程（Partially Observable Markov Decision Process, POMDP）と呼ばれ、このような環境中での強化学習は近年ではPartially Observable Reinforcement Learning（PORL）などと呼ばれます。ATARIゲームを学習する設定はゲーム画面のみをエージェントの入力としているので当然PORLに含まれますが、4ステップ間のゲーム画面をもって状態が構成されると仮定して学習を行っています。 <a href="#fnref2">↩</a></p>
</li>

<li id="fn3">
<p>この点に関して詳しくは M. Putermanの "Markov decision processes" や O . Sigaud, O. Buffetの "Markov decision processes in artificial intelligence" などを参照してください。 <a href="#fnref3">↩</a></p>
</li>

<li id="fn4">
<p>より正確には、「マルコフ決定過程で記述できる環境中において」 <a href="#fnref4">↩</a></p>
</li>

<li id="fn5">
<p>アルゴリズムの完全な形は次のサイトを参照してください： <br><a href="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html" class="autolink" rel="nofollow noopener" target="_blank">http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html</a> <a href="#fnref5">↩</a></p>
</li>

<li id="fn6">
<p>理論的には、学習率 $\alpha$ がアップデートごとに徐々に減衰していく必要があります。ただし強化学習アルゴリズムを長期間回そうという時や動的な環境にも常にある程度適応させたいといった状況には、あまり現実的ではありません。収束条件に関して詳しくはMelo先生の「Convergence of Q-learning: a simple proof」を参照：<br><a href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf" class="autolink" rel="nofollow noopener" target="_blank">http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf</a> <a href="#fnref6">↩</a></p>
</li>

<li id="fn7">
<p>Martin Riedmiller, "Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method".<br><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.1193&amp;rep=rep1&amp;type=pdf" class="autolink" rel="nofollow noopener" target="_blank">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.1193&amp;rep=rep1&amp;type=pdf</a> <a href="#fnref7">↩</a></p>
</li>

<li id="fn8">
<p>Sascha Lange, Thomas Gabel, and Martin Riedmiller, "Batch reinforcement learning".<br><a href="http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf" class="autolink" rel="nofollow noopener" target="_blank">http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf</a> <a href="#fnref8">↩</a></p>
</li>

<li id="fn9">
<p>Katsunari Shibata and Tomohiko Kawano: Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network, Advances in Neuro-Information Processing, Lecture Notes in Computer Science, Proc. of ICONIP (Int'l Conf. on Neural Information Processing) 08, Vol. 5506, pp. 755--762, 55060755.pdf (CD-ROM), 2009 <a href="#fnref9">↩</a></p>
</li>

<li id="fn10">
<p>Chainerは公開後も頻繁にアップデートされて新機能が追加されているので、あくまで現在（2015, 7月）の所感です。 <a href="#fnref10">↩</a></p>
</li>

<li id="fn11">
<p>ちょっと情報源がマユツバですが：<br><a href="http://www.quora.com/How-was-the-term-Q-learning-coined" class="autolink" rel="nofollow noopener" target="_blank">http://www.quora.com/How-was-the-term-Q-learning-coined</a> <a href="#fnref11">↩</a></p>
</li>

</ol>
</div>
<div class="hidden"><form class="js-task-list-update" action="/Ugo-Nama/items/08c6a5f6a571335972d5" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="_method" value="patch" /><input type="hidden" name="authenticity_token" value="t2UGgVR+6+LcQ41xwNkUYAZsXNa5jb0M1A8uXGmT0ZBLynI9IBfuL+pFNp4Ah93Y2XgZdzWZjC3YMxV1RnXU2A==" /><input type="hidden" name="updated_at_confirmation_in_unixtime" id="updated_at_confirmation_in_unixtime" value="1489331288" class="js-task-list-updated-at" /><textarea name="raw_body" id="raw_body" class="js-task-list-field">
#はじめに
そもそもDQNが作りたかったわけじゃなくて、他の目的でChainerを使いたかったのでその練習にDQNを書いたんですが、せっかくだし~~公開しようと思いました~~ **公開しました** 。またどうせ公開するなら、この機会にこれ（Q学習+関数近似）関連で持っている知識をついでに整理しようと思ってまとめました。

　ニュース記事とかNatureとかNIPSの論文だけ読むと、DQN作ったDeepmind/Googleすげー！！！って感覚になりそうですが、強化学習的な歴史的経緯を考えると強化学習+深層学習になった、むしろかなり当然の成り行きで生まれた技術であることがわかります。（ATARIのゲームを人間以上のパフォーマンスでプレイするというのがビジュアル的にわかりやすかった$\leftrightarrow$問題設定が良かったというのもあります。）

　この記事ではNIPSとNatureの以下の２本の論文

・ V. Mnih *et al.*, &quot;Playing atari with deep reinforcement learning&quot;
　　http://arxiv.org/pdf/1312.5602.pdf
・ V. Mnih *et al.*, &quot;Human-level control through deep reinforcement learning&quot;
　　http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html

について、そこにいたるまでの強化学習の経緯をなるべく関係するところだけを取り上げて解説するつもりです。

　オタクな追っかけ的情報ですが、筆頭著者のMnihさんはUCTをはじめ確率的プランニングで有名なSzepesvári先生と、おなじみニューラルネットワークのゴッドファーザー、Hinton先生の元で学んだ人で、DQNが生まれたのは自然な流れといえるでしょう。

#ていうか、強化学習って？
強化学習自体、聞きなれない方もいるかもしれません。Bishop先生の&quot;PRML&quot;とか、最近だとMurphy先生の&quot;Machine Learning&quot;とかでも「本書では機械学習の多くの分野を扱う。ただし強化学習は除く。」のような扱いになっています。

　これは、強化学習の特殊な条件設定のためだと思います。というのも、強化学習の理論は制御理論（最適制御理論・動的計画法）と機械学習をシェイカーに入れて力いっぱいかき混ぜたものになっているからです。ここではそういった理論に立ち入ることなく、「強化学習って何するのよ？」という事をできるだけ簡単に説明します（するつもりです）。

##強化学習：環境とエージェント
　強化学習とは、&quot;環境&quot;中に置かれた&quot;エージェント&quot;が、環境との相互作用を通して最適な方策（行動を決定するきまり）を得るための機械学習の手法のことを言います。

　Wikipediaによると、「強化学習(きょうかがくしゅう, Reinforcement Learning)とは、ある環境内におけるエージェントが、現在の状態を観測し、取るべき行動を決定する問題を扱う機械学習の一種。」 [^1] だそうです。

　百聞は一見にしかずというやつで、模式図で表すと次の図のようになります。
![RL.png](https://qiita-image-store.s3.amazonaws.com/0/72529/dc77a4fe-85a1-a69e-e0e3-571e2e04a33f.png)

強化学習は次の単純なステップを繰り返すことで時間が進行していきます。

&gt;1. エージェントは環境から受け取った観測 $o$（あるいは直接、環境の状態 $s$ ）を受け取り、方策 $\pi$ に基いて環境に行動 $a$ を返す。
&gt;2. 環境はエージェントから受け取った行動 $a$ と現在の状態 $s$ に基いて、次の状態 $s&#39;$ に変化し、その遷移に基いて次の観測 $o&#39;$ と、報酬 $r$ と呼ばれる直前の行動の良し悪しを示す1つの数（スカラー量）をエージェントに返す。
&gt;3. 時間の進行：$t \leftarrow t + 1$

ここで $\leftarrow$ は代入操作を表します。報酬は通常、状態と行動、そして次の状態の関数で与えられます $r=r(s,a,s&#39;)$。我々は強化学習の条件から、環境を直接操作するといったことはできません。我々が自由に操作できるのは、あくまでエージェントのみです。このような環境とエージェントの相互作用、そして制約が、強化学習の置かれた特殊性を表しています。

　ここでは簡単のため、観測は環境の状態が直接エージェントに渡される状況（ $o=s$ ）についてのみ説明します [^2] 。強化学習の枠組みにおいてエージェントにとって最適な方策とは、その方策に従って行動を決定していくことで、現時点から無限の未来までに得ることのできる報酬の和

```math
R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} = r_{t+1} + \gamma\ r_{t+2} + \gamma^2\ r_{t+3} + \cdots
```

を最大化させるような方策であると考えます。すなわちこれが強化学習における目的関数です。$t$ は現時刻、$r_i$ は時刻 $i$ でエージェントが受け取る報酬を表します。$\gamma$ は割引率と呼ばれる強化学習のパラメータです。この値は通常 $0$ 以上 $1$ 未満の連続値で与えるもので、無限大・無限小を含まないどのような報酬設定に対しても $R_t$ が有限の値になるように導入される定数です。たいてい、$\gamma = 0.99$などといった$1$に近い値に設定します。

　今のように環境の状態がエージェントの観測として直接渡される（ $o=s$ ）場合は***必ず、行動を出力とするある決定論的な関数*** $\pi^* (s)$ ***で表すことが可能な最適方策が、少なくとも1つ存在する***ことが理論的に知られています[^3]。詳細は割愛しますが、以下の節で紹介する学習手法はいずれも上記の目的関数を最大化させる決定論的な方策を探す手法という点で同じものです。

　なおDQNがATARIゲームを学習する場合、4ステップ間のゲーム画面（観測）をもって状態が構成されると仮定して学習を行っています。

#Deep Q-Network出生以前
ここではDeep Q-Networkが生まれた背景について紹介していきます。Deep Q-Networkを構成するQ学習、関数近似、Experience Replay（経験再生）、さらにNeural Fitted Q Iterationについて説明します。

##Q学習
Q学習は強化学習の古典的なアルゴリズムで、強化学習のアルゴリズムでは最も広く知られた存在かもしれません。Q学習に基づく手法では、みな **最適行動価値関数** と呼ばれる関数を近似することで最適方策を学習します。

　最適行動価値は各状態と行動の組 $(s,a)$ にそれぞれ１つ存在し、「状態 $s$ で行動 $a$ を取り、残りはずっと最適方策に従ったとした場合に得る報酬の和（即ち目的関数） $R$の期待値」を表す値です。これを全ての各状態と行動の組 $(s,a)$について求めたものを最適行動価値関数と呼び、 $Q^* (s,a)$ で表します。

　ややこしい定義ですが、この値はざっくり言うと、エージェントが置かれた状態 $s$ で行動 $a$ を取る**メリット** を表します。この**最適行動価値関数に関して重要な点は、この関数と最適な決定論的方策（のうちの1つである） $ \pi ^* (s)$ との間には**

```math
\pi^*(s) = \rm{argmax}_a Q^* (s,a)  
```

**の関係があるということです。**すなわち、Q学習において [^4] 最適行動価値関数を得ることと最適方策を得ることは、ほぼ同じ意味を持つのです。

　実際のQ学習ではすべての状態 $s$ と行動の組 $a$ に対してテーブル関数 $Q(s,a)$ を作成し、すべての要素を任意の値に初期化した後、毎回のデータ ($s$, $a$, $r$, $s&#39;$) に対して次の式で更新します。

```math
Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \Bigl(r + \gamma \ \max_{a&#39;} Q(s&#39;,a&#39;)\Bigr)
```
ここで $r$ は状態 $s$ で行動 $a$ を選択したあと環境から受け取った報酬、$s&#39;$ は同様に状態 $s$ で行動 $a$ を選択した後、次の時刻で受け取った状態を表します。$\alpha$ は学習率を表します。通常は $1$ 未満の $0.1$ や $0.01$ といった小さい値を使います。このアルゴリズムは $Q(s,a)$ を、毎回その関数自身を含む $r + \gamma \ \max_{a&#39;} Q(s&#39;,a&#39;)$ に毎回少しづつ近づけていく形になっています[^5]。

　上の更新式の表記は（特にSutton &amp; Barto本で強化学習を知られた方には）あまり強化学習ではポピュラーではないかもしれませんが、この表記は価値関数 $Q(s,a)$ を何に当てはめようとしているのかを明示的に表す形式になっています。当然、この更新式はよりポピュラーな表記である

```math
Q(s,a) \leftarrow Q(s,a) + \alpha \Bigl(r + \gamma\ \max_{a&#39;} Q(s&#39;,a&#39;) - Q(s,a)\Bigr)
```
と等価です。

　Q学習の最大の特徴は、**($s$, $a$) の全ての組からサンプル ($s$, $a$, $r$, $s&#39;$) が無限回得られるとするなら、それらをどのような順番で与えたとしても上記のアルゴリズムで必ず最適な価値関数 $Q^*(s,a)$ が得られる**という点にあります[^6]。この特徴は後で紹介するExperience Replayで最大限に利用されます。　

　すべての状態と行動についてテーブル関数 $Q(s,a)$ を作成するとは言いましたが、例えばDQNのように画像列などのむちゃくちゃに高次元なものを状態として与えたりすると、当然テーブル関数ではとてもメモリに乗りませんし、すべてサンプルを取ることなんて出来ません。 $10\times 10$のバイナリ画像でも $2^{100} \approx 10^{30} $ といったとても手に負えない状態数になってしまいます。そこで、こういった非常に高次元の状態を扱う場合には $Q(s,a)$ に関数近似を使います。

##DeepじゃないQ-network：Q学習 ＋ 関数近似
というわけで、関数 $Q(s,a)$ に関数近似を使うことを考えます。そこで $Q(s,a)$ が何らかのパラメータ $\theta$ を用いて表されていて、近似された関数を $Q_\theta(s,a)$ で表すことにします。

　この関数を用いてQ学習に相当する関数を近似するには、古典的には以下の勾配法によるアルゴリズムを使います。

```math
\theta \leftarrow \theta - \alpha \nabla_\theta L_\theta
```

ここで $L_\theta$ は以下の誤差関数で表されます。このへんは一般的な、勾配法での更新とおんなじです。

```math
L_\theta = \frac{1}{2}\Bigl( \rm{target} - Q_\theta(s,a)\Bigr)^2
```

ところで、真の行動価値 $Q^* (s,a)$ がわかっていれば純粋な教師あり学習として $\rm{target} = Q^* (s,a)$ とすれば良いのですが、強化学習では教師信号にあたる $Q^* (s,a)$ は与えられません。そこでテーブル関数を用いたQ学習と同様に

```math
 \rm{target} = r + \gamma \max_{a&#39;} Q_\theta (s&#39;,a&#39;)
``` 

をその時点での教師信号として使います。そして誤差関数の微分は、あくまで $Q_\theta(s,a)$ を教師信号に近づけるためとして、教師信号に対する微分は行いません。つまり

```math
\begin{align}
\nabla_\theta L_\theta &amp;=-\Bigl(\rm{target} - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)\\
&amp;=-\Bigl(r + \gamma \max_{a&#39;} Q_\theta (s&#39;,a&#39;) - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)
\end{align}
```

となります。

　結果、近似関数を用いた場合のQ学習は以下のようになります。

```math
\theta \leftarrow \theta + \alpha \Bigl(r + \gamma \max_{a&#39;} Q_\theta (s&#39;,a&#39;) - Q_\theta(s,a)\Bigr) \nabla_\theta Q_\theta(s,a)
```
テーブル関数を用いたQ学習の２つ目の更新式にとても良く似た式が現れました。

　関数近似を用いたQ学習で気をつけなければいけないことは、**誤差関数の微分の計算において教師信号に含まれる $\max_{a&#39;} Q_\theta (s&#39;,a&#39;)$ を微分してはならない**ということです。自分で微分してコードに落とすようなときはこのような間違いはまず起こりませんが、最近広く使われる自動微分を利用すると、知らない間に教師信号まで含めて微分してしまっているという事が起こりえます（しかも、学習初期は両者おんなじような感じで動くので気付きにくい）。(自分も一度やりました ﾎﾞｿｯ)

　教師信号まで含めて微分して学習させてしまうと、ここで述べた関数近似器を用いたQ学習とは異なる性質を持つアルゴリズムになってしまいます。

###関数近似にニューラルネットワークを使う
多層のニューラルネットワークのような非線形関数近似手法を用いたQ学習は、誤差逆伝播法が開発された第二次ニューラルネットワークの時代、強化学習が理論的に成熟し始めた時期に、Long-Ji Linさんという方が精力的に行いました。Experience Replayはこの方が論文&quot;Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching &quot; で公開した手法です。

　余談ですが、この論文は複雑でダイナミックなシミュレーション環境中で、なんとか生き残ろうとするエージェントを学習させるというとても面白い論文になっています。

　多層（といっても全結合の3層か4層程度）ニューラルネットワークではどうしても学習が遅く、また学習率を大きくしても学習が発散してしまいます。これをなんとか高速化する方法として、上記で述べたQ学習の特性を利用します。

　Experience Replayとはエージェントが経験したサンプル ($s$, $a$, $r$, $s&#39;$）を全て（あるいは有限の時間数ぶん）記録しておき、一度つかったサンプルを上記の関数近似を用いたQ学習で何度も利用するという手法です。これは、テーブル関数を用いたQ学習ではサンプルの順番をどのように並べ替えても学習は進行するので、関数近似器を使ったQ学習でも同様の性質を期待するという考え方になっています。

　Experience Replayのある種の極限を考えるようなアルゴリズムが、Riedmiller先生のNeural Fitted Q Iteration [^7]です。これはここまでの「自分でデータを集めて、学習する」といったオンラインの強化学習とは異なり、まずサンプル ($s$, $a$, $r$, $s&#39;$）からなる十分な数のデータがあるものとして、「これ以上サンプルを追加せず、与えられたデータから最適方策を学習する」、すなわちバッチ強化学習を行うことを考えるアルゴリズムです。

　Neural Fitted Q Iterationのアルゴリズムの擬似コードは以下のようになります。


&gt;input: 多数のサンプル（$s$, $a$, $r$, $s&#39;$）からなるデータ $D$
output: 学習後のQ関数 $Q_{\theta_N} (s,a)$ 
1. $k=0$ 
2. ニューラルネットワークのパラメータ $\theta_0$ を初期化する
3. 繰り返し： $N-1$ 回繰り返す
4. 　　　　データ $D$ からサンプルを $M$ 個取り出す
5.　　　　 $M$個のサンプルを元に、
　　　　　　　  教師信号 $ {\rm{target}}^t = r_t + \gamma \max_{a&#39;} Q_{\theta_k} (s_t&#39;,a&#39;) $ と
　　 　　　　　入力 $ (s_t, a_t) $ を作成する（ $t=1,2,...,M$ ） 
6.　　　　 作成した教師信号 ${\rm{target}}^t$ と入力$ (s_t, a_t) $を元に、
　　　　　 $Q_\theta (s, a)$ と教師信号との誤差$L_\theta$ を何らかの勾配法で最小化
　　　　　 する。学習が収束するなり一定の条件を満たしたら、その結果を
 　　　　　 $\theta_{k+1}$とする
7.　　　　 $k \leftarrow k+1$とする
8. $Q_{\theta_N} (s,a)$ を返す

Neural Fitted Q Iterationで重要な点は、繰り返し中の各回に行われる勾配法を使った最適化において、教師信号は常に同じパラメータ $\theta_k$ に基づく $r+\max_{a&#39;} Q_{\theta_k} (s_t&#39;,a&#39;) $ から生成されていて、そのためオンライン手法とは異なり、**各最適化中では完全に教師あり学習になっている**ところです。そのため、設定的には非常に学習が安定したものとなると期待されます。

　Neural Fitted Q Iterationはデータ数が一定として、そこから情報を引き出すアルゴリズムですが、さらに発展的な手法で、ある程度学習を行った後にエージェントを環境中で動かし、バッチのデータを後から追加していくGrowing Batchという考え方が提案されています [^8] 。ここまでくるとほとんど手法的にはNature論文のものと変わらないです。

　Riedmiller先生のチームはこのバッチ強化学習を主な手法として、ロボットの行動学習などに適用しています。先生もDeepmindに籍をおいているようで、驚くほど多くの強化学習研究者がDeepmindに集積されていることが伺えます。


#そしてDeep Q-Network：Q学習 ＋ 深層学習
　「DQNの何が新しい技術なのか？」という問いは、「以上の強化学習の関数近似に深層学習の技術を適用した」という点に尽きます。


　具体的には、NIPSとNatureの両方での論文で畳み込みニューラルネットワークが用いられていますが、この構造を使用したこと。それと近年開発されたRMSPropを始め、局所解にはまらない最適化手法を適用したこと。これらが本質的にDQNにおいて新しい点と言えます。
（追記：ネットワークに対して状態のみを入力として、その状態における全ての行動価値の推定値を一回のforward伝搬で表現するDQNの構造は新しいものだと思うのですが、~~すみません確信が持てません。~~）
（追記:$\uparrow$の追記で紹介したDQNの構造ですが，先例がありました。遅くとも2009年の時点で柴田\&amp;河野が同様の構造を持った5層の全結合ニューラルネットワークを用いてQ学習を構成し、画像入力を用いた強化学習をAIBOの行動学習タスクに適用しています[^9]。）

　DQNが最初に現れたNIPSでの論文&quot;Playing atari with deep reinforcement learning&quot;ではExperience Replayをミニバッチ手法と組み合わせて大規模に使っています。しかしこれでもなお、オンライン強化学習アルゴリズムはまだ不安定なところがあります。

　一方、Natureでの論文&quot;Human-level control through deep reinforcement learning&quot;ではNeural Fitted Q Iteration ＋ Growing Batchを適用することで、オンライン「ほとんどバッチ」強化学習による安定化が図られています。

　私の知っている、DQNの生い立ちはこんなところです。

#DQNをPython + Chainerで書いたよ（コード公開しました）
というわけでDQNをChainerの練習に実装しました。githubで~~公開予定です。~~
$\rightarrow$ **公開しました。** 
https://github.com/ugo-nama-kun/DQN-chainer

　**追記１**：公開したコードに関してnature/nipsバージョンの両方を公開しました。テストには、強化学習研究でベンチマークなどを行う際によく用いられるRLglueとRLglue Python-codec、ATARIエミュレータとRLglueをつなぐArcade Learning Environment、PythonパッケージにはNumpy、Scipy、そしてモチロンChainerが必要です。GPUでの計算を想定しているので、NVIDIAのGPUが必要です。Ubuntu 14.04LTSでのみ動作確認しています。

　**追記２**：DQNのExperience Replay用に保存する履歴数を、オリジナルの1/10に設定しています（ $ 10^6 \rightarrow 10^5$ ）。これは単純に、私の小規模なマシンでも動くように規模縮小したかったからです。今回の履歴の実装は、Nature誌中に記載されているアルゴリズムを愚直に実装したものなので非効率的な面があると思います。少なくともPongでは問題なく学習が行われることを確認しています。

　**追記3**：動作確認は全てATARIゲームのうち、Pongで行っています。他のゲームで学習させる方法はreadme.txtを参照してください。規模縮小しているため、想定されるパフォーマンスが学習されないかもしれません。


　NIPSバージョンとNatureバージョンの両方を作成しましたが、NIPSバージョンは論文中に不明なパラメータも多く、収束の雰囲気はNatureバージョンのほうがよろしい感じです。NIPSバージョンはおまけ程度と考えてください。

Natureバージョンでの学習結果はyoutubeに載せてます $\downarrow$
https://www.youtube.com/watch?v=N813o-Xb6S8

追記：NIPSバージョンの学習もうまくいったので載せました（結果の挙動は殆ど変わりませんが） $\downarrow$
https://www.youtube.com/watch?v=ez_JEHMUpng

　Chainerを使った感想ですが[^10]、一度TheanoでもDQNを実装してyoutubeに動画を公開をしてみた経験から言うと、ChainerはTheanoベースのものに比べてエラーがどこで発生しているか本当にわかりやすくなっています。かなりの自由度もあるので、ニューラルネットワークのスタンダードなコンポーネントを組み合わせて使うような（畳み込みニューラルネットワークを使いたいとか、層を積んでいきたいとか）使い方ならば断然オススメできます。

　一方でTheanoは、なにか新しいアルゴリズムを試したいとき、ほぼそれがどのようなものでも同じような操作でGPUでの高速化が可能な点がやはり魅力です。例えば「LSTMセルのようなでも別の、もっとイイものを新たに開発してやるぜっ！それをガッツリ最適化するぜ！」って時にはTheanoのような一般的なものを使う必要がでてくる感じがします。こうなるとほとんど先端研究とか学術的（でさらに特殊）な場面で使う感じになりそうですね。以上感想でした。

#Tips：DQNという命名は何故起こったのか？
この話は完全に蛇足になります。

　ヤンキー的な人をDQNと呼ぶほうがどちらかと言うと状況的にまれな現象だと思うのですが、ともかく今回のアルゴリズムがDeep Q-Network = DQNになってマッチングしてしまったのも、結構奇跡的な感じがします。

　それぞれのD-Q-Nのコンポーネントを掘り起こすとDは深層学習（Deep learning）、QはQ学習、Nはニューラルネットワーク（Neural Network）になります。

　まずDeepは昨今の流れ的によしとして（このDeepナントカという呼び方は恐らくHinton先生たちの論文&quot;A fast learning algorithm for deep belief nets&quot;以降から爆発的に増えたという見方がされています。）、Q学習のQは何なんでしょう。強化学習の生みの親のひとり、Barto先生のコメントによると、この&quot;Q&quot;は&quot;Quality&quot;のQ（と思ってるよ）ということらしいです [^11] 。

　つぎに、Q-Networkという呼び方はあまり強化学習で一般的でない印象です。論文中でも、だいたい「Q学習で関数近似器にニューラルネットワークを使った」といった言い方がなされている気がします。近い表現は先ほどのLinさんが1993年の論文でQ-netという呼び方をしていて、恐らくこれが今回Q-Networkと呼ばれるようになった源流ではないかと思います。



#注：
[^1]: 強化学習 in Wikipedia&lt;br&gt;https://ja.wikipedia.org/wiki/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92

[^2]: このような状況（本当はもう少し色々仮定がありますが）を、「環境はマルコフ決定過程（Markov Decision Process, MDP）で記述できるものと仮定する」などと言います。状態が直接エージェントに渡されない場合は部分観測マルコフ決定過程（Partially Observable Markov Decision Process, POMDP）と呼ばれ、このような環境中での強化学習は近年ではPartially Observable Reinforcement Learning（PORL）などと呼ばれます。ATARIゲームを学習する設定はゲーム画面のみをエージェントの入力としているので当然PORLに含まれますが、4ステップ間のゲーム画面をもって状態が構成されると仮定して学習を行っています。

[^3]: この点に関して詳しくは M. Putermanの &quot;Markov decision processes&quot; や O . Sigaud, O. Buffetの &quot;Markov decision processes in artificial intelligence&quot; などを参照してください。

[^4]: より正確には、「マルコフ決定過程で記述できる環境中において」

[^5]: アルゴリズムの完全な形は次のサイトを参照してください： &lt;br&gt;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html

[^6]: 理論的には、学習率 $\alpha$ がアップデートごとに徐々に減衰していく必要があります。ただし強化学習アルゴリズムを長期間回そうという時や動的な環境にも常にある程度適応させたいといった状況には、あまり現実的ではありません。収束条件に関して詳しくはMelo先生の「Convergence of Q-learning: a simple proof」を参照：&lt;br&gt;http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf

[^7]: Martin Riedmiller, &quot;Neural fitted Q iteration–first experiences with a data efficient neural reinforcement learning method&quot;.&lt;br&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.72.1193&amp;rep=rep1&amp;type=pdf

[^8]: Sascha Lange, Thomas Gabel, and Martin Riedmiller, &quot;Batch reinforcement learning&quot;.&lt;br&gt;http://ml.informatik.uni-freiburg.de/_media/publications/langegabelriedmiller2011chapter.pdf

[^9]: Katsunari Shibata and Tomohiko Kawano: Learning of Action Generation from Raw Camera Images in a Real-World-like Environment by Simple Coupling of Reinforcement Learning and a Neural Network, Advances in Neuro-Information Processing, Lecture Notes in Computer Science, Proc. of ICONIP (Int&#39;l Conf. on Neural Information Processing) 08, Vol. 5506, pp. 755--762, 55060755.pdf (CD-ROM), 2009

[^10]: Chainerは公開後も頻繁にアップデートされて新機能が追加されているので、あくまで現在（2015, 7月）の所感です。

[^11]: ちょっと情報源がマユツバですが：&lt;br&gt;http://www.quora.com/How-was-the-term-Q-learning-coined
</textarea><input type="submit" name="commit" value="Save changes" data-disable-with="Save changes" /></form></div></section></div><div class="col-sm-3"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた by @movingsloth on @Qiita" data-url="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた" href="http://b.hatena.ne.jp/entry/http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div><section class="itemsShowAuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><a href="/Ugo-Nama"><img alt="" class="itemsShowAuthorInfo_userIcon" itemprop="image" src="https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182" /></a><div class="itemsShowAuthorInfo_profileStats"><strong class="itemsShowAuthorInfo_userName" itemprop="name"><a itemprop="url" href="/Ugo-Nama">Ugo-Nama</a></strong><div class="itemsShowAuthorInfo_contribution"><span class="itemsShowAuthorInfo_count">934</span><span class="itemsShowAuthorInfo_unit">Contribution</span></div><div id="js-react-on-rails-context" data-rails-context="{}"></div>
<div class="js-react-on-rails-component" data-component-name="UserFollowButton" data-props="{&quot;initial_followed_by&quot;:false,&quot;position&quot;:&quot;author-info&quot;,&quot;size&quot;:&quot;btn-xs&quot;,&quot;url_name&quot;:&quot;Ugo-Nama&quot;}" data-trace="false" data-dom-id="UserFollowButton-react-component-5485a6a9-0556-47d3-b769-5177248f1243"></div>
    <div id="UserFollowButton-react-component-5485a6a9-0556-47d3-b769-5177248f1243"></div>
    
</div><section class="itemsShowAuthorPopularItems"><h5 class="itemsShowAuthorPopularItems_sectionTitle">人気の投稿</h5><ul class="itemsShowAuthorPopularItems_posts list-unstyled"><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Ugo-Nama/items/08c6a5f6a571335972d5">DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた</a></li><li itemscope="" itemtype="http://schema.org/Article"> <a itemprop="url" track="click" data-label="AuthorPopularItemsAtSidebar" href="/Ugo-Nama/items/04814a13c9ea84978a4c">誤差逆伝播法のノート</a></li></ul></section></section><div class="scroll-chaser"><div class="google-adsense"><style>.test-text-responsible { width: 200px; height: 200px; }@media(min-width: 1200px) {  .test-text-responsible { width: 250px; height: 250px; }}@media(max-width: 979px) and (min-width: 768px) {  .test-text-responsible { width: 120px; height: 240px; }}@media(max-width: 767px) {  .test-text-responsible { width: 320px; height: 50px; }}</style><script async="" src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><ins class="adsbygoogle test-text-responsible" data-ad-client="ca-pub-8127218772604357" data-ad-slot="3880091879" style="display:inline-block"></ins><script>(adsbygoogle = window.adsbygoogle || []).push({});</script></div></div><div class="js-react-on-rails-component" data-component-name="Toc" data-props="{&quot;body&quot;:&quot;\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\&quot;\u003eはじめに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%A6%E3%81%84%E3%81%86%E3%81%8B%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E3%81%A3%E3%81%A6\&quot;\u003eていうか、強化学習って？\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92%E7%92%B0%E5%A2%83%E3%81%A8%E3%82%A8%E3%83%BC%E3%82%B8%E3%82%A7%E3%83%B3%E3%83%88\&quot;\u003e強化学習：環境とエージェント\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep-q-network%E5%87%BA%E7%94%9F%E4%BB%A5%E5%89%8D\&quot;\u003eDeep Q-Network出生以前\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#q%E5%AD%A6%E7%BF%92\&quot;\u003eQ学習\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#deep%E3%81%98%E3%82%83%E3%81%AA%E3%81%84q-networkq%E5%AD%A6%E7%BF%92--%E9%96%A2%E6%95%B0%E8%BF%91%E4%BC%BC\&quot;\u003eDeepじゃないQ-network：Q学習 ＋ 関数近似\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E9%96%A2%E6%95%B0%E8%BF%91%E4%BC%BC%E3%81%AB%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%82%92%E4%BD%BF%E3%81%86\&quot;\u003e関数近似にニューラルネットワークを使う\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E3%81%9D%E3%81%97%E3%81%A6deep-q-networkq%E5%AD%A6%E7%BF%92--%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92\&quot;\u003eそしてDeep Q-Network：Q学習 ＋ 深層学習\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#dqn%E3%82%92python--chainer%E3%81%A7%E6%9B%B8%E3%81%84%E3%81%9F%E3%82%88%E3%82%B3%E3%83%BC%E3%83%89%E5%85%AC%E9%96%8B%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F\&quot;\u003eDQNをPython + Chainerで書いたよ（コード公開しました）\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#tipsdqn%E3%81%A8%E3%81%84%E3%81%86%E5%91%BD%E5%90%8D%E3%81%AF%E4%BD%95%E6%95%85%E8%B5%B7%E3%81%93%E3%81%A3%E3%81%9F%E3%81%AE%E3%81%8B\&quot;\u003eTips：DQNという命名は何故起こったのか？\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\&quot;#%E6%B3%A8\&quot;\u003e注：\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n&quot;,&quot;wrapper&quot;:&quot;#article-body-wrapper&quot;}" data-trace="false" data-dom-id="Toc-react-component-737d0fb6-f9a3-4ec6-858b-83eb181b9331"></div>
    <div id="Toc-react-component-737d0fb6-f9a3-4ec6-858b-83eb181b9331"></div>
    
</div></div><div class="row"><div class="col-sm-9"><div class="ArticleFooter__menu"><div class="s-flex-align-center"><div class="js-likebutton" data-props="{&quot;like_status&quot;:false,&quot;like_count&quot;:595,&quot;show_count&quot;:true,&quot;uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable_type&quot;:&quot;Article&quot;,&quot;position&quot;:&quot;article-footer&quot;}"></div><div class="ArticleFooter__userList"><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="neka-nat@github"><a itemprop="url" href="/neka-nat@github"><img alt="neka-nat@github" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/14516/profile-images/1473683376" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="menphim"><a itemprop="url" href="/menphim"><img alt="menphim" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/55217/profile-images/1473693620" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="pyr_revs"><a itemprop="url" href="/pyr_revs"><img alt="pyr_revs" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/69525/profile-images/1473698177" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="tokuhisa_f"><a itemprop="url" href="/tokuhisa_f"><img alt="tokuhisa_f" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/18292/profile-images/1473682394" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="st450"><a itemprop="url" href="/st450"><img alt="st450" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63970/profile-images/1473696432" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="sotetsuk"><a itemprop="url" href="/sotetsuk"><img alt="sotetsuk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/35293/profile-images/1473686670" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ixixi"><a itemprop="url" href="/ixixi"><img alt="ixixi" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/8954/profile-images/1473681204" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="ssssssk"><a itemprop="url" href="/ssssssk"><img alt="ssssssk" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/80548/profile-images/1473701825" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="mero"><a itemprop="url" href="/mero"><img alt="mero" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/63104/profile-images/1473696161" /></a></div></div><div class="ArticleFooter__user"><div class="js-hovercard" data-hovercard-target-name="hiro_matsuno2"><a itemprop="url" href="/hiro_matsuno2"><img alt="hiro_matsuno2" class="thumb thumb--xs" src="https://qiita-image-store.s3.amazonaws.com/0/9764/profile-images/1473681543" /></a></div></div><div class="ArticleFooter__user"><a href="/Ugo-Nama/items/08c6a5f6a571335972d5/likers"><span class="fa fa-ellipsis-h"></span></a></div></div></div><div class="u-flex u-align-center"><div class="ArticleFooter__stock"><div class="js-stockbutton" data-position="footer_menu" data-props="{&quot;stock_status&quot;:false}"></div></div><div class="ArticleFooter__editRequest"><a class="u-link-no-underline" data-toggle="tooltip" title="You can propose improvements about the article to the author 💪" href="/drafts/08c6a5f6a571335972d5/edit"><span class="fa fa-send-o fa-lg"></span> <span>Edit request</span></a></div><div class="dropdown ArticleFooter__dropdown"><a class="dropdown-toggle" data-toggle="dropdown" href="#"><span class="fa fa-ellipsis-h"></span></a><ul class="dropdown-menu dropdown-menu-right"><li><a href="/Ugo-Nama/items/08c6a5f6a571335972d5.md"><span class="fa fa-fw fa-file-text-o"></span> Show article as Markdown</a></li><li><a data-target=".js-report-form" data-toggle="modal" href="#"><i class="fa fa-fw fa-flag"></i> Report article</a></li></ul></div></div></div><ul class="references js-referencesView"><li class="references_header"><i class="fa fa-fw fa-link"></i> Linked from these articles</li><a class="references_toggleOldReferences js-toggleOldReferences" href="#"><i class="fa fa-expand js-toggleOldReferencesIcon"></i><span class="js-toggleOldReferencesText">Show old 18 links</span></a><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/mokemokechicken/items/81cea0ea2400451acf0d#_reference-024b9f9691569fe22ade"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/11124/profile-images/1473681925" />Chainerで機械学習と戯れる：加算と減算を同時に学習できるか？</a><time class="references_datetime js-dateTimeView" datetime="2015-07-12T04:11:59+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/mokemokechicken/items/f1b71170d332c0543db8#_reference-6ea53a63964c890cb29e"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/11124/profile-images/1473681925" />機械学習と戯れる：足し算ゲームを強化学習で学習できるか？</a><time class="references_datetime js-dateTimeView" datetime="2015-07-13T03:25:41+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/mokemokechicken/items/aa1535696514bfe51329#_reference-602b53ec7d9d7eb2c656"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/11124/profile-images/1473681925" />Chainerで機械学習と戯れる： 足し算ゲームをChainerを使って強化学習できるか？</a><time class="references_datetime js-dateTimeView" datetime="2015-07-15T15:27:48+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/mokemokechicken/items/23dd55aae4c22884277a#_reference-2f789be5e9b87df46bfd"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/11124/profile-images/1473681925" />Chainerで機械学習と戯れる： Jumpで穴を飛び越えろゲーム（自作）を強化学習できるか？</a><time class="references_datetime js-dateTimeView" datetime="2015-07-19T09:53:54+00:00">over 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/imenurok/items/c6aa868107091cfa509c#_reference-27ced69453b3651fbdb4"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/102946/profile-images/1473708857" />東方ProjectをDeepLearningで攻略した…かった。</a><time class="references_datetime js-dateTimeView" datetime="2016-02-17T07:36:35+00:00">about 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/_329_/items/067c671d4919dd3fa8d2#_reference-e36508b8bd6ffd9a8d61"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/87943/profile-images/1473704263" />ちょいワルQ-Networkの話</a><time class="references_datetime js-dateTimeView" datetime="2016-02-23T11:12:03+00:00">about 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/dsanno/items/1a80471690367a81c6e7#_reference-df7859b892263ef2720c"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/58026/profile-images/1473694517" />DQNプニキにホームランを打たせたい</a><time class="references_datetime js-dateTimeView" datetime="2016-02-24T13:03:58+00:00">about 1 year ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/ashitani/items/bb393e24c20e83e54577#_reference-ce560049f69bbf954ef2"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/30340/profile-images/1473685480" />倒立振子で学ぶ DQN (Deep Q Network)</a><time class="references_datetime js-dateTimeView" datetime="2016-03-27T01:17:03+00:00">12 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/yai/items/f09b681f6a2d263ee6d6#_reference-146c139f117bb1974769"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/117826/profile-images/1473713721" />TensorFlowでDQNを実装（したかった・・・）</a><time class="references_datetime js-dateTimeView" datetime="2016-04-25T13:55:21+00:00">11 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/icoxfog417/items/242439ecd1a477ece312#_reference-935258c7be6fd79ea0b3"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/25990/profile-images/1484303516" />ゼロからDeepまで学ぶ強化学習</a><time class="references_datetime js-dateTimeView" datetime="2016-06-07T08:03:42+00:00">9 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/yuishihara/items/0e530e9c0a17a7fa0111#_reference-1f79221dfb5fabd30d31"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43125/profile-images/1473689418" />いまさらだけどTensorFlowでDQN(不完全版)を実装する</a><time class="references_datetime js-dateTimeView" datetime="2016-07-24T12:09:42+00:00">8 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/HirofumiYashima/items/16da52eaea74e55d75da#_reference-bf5eeb52f22f59f4a203"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43487/profile-images/1473689546" />深層強化学習モデルで、Atari2600ゲームを実行する環境構築とモデル ～データセットDL元URLと、DQN独自実装モデル 事例２件の紹介（muupannさん と mhausknさん）</a><time class="references_datetime js-dateTimeView" datetime="2016-07-28T02:42:33+00:00">8 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/trtd56/items/3a09d37788d8d13ff131#_reference-bd65a7343e6c75e0c3a0"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/91517/profile-images/1473705329" />Chainer✕OpenAI GymでDQN(もどき)に挑戦！</a><time class="references_datetime js-dateTimeView" datetime="2016-07-30T06:55:08+00:00">8 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/eve_yk/items/2ace6d4c1dad7e912df1#_reference-3eab8d63c5835bb56ae6"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/110468/profile-images/1473711224" />強化学習で参考になったサイトまとめ</a><time class="references_datetime js-dateTimeView" datetime="2016-09-10T12:17:02+00:00">6 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/okdshin/items/c486da9c3e563147b0ff#_reference-ceffbeeed5217ce9ab25"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/12504/profile-images/1479967930" />外部メモリ（External Memory）を利用した強化学習</a><time class="references_datetime js-dateTimeView" datetime="2016-11-03T10:44:42+00:00">4 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/t-ae/items/daa9fcf9286a10a19c89#_reference-167f15a8a4dab6b2794d"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/121603/profile-images/1473714961" />Keras+DQNでリバーシのAI書く</a><time class="references_datetime js-dateTimeView" datetime="2016-12-27T12:39:26+00:00">3 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/ryo_grid/items/af60750659d1d7ffeef9#_reference-159542a72cd686e383e2"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/12325/profile-images/1473682364" />遺伝的アルゴリズムxニューラルネットワーク（NeuroEvolution）でOpen AI Gymの二足歩行ロボを歩かせた話</a><time class="references_datetime js-dateTimeView" datetime="2017-01-06T06:37:39+00:00">2 months ago</time></li><li class="references_reference js-reference js-oldReference"><span>Linked from </span><a href="/oggata/items/0f95551a88ae314aa8cf#_reference-01c0b16ecfbbdbbf3cd9"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/43766/profile-images/1473689641" />Deep-Q-Network(ConvNetJS)で簡単なカードデッキ構築ゲームを学習させてみる</a><time class="references_datetime js-dateTimeView" datetime="2017-01-06T16:43:10+00:00">2 months ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/TSY/items/3aab949ec4c3b1098af0#_reference-8b029d87f31079412fed"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/59997/profile-images/1486911502" />CPUで気軽に強化学習してみた話(DeepMind A3C)</a><time class="references_datetime js-dateTimeView" datetime="2017-02-10T09:03:53+00:00">about 1 month ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/yukiB/items/0a3faa759ca5561e12f8#_reference-3e09eabe27c5f6695584"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/59864/profile-images/1473695058" />[Python]強化学習(DQN)を実装しながらKerasに慣れる</a><time class="references_datetime js-dateTimeView" datetime="2017-02-17T02:43:48+00:00">27 days ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/uezo/items/87b25c93199d72a56a9a#_reference-7627be3519db3d0a184c"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/53296/profile-images/1473693016" />ChainerRLで三目並べを深層強化学習（Double DQN）してみた</a><time class="references_datetime js-dateTimeView" datetime="2017-02-26T01:09:07+00:00">18 days ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/tsunaki/items/ec2556dc147c6d263b9d#_reference-c219d612286c4b151043"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/52867/profile-images/1479168990" />[How to!] スーパーマリオをTensorflowで学習＆プレイしよう!!</a><time class="references_datetime js-dateTimeView" datetime="2017-03-02T21:45:52+00:00">13 days ago</time></li><li class="references_reference js-reference"><span>Linked from </span><a href="/7of9/items/adcc7be67098e1579435#_reference-25161b7741af7cecbb50"><img alt="" width="18" height="18" src="https://qiita-image-store.s3.amazonaws.com/0/32870/profile-images/1473685996" />link &gt; Q-learning with 2D actions and 2D states / Q学習</a><time class="references_datetime js-dateTimeView" datetime="2017-03-11T23:26:54+00:00">4 days ago</time></li></ul><div class="itemsShowBody_articleColumnFooter"><div class="socialButtons"><div class="socialButtons_twitter"><a class="twitter-share-button" data-text="DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた by @movingsloth on @Qiita" data-url="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" href="https://twitter.com/share">Tweet</a></div><div class="socialButtons_hatebu"><a class="hatena-bookmark-button" data-hatena-bookmark-layout="simple-balloon" data-hatena-bookmark-title="DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた" href="http://b.hatena.ne.jp/entry/http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" title="Add to Hatena Bookmark"><img alt="Add to Hatena Bookmark" height="20" src="https://b.st-hatena.com/images/entry-button/button-only@2x.png" style="border: none;" width="20" /></a><script async="async" charset="utf-8" src="https://b.st-hatena.com/js/bookmark_button.js" type="text/javascript"></script></div><div class="socialButtons_googlePlus"><div class="g-plusone" data-href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" data-size="medium"></div></div><div class="socialButtons_facebook"><div class="fb-like" data-action="like" data-href="http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5" data-layout="button_count" data-share="false" data-show-faces="false"></div></div><div class="socialButtons_pocket"><a class="pocket-btn" data-lang="en" data-pocket-count="horizontal" data-pocket-label="pocket"></a></div></div></div><div class="itemsShowComment_wrapper" id="comments"><div class="js-react-on-rails-component" data-component-name="CommentListContainer" data-props="{&quot;currentUser&quot;:null,&quot;initialComments&quot;:[{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eDQN-Chainerを改造して楽しませて頂いております\u003c/p\u003e\n\n\u003cp\u003e絵を見せて行動させるコードを書くだけで,それなりにDQNで機械学習気分が楽しめています\u003cbr\u003e\nが,上手く学習させるのは,やっぱりそれなりに難しいですね......\u003cbr\u003e\nあまり上手く行ってない部分に,少々相談に乗って頂けないかと考えています\u003c/p\u003e\n\n\u003cp\u003e簡単な所で,２つ学習させてみました\u003cbr\u003e\n一つは障害物を避ける\u003cbr\u003e\nもう一つは目標に到達する\u003cbr\u003e\nrewardが違うだけでコードはほぼ同じです\u003c/p\u003e\n\n\u003cp\u003eしかし,目標に到達させる学習があまり上手く行きません\u003cbr\u003e\n色々rewardを変化させたりするのですが簡単には以下です\u003cbr\u003e\n目標に到達で+100 とEpisodeEndとする\u003cbr\u003e\n前stepより目標との距離が近づいた+1\u003cbr\u003e\n前stepより目標との距離が遠くなった-1\u003cbr\u003e\n他\u003cbr\u003e\n自分中心で周囲のオブジェクトを描く,目標物は視界から外れたらその方角を示すマークを画面の周囲に描く\u003cbr\u003e\n自分中心でフェースアップの絵を見せるケースでは,移動は速度ベクトルを持ち,旋回を行う\u003cbr\u003e\n自分中心でノースアップの絵を見せるケースでは,移動は前後左右の任意に一定距離を移動\u003cbr\u003e\n目標物は50step程度までで到達できる位置に配置\u003cbr\u003e\nほとんど学習の効果がないような真っ直ぐ進む(selectionの一番目の行為)を続けるだけになってしまいます\u003c/p\u003e\n\n\u003cp\u003e障害物を避けるのは簡単な迷路に放り込んで放浪させています\u003cbr\u003e\n完全に回避が出来ている訳ではありませんが,それなりの期間を避け続けています\u003c/p\u003e\n\n\u003cp\u003erewardの与え方がおかしいのでしょうか(-1から1に正規化すべき?)\u003cbr\u003e\n目標到達の学習は学習バッファ内で成功体験が少なくて難しいのでしょうか(目標の遠近でrewardは与えてみたのですが...)\u003cbr\u003e\n漠然とお尋ねするだけで,ソースもお見せしないまま,突然の助言のお願いをするのは申し訳なく思いますが,よろしくお願いします\u003c/p\u003e\n\n\u003cp\u003eP.S.pyCON,休みが取れたら是非行ってみたく思っています\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-08-30T23:04:35+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:323071,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;DQN-Chainerを改造して楽しませて頂いております\n\n絵を見せて行動させるコードを書くだけで,それなりにDQNで機械学習気分が楽しめています\nが,上手く学習させるのは,やっぱりそれなりに難しいですね......\nあまり上手く行ってない部分に,少々相談に乗って頂けないかと考えています\n\n簡単な所で,２つ学習させてみました\n一つは障害物を避ける\nもう一つは目標に到達する\nrewardが違うだけでコードはほぼ同じです\n\nしかし,目標に到達させる学習があまり上手く行きません\n色々rewardを変化させたりするのですが簡単には以下です\n目標に到達で+100 とEpisodeEndとする\n前stepより目標との距離が近づいた+1\n前stepより目標との距離が遠くなった-1\n他\n自分中心で周囲のオブジェクトを描く,目標物は視界から外れたらその方角を示すマークを画面の周囲に描く\n自分中心でフェースアップの絵を見せるケースでは,移動は速度ベクトルを持ち,旋回を行う\n自分中心でノースアップの絵を見せるケースでは,移動は前後左右の任意に一定距離を移動\n目標物は50step程度までで到達できる位置に配置\nほとんど学習の効果がないような真っ直ぐ進む(selectionの一番目の行為)を続けるだけになってしまいます\n\n障害物を避けるのは簡単な迷路に放り込んで放浪させています\n完全に回避が出来ている訳ではありませんが,それなりの期間を避け続けています\n\nrewardの与え方がおかしいのでしょうか(-1から1に正規化すべき?)\n目標到達の学習は学習バッファ内で成功体験が少なくて難しいのでしょうか(目標の遠近でrewardは与えてみたのですが...)\n漠然とお尋ねするだけで,ソースもお見せしないまま,突然の助言のお願いをするのは申し訳なく思いますが,よろしくお願いします\n\nP.S.pyCON,休みが取れたら是非行ってみたく思っています\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-25f667fe11d9af2c785b&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-08-30T22:17:26+09:00&quot;,&quot;id&quot;:91643,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91643/profile-images/1473705372&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;tsujimo&quot;},&quot;uuid&quot;:&quot;25f667fe11d9af2c785b&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e興味を持って頂いてありがとうございます．\u003c/p\u003e\n\n\u003cp\u003eDeep Reinforcement Learning (以下，DRL)は他の一般物体認識みたいな話題と比べても，\u003cbr\u003e\nまだまだ発展途上の分野なので（教師あり学習とは違って，収束するかどうかさえ怪しい），\u003cbr\u003e\nこれが正解！といった言い方はできないのが正直なところです．\u003cbr\u003e\nなので，思いつく改善点を言うにとどめます．\u003c/p\u003e\n\n\u003cp\u003eおそらくALEの代わりにRLGlueのenvironmentを新たに書かれて，それで実験を行われているようですね．\u003cbr\u003e\nそして2種類の実験をされている．\u003c/p\u003e\n\n\u003cp\u003e1つ目のタスクは目標の到達\u003cbr\u003e\n2つ目のタスクは障害物の回避\u003c/p\u003e\n\n\u003cp\u003eそして状況はさらに2つある．\u003cbr\u003e\n1つ目の状況はロボットにカメラがついていて，カメラ画像をもとに動き回っているような状況と考えればいいんでしょうか？\u003cbr\u003e\n2つ目の状況はロボットを天井のカメラから眺めていて，そのカメラ画像をもとにロボットを動かしている感じですかね．\u003c/p\u003e\n\n\u003cp\u003eもし勘違いしていたら申し訳ないです．\u003c/p\u003e\n\n\u003cp\u003eまず，reward設定からお話すると，お考えの通り+100は結構な大きさなので報酬は+1か-1に正規化したほうが良さそうというのが今のところの(DRLの)状況みたいです．また，近づいたら正の定数，遠くなったら負の定数といった設定は確かに強化学習で最適経路探索させる時に使われることがあるものですが，最適価値関数の値（つまり，報酬が常に+1になるような場合）が更にかなり大きくなってしまうので，勾配法でのアップデートだと学習が遅くなってしまいそうな気がします（正直，このあたりまだまだわかりません）．\u003c/p\u003e\n\n\u003cp\u003eなので，目標に到達したら+1，障害物に当たったら-1，その他は0のようにすればいいのではないでしょうか．\u003cbr\u003e\nこのようにすれば，価値関数の値を眺め（モニタリングし）て，ゴールに近い時に価値が大きくなり，失敗に近ければ価値が下がるようになってるか確認しながらパラメータの調整ができます．少なくともnatureの方はそういう価値の伝播が見やすいアルゴリズムなので，おすすめします．\u003c/p\u003e\n\n\u003cp\u003eお話を聞くぶんには大丈夫な感じはしますが，4ステップの視覚情報で状態を構成するにはタスクが難しすぎる可能性もあります．２つ目の状況だと大丈夫そうですが，１つ目の状況は判断がちょっとつきません．マークが付いているなら上手く見つけてくれそうな感じはしますが・・・．あと意外に，DQNは明度情報しか使ってないのでマークが背景の明度とたまたま同じで，DQNには見えてないということは無いですかね．\u003c/p\u003e\n\n\u003cp\u003e実は2つめの状況と似たような実験をしてみたことがあるのですが，自分の場合は目標の領域に入っているとき報酬+1を与え，他は0にしてうまくいきました．といってもその時のは白黒のバイナリイメージで，数ステップでゴールに到達するような小さいタスクですが．（じつはこのタスクは線形の近似器でも解けてしまいます．プログラムのチェックに使ったのものです．）\u003c/p\u003e\n\n\u003cp\u003eあとお話の状況だと，成功体験が少ないかもしれない，というのは確かにあるかもしれませんね．こういう話は阪大の浅田先生がかなり昔にロボットでされています．\u003cbr\u003e\nPurposive behavior acquisition for a real robot by vision-based reinforcement learning\u003cbr\u003e\n\u003ca href=\&quot;http://www.igi.tugraz.at/lehre/MLB/SS02/asada96.pdf\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://www.igi.tugraz.at/lehre/MLB/SS02/asada96.pdf\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eこの論文中のLEM（Learning from Easy Missions）では，まずは最初のいくらかのエピソードをゴールに近い簡単なタスクからスタートさせて，うまくいくようになったら（TD誤差が小さくなったら），より難しい（目標より遠い）タスクに移行していく方法です．（今風に言えば，強化学習版カリキュラム学習と呼ぶんでしょうか．）\u003c/p\u003e\n\n\u003cp\u003ePreferred Researchの方も複雑なタスクをLEM的な方法で強化学習されてるようです．\u003cbr\u003e\n\u003ca href=\&quot;https://research.preferred.jp/2015/06/distributed-deep-reinforcement-learning/\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttps://research.preferred.jp/2015/06/distributed-deep-reinforcement-learning/\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cdel\u003eちなみに素朴な話だと，こういう系で，近づいていることがRewardで与えられることがわかっているなら，実は正のRewardの時のActionのみを使って教師あり学習してしまえばいいんですが，これを言っちゃあ元も子もないですね^^;\u003c/del\u003e \u003c/p\u003e\n\n\u003cp\u003e追記）後日思い直してみると，以下の図のような状況では以前の回答で提案した教師あり学習ではダメですね．\u003cbr\u003e\nエージェントがゴール（G）に向かう最短距離は点線の矢印で表されますが，障害物がある場合は回り道しなくてはいけません（赤線・青線）．これは単純な例ですが，より複雑な系でも同様の理由で教師あり学習ではダメな場合が出てきそうです．私の考えが浅かったです．\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/a7494df4-da76-d115-2a09-b0ebbd05df08.png\&quot; target=\&quot;_blank\&quot; rel=\&quot;nofollow noopener\&quot;\u003e\u003cimg src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/a7494df4-da76-d115-2a09-b0ebbd05df08.png\&quot; alt=\&quot;Untitled drawing.png\&quot;\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e自分で環境をつくって解かせる場合，エージェントがうまく動かない!ということで環境の方をいじると，知らない間に強化学習で解く必要のない自明な環境になってしまう時があるので注意が必要です^^;;\u003c/p\u003e\n\n\u003cp\u003e以上，参考にしていただければと思います．pyconjpでお会いできたらいいですね！\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-08-31T00:21:02+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:323090,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;興味を持って頂いてありがとうございます．\n\nDeep Reinforcement Learning (以下，DRL)は他の一般物体認識みたいな話題と比べても，\nまだまだ発展途上の分野なので（教師あり学習とは違って，収束するかどうかさえ怪しい），\nこれが正解！といった言い方はできないのが正直なところです．\nなので，思いつく改善点を言うにとどめます．\n\nおそらくALEの代わりにRLGlueのenvironmentを新たに書かれて，それで実験を行われているようですね．\nそして2種類の実験をされている．\n\n1つ目のタスクは目標の到達\n2つ目のタスクは障害物の回避\n\nそして状況はさらに2つある．\n1つ目の状況はロボットにカメラがついていて，カメラ画像をもとに動き回っているような状況と考えればいいんでしょうか？\n2つ目の状況はロボットを天井のカメラから眺めていて，そのカメラ画像をもとにロボットを動かしている感じですかね．\n\nもし勘違いしていたら申し訳ないです．\n\nまず，reward設定からお話すると，お考えの通り+100は結構な大きさなので報酬は+1か-1に正規化したほうが良さそうというのが今のところの(DRLの)状況みたいです．また，近づいたら正の定数，遠くなったら負の定数といった設定は確かに強化学習で最適経路探索させる時に使われることがあるものですが，最適価値関数の値（つまり，報酬が常に+1になるような場合）が更にかなり大きくなってしまうので，勾配法でのアップデートだと学習が遅くなってしまいそうな気がします（正直，このあたりまだまだわかりません）．\n\nなので，目標に到達したら+1，障害物に当たったら-1，その他は0のようにすればいいのではないでしょうか．\nこのようにすれば，価値関数の値を眺め（モニタリングし）て，ゴールに近い時に価値が大きくなり，失敗に近ければ価値が下がるようになってるか確認しながらパラメータの調整ができます．少なくともnatureの方はそういう価値の伝播が見やすいアルゴリズムなので，おすすめします．\n\nお話を聞くぶんには大丈夫な感じはしますが，4ステップの視覚情報で状態を構成するにはタスクが難しすぎる可能性もあります．２つ目の状況だと大丈夫そうですが，１つ目の状況は判断がちょっとつきません．マークが付いているなら上手く見つけてくれそうな感じはしますが・・・．あと意外に，DQNは明度情報しか使ってないのでマークが背景の明度とたまたま同じで，DQNには見えてないということは無いですかね．\n\n実は2つめの状況と似たような実験をしてみたことがあるのですが，自分の場合は目標の領域に入っているとき報酬+1を与え，他は0にしてうまくいきました．といってもその時のは白黒のバイナリイメージで，数ステップでゴールに到達するような小さいタスクですが．（じつはこのタスクは線形の近似器でも解けてしまいます．プログラムのチェックに使ったのものです．）\n\n\nあとお話の状況だと，成功体験が少ないかもしれない，というのは確かにあるかもしれませんね．こういう話は阪大の浅田先生がかなり昔にロボットでされています．\nPurposive behavior acquisition for a real robot by vision-based reinforcement learning\nhttp://www.igi.tugraz.at/lehre/MLB/SS02/asada96.pdf\n\nこの論文中のLEM（Learning from Easy Missions）では，まずは最初のいくらかのエピソードをゴールに近い簡単なタスクからスタートさせて，うまくいくようになったら（TD誤差が小さくなったら），より難しい（目標より遠い）タスクに移行していく方法です．（今風に言えば，強化学習版カリキュラム学習と呼ぶんでしょうか．）\n\nPreferred Researchの方も複雑なタスクをLEM的な方法で強化学習されてるようです．\nhttps://research.preferred.jp/2015/06/distributed-deep-reinforcement-learning/\n\n\n ~~ちなみに素朴な話だと，こういう系で，近づいていることがRewardで与えられることがわかっているなら，実は正のRewardの時のActionのみを使って教師あり学習してしまえばいいんですが，これを言っちゃあ元も子もないですね^^;~~ \n\n追記）後日思い直してみると，以下の図のような状況では以前の回答で提案した教師あり学習ではダメですね．\nエージェントがゴール（G）に向かう最短距離は点線の矢印で表されますが，障害物がある場合は回り道しなくてはいけません（赤線・青線）．これは単純な例ですが，より複雑な系でも同様の理由で教師あり学習ではダメな場合が出てきそうです．私の考えが浅かったです．\n\n![Untitled drawing.png](https://qiita-image-store.s3.amazonaws.com/0/72529/a7494df4-da76-d115-2a09-b0ebbd05df08.png)\n\n\n自分で環境をつくって解かせる場合，エージェントがうまく動かない!ということで環境の方をいじると，知らない間に強化学習で解く必要のない自明な環境になってしまう時があるので注意が必要です^^;;\n\n\n以上，参考にしていただければと思います．pyconjpでお会いできたらいいですね！\n\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-4712c4259582782a5c9b&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;4712c4259582782a5c9b&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e早速の返答ありがとうございます\u003cbr\u003e\n概ねその通りです\u003cbr\u003e\n周囲の状況が絵で得点と行動が数値なら....と,バッサリやると上手くいきました\u003cbr\u003e\n機械学習もchainerも初めてなもんですから,目を覆わんばかりな事をやっているかもしれません\u003cbr\u003e\n障害物を避ける様は,まあ誇らしくも見えたりなんかもしました\u003c/p\u003e\n\n\u003cp\u003eご返答に影響はないとは思いますが,補足します\u003cbr\u003e\n1つ目の状況:レーダー画面チックなものを見せています\u003cbr\u003e\n問題の設定:ほんとは迷路中で目標に到達させたかったですが,なかなか学習しないので,目標到達と障害物回避を分けて学習させています,高位に学習機を置いて階層化できれば......なんて夢想しています\u003cbr\u003e\n明度:オブジェクトは明度のみで表現しています,近いのを使うと目標まで障害物と判定してしまうようです,PILで84x84を直接作っているので中間の明るさは出ないのでなんとかなるかな?と思っていますが,危ういのかもしれません\u003c/p\u003e\n\n\u003cp\u003e学習の初期のうち,典型的状況を意図的に見せてしまおうかとも考えていましたが,禁じてなのはともかく,有効なのは間違いなさそうですね(典型的なのとやさしいのは同じ状況かも?)\u003cbr\u003e\nしかし,ステップが進み学習バッファ(Experience Replay Buffer=data_size)から溢れてしまうと,どうなってしまうのでしょうか?\u003cbr\u003e\nニューロンに記憶が残っているとも言えますが,その後のハズレ体験が徐々に学習されてしまうのでしょうか?\u003cbr\u003e\n色々興味が尽きません\u003c/p\u003e\n\n\u003cp\u003e聞いてばっかりで申し訳ないのですが,これはなんの目的があるのでしょうか?\u003cbr\u003e\n(agent_startだけを見てコードを組んでいたので)見落としていたのですが,前ステップと今ステップの状況を同時に見る事になると思うのですが,振動防止チックなものと捉えて宜しいでしょうか?\u003cbr\u003e\ndqn_agent_nature.py:L219\u0026gt; obs_processed = np.maximum(obs_array, self.last_observation)  # Take maximum from two frames\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-08-31T01:36:23+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:323112,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;早速の返答ありがとうございます\n概ねその通りです\n周囲の状況が絵で得点と行動が数値なら....と,バッサリやると上手くいきました\n機械学習もchainerも初めてなもんですから,目を覆わんばかりな事をやっているかもしれません\n障害物を避ける様は,まあ誇らしくも見えたりなんかもしました\n\nご返答に影響はないとは思いますが,補足します\n1つ目の状況:レーダー画面チックなものを見せています\n問題の設定:ほんとは迷路中で目標に到達させたかったですが,なかなか学習しないので,目標到達と障害物回避を分けて学習させています,高位に学習機を置いて階層化できれば......なんて夢想しています\n明度:オブジェクトは明度のみで表現しています,近いのを使うと目標まで障害物と判定してしまうようです,PILで84x84を直接作っているので中間の明るさは出ないのでなんとかなるかな?と思っていますが,危ういのかもしれません\n\n学習の初期のうち,典型的状況を意図的に見せてしまおうかとも考えていましたが,禁じてなのはともかく,有効なのは間違いなさそうですね(典型的なのとやさしいのは同じ状況かも?)\nしかし,ステップが進み学習バッファ(Experience Replay Buffer=data_size)から溢れてしまうと,どうなってしまうのでしょうか?\nニューロンに記憶が残っているとも言えますが,その後のハズレ体験が徐々に学習されてしまうのでしょうか?\n色々興味が尽きません\n\n聞いてばっかりで申し訳ないのですが,これはなんの目的があるのでしょうか?\n(agent_startだけを見てコードを組んでいたので)見落としていたのですが,前ステップと今ステップの状況を同時に見る事になると思うのですが,振動防止チックなものと捉えて宜しいでしょうか?\ndqn_agent_nature.py:L219\u003e obs_processed = np.maximum(obs_array, self.last_observation)  # Take maximum from two frames\n\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-7a028b10ed0e43910d32&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-08-30T22:17:26+09:00&quot;,&quot;id&quot;:91643,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91643/profile-images/1473705372&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;tsujimo&quot;},&quot;uuid&quot;:&quot;7a028b10ed0e43910d32&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cblockquote\u003e\n\u003cp\u003e学習の初期のうち,典型的状況を意図的に見せてしまおうかとも考えていましたが,禁じてなのはともかく,有効なのは間違いなさそうですね(典型的なのとやさしいのは同じ状況かも?)\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e学習がすすむための取っ掛かり，つまり一種の初期化処理としてそういう処理はいい選択ですね．僅かな例を与えて，それをもとに様々な状況でも問題解決できるように学習させることが目標なら，それもまた面白い話と思います．こういうのは強化学習における転移学習（Transfer Learning）とも呼ばれます．最近はUCBの人たちがPR2というロボット使って，そういうことやってたりもします（方法はQ学習とはまた異なるものですが）．\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003eしかし,ステップが進み学習バッファ(Experience Replay Buffer=data_size)から溢れてしまうと,どうなってしまうのでしょうか? ニューロンに記憶が残っているとも言えますが,その後のハズレ体験が徐々に学習されてしまうのでしょうか?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eQ学習は「体験を学ぶ」のではなく，「体験を通して最適価値関数を学習する」というものなのでバッファから記憶が漏れていっても問題ありません．ただ，バッファ内に成功体験が一回も含まれていないのは良くないと思います．\u003c/p\u003e\n\n\u003cp\u003e簡単なタスクを最初に与えるのは，成功例（ゴール）を多く見せてその直近の状態と報酬をとにかく価値関数で関連付けるためです．そうすることで難しいタスクに移行したときには，Q学習では報酬と関連付けられた状態を「擬似的なゴール」とみなして（関連付けられた状態からどうするかは，先の簡単なタスクで既に学習しています），新たに遭遇する状況のみに関して学習すればいいので学習が速くなります．成功に近い状態からの行動選択はすでに学習済みなので，記憶バッファ内に成功例は増えていくはずです．\u003c/p\u003e\n\n\u003cblockquote\u003e\n\u003cp\u003e振動防止チックなものと捉えて宜しいでしょうか?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eはい．これは原論文の実装に沿ったものです．ATARIのゲームはゲームカセットの種類によって，画面がちらつく（2コマに2度しか弾が現れないシューティングゲームなど）ものがあるのでその対策とのことです．\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-08-31T18:45:48+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:324156,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;\u003e学習の初期のうち,典型的状況を意図的に見せてしまおうかとも考えていましたが,禁じてなのはともかく,有効なのは間違いなさそうですね(典型的なのとやさしいのは同じ状況かも?)\n\n学習がすすむための取っ掛かり，つまり一種の初期化処理としてそういう処理はいい選択ですね．僅かな例を与えて，それをもとに様々な状況でも問題解決できるように学習させることが目標なら，それもまた面白い話と思います．こういうのは強化学習における転移学習（Transfer Learning）とも呼ばれます．最近はUCBの人たちがPR2というロボット使って，そういうことやってたりもします（方法はQ学習とはまた異なるものですが）．\n\n\u003eしかし,ステップが進み学習バッファ(Experience Replay Buffer=data_size)から溢れてしまうと,どうなってしまうのでしょうか? ニューロンに記憶が残っているとも言えますが,その後のハズレ体験が徐々に学習されてしまうのでしょうか?\n\nQ学習は「体験を学ぶ」のではなく，「体験を通して最適価値関数を学習する」というものなのでバッファから記憶が漏れていっても問題ありません．ただ，バッファ内に成功体験が一回も含まれていないのは良くないと思います．\n\n簡単なタスクを最初に与えるのは，成功例（ゴール）を多く見せてその直近の状態と報酬をとにかく価値関数で関連付けるためです．そうすることで難しいタスクに移行したときには，Q学習では報酬と関連付けられた状態を「擬似的なゴール」とみなして（関連付けられた状態からどうするかは，先の簡単なタスクで既に学習しています），新たに遭遇する状況のみに関して学習すればいいので学習が速くなります．成功に近い状態からの行動選択はすでに学習済みなので，記憶バッファ内に成功例は増えていくはずです．\n\n\u003e振動防止チックなものと捉えて宜しいでしょうか?\n\nはい．これは原論文の実装に沿ったものです．ATARIのゲームはゲームカセットの種類によって，画面がちらつく（2コマに2度しか弾が現れないシューティングゲームなど）ものがあるのでその対策とのことです．\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-54b1010ead60a9f7c827&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;54b1010ead60a9f7c827&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003e突然、すみません。\u003cbr\u003e\nこれからDeep Q-Networkの研究を始めようとしていて、githubに載せていたソースを使って実行してみたのですが下記のようなエラーが出ました。\u003cbr\u003e\nROMは\u003ca href=\&quot;http://atariage.com/system_items.html?SystemID=2600\u0026amp;ItemTypeID=ROM\u0026amp;orderBy=Name\u0026amp;orderByValue=Ascending\u0026amp;recordsPerPage=100\u0026amp;currentPage=5%E3%80%80%E3%81%8B%E3%82%89%E6%8C%81%E3%81%A3%E3%81%A6%E3%81%8D%E3%81%9F%E3%81%AE%E3%81%A7%E3%81%99%E3%81%8C%E3%80%81%E7%89%B9%E5%AE%9A%E3%81%AEROM%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%A7%E3%81%AF%E3%81%AA%E3%81%84%E3%81%A8%E8%A1%A8%E7%A4%BA%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://atariage.com/system_items.html?SystemID=2600\u0026amp;ItemTypeID=ROM\u0026amp;orderBy=Name\u0026amp;orderByValue=Ascending\u0026amp;recordsPerPage=100\u0026amp;currentPage=5　から持ってきたのですが、特定のROMファイルではないと表示されます。\u003c/a\u003e\u003cbr\u003e\nどうすればよいか教えていただければ幸いです。\u003c/p\u003e\n\n\u003cp\u003e./ale -game_controller rlglue -use_starting_actions true random_seed time -display_screen true -frame_skip 3 ROM/breakout.bin\u003cbr\u003e\nA.L.E: Arcade Learning Environment (version 0.4)\u003cbr\u003e\n[Powered by Stella]\u003cbr\u003e\nUse -help for help screen.\u003cbr\u003e\nNo ROM File specified or the ROM file was not found.\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-10-12T16:36:00+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:355328,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;突然、すみません。\nこれからDeep Q-Networkの研究を始めようとしていて、githubに載せていたソースを使って実行してみたのですが下記のようなエラーが出ました。\nROMはhttp://atariage.com/system_items.html?SystemID=2600\u0026ItemTypeID=ROM\u0026orderBy=Name\u0026orderByValue=Ascending\u0026recordsPerPage=100\u0026currentPage=5　から持ってきたのですが、特定のROMファイルではないと表示されます。\nどうすればよいか教えていただければ幸いです。\n\n./ale -game_controller rlglue -use_starting_actions true random_seed time -display_screen true -frame_skip 3 ROM/breakout.bin\nA.L.E: Arcade Learning Environment (version 0.4)\n[Powered by Stella]\nUse -help for help screen.\nNo ROM File specified or the ROM file was not found.\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-c932ebcaa6ac8268f40b&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-10-12T16:22:52+09:00&quot;,&quot;id&quot;:96384,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/96384/profile-images/1473763015&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;nk_pv&quot;},&quot;uuid&quot;:&quot;c932ebcaa6ac8268f40b&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eBreakoutのROMはおそらくATARI社製の1種類しか無かったかと思います．\u003cbr\u003e\n表示されているエラーをみると，エミュレータがROMの情報を受け取れていないようですね．\u003cbr\u003e\nDQN-Chainer側のエラーではないようです．\u003c/p\u003e\n\n\u003cp\u003eこちらの環境ではBreakoutでの実行を確認しているので単純な回答となってしまいますが，\u003cbr\u003e\n・ゲームのROMファイルは今現在指定されている場所に正しく置かれているでしょうか？\u003cbr\u003e\n・breakout.binのファイルが壊れている等のことはないでしょうか？Stellaエミュレータが公開されているので，現在お持ちのROMファイルがそれで読み込めて実際自分で遊べるかどうか確かめてみてください．\u003cbr\u003e\nStella : \u003ca href=\&quot;http://stella.sourceforge.net/\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://stella.sourceforge.net/\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e著作権の問題もあると思われるのでROMファイルの入手先等の情報についての詳しい話に関しては申し訳ありませんが，お答えできません．ただtwitterでメッセージをいただければ個人的に相談にのることはできるかと思います．\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-10-13T19:56:53+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:356163,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:1,&quot;raw_body&quot;:&quot;BreakoutのROMはおそらくATARI社製の1種類しか無かったかと思います．\n表示されているエラーをみると，エミュレータがROMの情報を受け取れていないようですね．\nDQN-Chainer側のエラーではないようです．\n\nこちらの環境ではBreakoutでの実行を確認しているので単純な回答となってしまいますが，\n・ゲームのROMファイルは今現在指定されている場所に正しく置かれているでしょうか？\n・breakout.binのファイルが壊れている等のことはないでしょうか？Stellaエミュレータが公開されているので，現在お持ちのROMファイルがそれで読み込めて実際自分で遊べるかどうか確かめてみてください．\nStella : http://stella.sourceforge.net/\n\n著作権の問題もあると思われるのでROMファイルの入手先等の情報についての詳しい話に関しては申し訳ありませんが，お答えできません．ただtwitterでメッセージをいただければ個人的に相談にのることはできるかと思います．\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-9ac5e4277efe2d94abdd&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;9ac5e4277efe2d94abdd&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eありがとうございます、指定場所が間違って指定されていたようで気づきませんでした。\u003c/p\u003e\n\n\u003cp\u003e次に以下のようなエラーが出たのですが、どうすれば良いでしょうか。\u003c/p\u003e\n\n\u003cp\u003eRL-Glue interface unavailable. Please recompile with RL-Glue support.\u003c/p\u003e\n\n\u003cp\u003eTwitterの方でご連絡したいので承認していただけますと、ありがたいです。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-10-17T16:47:31+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:360037,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;ありがとうございます、指定場所が間違って指定されていたようで気づきませんでした。\n\n次に以下のようなエラーが出たのですが、どうすれば良いでしょうか。\n\nRL-Glue interface unavailable. Please recompile with RL-Glue support.\n\nTwitterの方でご連絡したいので承認していただけますと、ありがたいです。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-475a65dd49e1b939d6d6&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2015-10-12T16:22:52+09:00&quot;,&quot;id&quot;:96384,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/96384/profile-images/1473763015&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;nk_pv&quot;},&quot;uuid&quot;:&quot;475a65dd49e1b939d6d6&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eRL-glueが上手く行ってないようですね．\u003c/p\u003e\n\n\u003cp\u003eRL-glueのインストールは正しくなされたでしょうか？\u003cbr\u003e\nreadme.txtに記載したとおり，Rl-glue python codecのサンプルプログラムが正しく動作すれば問題なく実行できるはずです．\u003c/p\u003e\n\n\u003cp\u003e追伸）\u003cbr\u003e\ntwitterの方で連絡をまだ受け取れていませんが，自己解決されたのでしょうか？\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2015-10-19T22:01:39+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:361216,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;RL-glueが上手く行ってないようですね．\n\nRL-glueのインストールは正しくなされたでしょうか？\nreadme.txtに記載したとおり，Rl-glue python codecのサンプルプログラムが正しく動作すれば問題なく実行できるはずです．\n\n\n追伸）\ntwitterの方で連絡をまだ受け取れていませんが，自己解決されたのでしょうか？\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-f73564430dfb147195a3&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;f73564430dfb147195a3&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eUgo-Namaさん\u003cbr\u003e\nはじめまして、現在DQNに関して研究しておりますtochiro0104です。\u003cbr\u003e\nUgo-Namaさんの記事をベースにしまして、\u003ca href=\&quot;http://vaaaaaanquish.hatenablog.com/entry/2015/12/11/215417\&quot; class=\&quot;autolink\&quot; rel=\&quot;nofollow noopener\&quot; target=\&quot;_blank\&quot;\u003ehttp://vaaaaaanquish.hatenablog.com/entry/2015/12/11/215417\u003c/a\u003e\u003cbr\u003e\nに書いてあります通りに実装しているのですが、エラーにはまりご相談したくコメントいたしました。\u003c/p\u003e\n\n\u003cp\u003e\u0026lt;エラー内容\u0026gt;\u003c/p\u003e\n\n\u003cp\u003e./ale -game_controller rlglue -use_starting_actions true -random_seed time -display_screen true -frame_skip 4 ../rom/Pong.bin\u003c/p\u003e\n\n\u003cp\u003eRL-Glue interface unavailable. Please recompile with RL-Glue support.\u003c/p\u003e\n\n\u003cp\u003e以下のようにおそらく、RL-glueインストールが正しくなされているのではないかとは思っておりまして、\u003cbr\u003e\nその他、RL-Glue interface unavailable. Please recompile with RL-Glue support.のエラー原因になるところがありましたらご指摘いただければと思います。\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/fc6c5d92-f17d-38f0-c464-98f2c745c8fe.png\&quot; target=\&quot;_blank\&quot; rel=\&quot;nofollow noopener\&quot;\u003e\u003cimg width=\&quot;419\&quot; alt=\&quot;スクリーンショット 2016-03-13 19.21.00.png\&quot; src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/fc6c5d92-f17d-38f0-c464-98f2c745c8fe.png\&quot;\u003e\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/e0a97bfc-058c-e19c-10e4-403bd49e3e90.png\&quot; target=\&quot;_blank\&quot; rel=\&quot;nofollow noopener\&quot;\u003e\u003cimg width=\&quot;431\&quot; alt=\&quot;スクリーンショット 2016-03-13 19.19.28.png\&quot; src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/e0a97bfc-058c-e19c-10e4-403bd49e3e90.png\&quot;\u003e\u003c/a\u003e\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-03-13T19:22:48+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:485760,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Ugo-Namaさん\nはじめまして、現在DQNに関して研究しておりますtochiro0104です。\nUgo-Namaさんの記事をベースにしまして、http://vaaaaaanquish.hatenablog.com/entry/2015/12/11/215417\nに書いてあります通りに実装しているのですが、エラーにはまりご相談したくコメントいたしました。\n\n\u003cエラー内容\u003e\n###\n./ale -game_controller rlglue -use_starting_actions true -random_seed time -display_screen true -frame_skip 4 ../rom/Pong.bin\n\nRL-Glue interface unavailable. Please recompile with RL-Glue support.\n###\n\n以下のようにおそらく、RL-glueインストールが正しくなされているのではないかとは思っておりまして、\nその他、RL-Glue interface unavailable. Please recompile with RL-Glue support.のエラー原因になるところがありましたらご指摘いただければと思います。\n\n\u003cimg width=\&quot;419\&quot; alt=\&quot;スクリーンショット 2016-03-13 19.21.00.png\&quot; src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/fc6c5d92-f17d-38f0-c464-98f2c745c8fe.png\&quot;\u003e\n\u003cimg width=\&quot;431\&quot; alt=\&quot;スクリーンショット 2016-03-13 19.19.28.png\&quot; src=\&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/e0a97bfc-058c-e19c-10e4-403bd49e3e90.png\&quot;\u003e\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-b34fbf7fc1874f8b2120&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2016-03-13T19:02:30+09:00&quot;,&quot;id&quot;:117168,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/profile-images/1473713495&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;tochiro0104&quot;},&quot;uuid&quot;:&quot;b34fbf7fc1874f8b2120&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003etochiro0104さん，\u003cbr\u003e\nはじめまして，ugo-namaです．\u003c/p\u003e\n\n\u003cp\u003e興味を持っていただきありがとうございます．お役に立てれば幸いです．\u003cbr\u003e\n参考にされているvaaaaanquishさんの記事に従っていれば，私も特に問題はないと思うのですが・・・\u003c/p\u003e\n\n\u003cp\u003eもう試されているかもしれませんが，ひとまずRL-glueのサンプルコードは動きますか？\u003cbr\u003e\nALEのcmake \u0026amp; makeのためのmakefileは正しく設定されましたか？\u003cbr\u003e\nUSE_SDL := 1\u003cbr\u003e\nUSE_RLGLUE := 1\u003cbr\u003e\nとする必要があります．どちらもdefault（@ALE-0.5.1）では0になってるので書き換える必要があります．\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-03-15T18:14:43+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:487887,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;tochiro0104さん，\nはじめまして，ugo-namaです．\n\n興味を持っていただきありがとうございます．お役に立てれば幸いです．\n参考にされているvaaaaanquishさんの記事に従っていれば，私も特に問題はないと思うのですが・・・\n\nもう試されているかもしれませんが，ひとまずRL-glueのサンプルコードは動きますか？\nALEのcmake \u0026 makeのためのmakefileは正しく設定されましたか？\nUSE_SDL := 1\nUSE_RLGLUE := 1\nとする必要があります．どちらもdefault（@ALE-0.5.1）では0になってるので書き換える必要があります．\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-2df3c1a02f9013d29bd8&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;2df3c1a02f9013d29bd8&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eUgo-Namaさん\u003cbr\u003e\nはじめまして、fragilebluです。\u003c/p\u003e\n\n\u003cp\u003e早速ですが、ale が以下のようにコアダンプしてしまいます。\u003c/p\u003e\n\n\u003cp\u003e以下のchainerの簡易テストは実行しているので問題ないと思うのですが、\u003cbr\u003e\n python -c \&quot;import chainer\&quot;\u003cbr\u003e\n python -c \&quot;import cupy\&quot;\u003cbr\u003e\nまた、rl_glue3.04は、配下のexampleの２つのプログラムで動作確認済みです。\u003cbr\u003e\n後はALEを0.5.0にしても同様の事象が発生してしまいます。\u003cbr\u003e\n対処法がわかれば教えていただきたいのですが・・・\u003c/p\u003e\n\n\u003cp\u003e環境：\u003cbr\u003e\nubuntu16.04\u003cbr\u003e\ncuda 7.5\u003cbr\u003e\nchainer 1.12.0\u003cbr\u003e\nRL-Glue Version 3.04, Build 909\u003cbr\u003e\nALE 0.5.1\u003cbr\u003e\npython 2.7.12\u003c/p\u003e\n\n\u003cp\u003e\u003cbr\u003e\n$ ./ale -game_controller rlglue -use_starting_actions true -random_seed time -display_screen true -frame_skip 4 ../Roms/Pong.bin\u003cbr\u003e\nA.L.E: Arcade Learning Environment (version 0.5.0)\u003cbr\u003e\n[Powered by Stella]\u003cbr\u003e\nUse -help for help screen.\u003cbr\u003e\nGame console created:\u003cbr\u003e\n  ROM file:  ../Roms/Pong.bin\u003cbr\u003e\n  Cart Name: Video Olympics (1978) (Atari)\u003cbr\u003e\n  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\u003cbr\u003e\n  Display Format:  AUTO-DETECT ==\u0026gt; NTSC\u003cbr\u003e\n  ROM Size:        2048\u003cbr\u003e\n  Bankswitch Type: AUTO-DETECT ==\u0026gt; 2K\u003c/p\u003e\n\n\u003cp\u003eScreen Display Active. [Manual Control Mode] &#39;m&#39; [Slowdown] &#39;a&#39; [Speedup] &#39;s&#39; [VolumeDown] &#39;[&#39; [VolumeUp] &#39;]&#39;.\u003cbr\u003e\nRunning ROM file...\u003cbr\u003e\nRandom seed is 0\u003cbr\u003e\nGame will be controlled through RL-Glue.\u003cbr\u003e\nInitializing ALE RL-Glue ...\u003cbr\u003e\nSegmentation fault (コアダンプ)\u003c/p\u003e\n\n\u003cp\u003e\u003cbr\u003e\n$ python dqn_agent_nature.py \u003cbr\u003e\nRL-Glue Python Agent Codec Version: 2.02 (Build 738)\u003cbr\u003e\n    Connecting to 127.0.0.1 on port 4096...\u003cbr\u003e\n     Agent Codec Connected\u003cbr\u003e\nInitializing DQN...\u003cbr\u003e\nModel Building\u003cbr\u003e\nInitizlizing Optimizer\u003cbr\u003e\n/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/chainer/function_set.py:62: FutureWarning: &#39;collect_parameters&#39; is deprecated. You can pass FunctionSet itself to &#39;optimizer.setup&#39;\u003cbr\u003e\n  warnings.warn(msg, FutureWarning)\u003cbr\u003e\nTraceback (most recent call last):\u003cbr\u003e\n  File \&quot;dqn_agent_nature.py\&quot;, line 305, in \u003cbr\u003e\n    AgentLoader.loadAgent(dqn_agent())\u003cbr\u003e\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/AgentLoader.py\&quot;, line 58, in loadAgent\u003cbr\u003e\n    client.runAgentEventLoop()\u003cbr\u003e\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 144, in runAgentEventLoop\u003cbr\u003e\n    switch\u003ca href=\&quot;self\&quot;\u003eagentState\u003c/a\u003e\u003cbr\u003e\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 138, in \u003cbr\u003e\n    Network.kAgentStart: lambda self: self.onAgentStart(),\u003cbr\u003e\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 51, in onAgentStart\u003cbr\u003e\n    action = self.agent.agent_start(observation)\u003cbr\u003e\n  File \&quot;dqn_agent_nature.py\&quot;, line 196, in agent_start\u003cbr\u003e\n    obs_array = (spm.imresize(tmp, (110, 84)))[110-84-8:110-8, :]  # Scaling\u003cbr\u003e\nAttributeError: &#39;module&#39; object has no attribute &#39;imresize&#39;\u003c/p\u003e\n\n\u003cp\u003e\u003cbr\u003e\nDQN-ALE Experiment starting up!\u003cbr\u003e\nRL-Glue Python Experiment Codec Version: 2.02 (Build 738)\u003cbr\u003e\n    Connecting to 127.0.0.1 on port 4096...\u003cbr\u003e\nFreeze learning for Evaluation\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-07-30T15:13:10+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:603119,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Ugo-Namaさん\nはじめまして、fragilebluです。\n\n早速ですが、ale が以下のようにコアダンプしてしまいます。\n\n以下のchainerの簡易テストは実行しているので問題ないと思うのですが、\n python -c \&quot;import chainer\&quot;\n python -c \&quot;import cupy\&quot;\nまた、rl_glue3.04は、配下のexampleの２つのプログラムで動作確認済みです。\n後はALEを0.5.0にしても同様の事象が発生してしまいます。\n対処法がわかれば教えていただきたいのですが・・・\n\n環境：\nubuntu16.04\ncuda 7.5\nchainer 1.12.0\nRL-Glue Version 3.04, Build 909\nALE 0.5.1\npython 2.7.12\n\n\u003cale 実行メッセージ\u003e\n$ ./ale -game_controller rlglue -use_starting_actions true -random_seed time -display_screen true -frame_skip 4 ../Roms/Pong.bin\nA.L.E: Arcade Learning Environment (version 0.5.0)\n[Powered by Stella]\nUse -help for help screen.\nGame console created:\n  ROM file:  ../Roms/Pong.bin\n  Cart Name: Video Olympics (1978) (Atari)\n  Cart MD5:  60e0ea3cbe0913d39803477945e9e5ec\n  Display Format:  AUTO-DETECT ==\u003e NTSC\n  ROM Size:        2048\n  Bankswitch Type: AUTO-DETECT ==\u003e 2K\n\nScreen Display Active. [Manual Control Mode] &#39;m&#39; [Slowdown] &#39;a&#39; [Speedup] &#39;s&#39; [VolumeDown] &#39;[&#39; [VolumeUp] &#39;]&#39;.\nRunning ROM file...\nRandom seed is 0\nGame will be controlled through RL-Glue.\nInitializing ALE RL-Glue ...\nSegmentation fault (コアダンプ)\n\n\u003cdqn_agent_nature.py側でのメッセージ\u003e\n$ python dqn_agent_nature.py \nRL-Glue Python Agent Codec Version: 2.02 (Build 738)\n\tConnecting to 127.0.0.1 on port 4096...\n\t Agent Codec Connected\nInitializing DQN...\nModel Building\nInitizlizing Optimizer\n/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/chainer/function_set.py:62: FutureWarning: &#39;collect_parameters&#39; is deprecated. You can pass FunctionSet itself to &#39;optimizer.setup&#39;\n  warnings.warn(msg, FutureWarning)\nTraceback (most recent call last):\n  File \&quot;dqn_agent_nature.py\&quot;, line 305, in \u003cmodule\u003e\n    AgentLoader.loadAgent(dqn_agent())\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/AgentLoader.py\&quot;, line 58, in loadAgent\n    client.runAgentEventLoop()\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 144, in runAgentEventLoop\n    switch[agentState](self)\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 138, in \u003clambda\u003e\n    Network.kAgentStart: lambda self: self.onAgentStart(),\n  File \&quot;/home/alienware/.pyenv/versions/2.7.12/lib/python2.7/site-packages/rlglue/agent/ClientAgent.py\&quot;, line 51, in onAgentStart\n    action = self.agent.agent_start(observation)\n  File \&quot;dqn_agent_nature.py\&quot;, line 196, in agent_start\n    obs_array = (spm.imresize(tmp, (110, 84)))[110-84-8:110-8, :]  # Scaling\nAttributeError: &#39;module&#39; object has no attribute &#39;imresize&#39;\n\n\u003cexperiment_ale.py側でのメッセージ\u003e\nDQN-ALE Experiment starting up!\nRL-Glue Python Experiment Codec Version: 2.02 (Build 738)\n\tConnecting to 127.0.0.1 on port 4096...\nFreeze learning for Evaluation\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-1322f892be88abfbab0d&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2014-12-11T21:40:37+09:00&quot;,&quot;id&quot;:62649,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/62649/profile-images/1473696022&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;fragileblu&quot;},&quot;uuid&quot;:&quot;1322f892be88abfbab0d&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003efragilebluさん，\u003c/p\u003e\n\n\u003cp\u003eALEのコアダンプのほうがちょっとわかりません・・・ごめんなさい．\u003c/p\u003e\n\n\u003cp\u003epython dqn_agent_nature.py のほうは， Scipyで画像使う時のPillowが入ってないようですね．\u003cbr\u003e\nインストールしてから試してみてください．\u003cbr\u003e\nPillowのインストールをREADMEに書き漏らしていたようなので加えておきました．\u003c/p\u003e\n\n\u003cp\u003e個人的にはそろそろALEのフォローが辛いのでOpenAI Gymを使われることをオススメします・・・．\u003cbr\u003e\n（Gymのほうはデフォルトで学習させると，フレームのスキップ数が幾らか乱択なので，　ALEでやるような固定の場合に比べて学習が遅くなるかパフォーマンスが悪い印象があります．）\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-08-03T22:28:39+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:606446,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;fragilebluさん，\n\nALEのコアダンプのほうがちょっとわかりません・・・ごめんなさい．\n\npython dqn_agent_nature.py のほうは， Scipyで画像使う時のPillowが入ってないようですね．\nインストールしてから試してみてください．\nPillowのインストールをREADMEに書き漏らしていたようなので加えておきました．\n\n個人的にはそろそろALEのフォローが辛いのでOpenAI Gymを使われることをオススメします・・・．\n（Gymのほうはデフォルトで学習させると，フレームのスキップ数が幾らか乱択なので，　ALEでやるような固定の場合に比べて学習が遅くなるかパフォーマンスが悪い印象があります．）\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-838df6c52011f7a97d3d&quot;,&quot;user&quot;:{&quot;contribution&quot;:934,&quot;created_at&quot;:&quot;2015-03-17T09:44:47+09:00&quot;,&quot;id&quot;:72529,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;uuid&quot;:&quot;838df6c52011f7a97d3d&quot;,&quot;via_email&quot;:false},{&quot;banned&quot;:false,&quot;body&quot;:&quot;\u003cp\u003eUgo-Namaさん\u003c/p\u003e\n\n\u003cp\u003efragilebluです。\u003c/p\u003e\n\n\u003cp\u003eご解答いただきありがとうございます。\u003cbr\u003e\nPillowを入れることですべて解決しました。ありがとうございました。\u003cbr\u003e\nOpenAI Gymは、まだ扱ったことがないので、挑戦してみます。\u003c/p\u003e\n&quot;,&quot;created_at&quot;:&quot;2016-08-04T22:05:37+09:00&quot;,&quot;expanded_references&quot;:&quot;&quot;,&quot;id&quot;:607606,&quot;is_team&quot;:false,&quot;item_id&quot;:314984,&quot;item_type&quot;:&quot;PublicDomainArticle&quot;,&quot;item_uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;likable&quot;:false,&quot;liked&quot;:false,&quot;public_likes_count&quot;:0,&quot;raw_body&quot;:&quot;Ugo-Namaさん\n\nfragilebluです。\n\nご解答いただきありがとうございます。\nPillowを入れることですべて解決しました。ありがとうございました。\nOpenAI Gymは、まだ扱ったことがないので、挑戦してみます。\n&quot;,&quot;reaction_types&quot;:[{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f44d.png&quot;,&quot;name&quot;:&quot;+1&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f64f.png&quot;,&quot;name&quot;:&quot;pray&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f389.png&quot;,&quot;name&quot;:&quot;tada&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f647.png&quot;,&quot;name&quot;:&quot;bow&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f631.png&quot;,&quot;name&quot;:&quot;scream&quot;},{&quot;image_url&quot;:&quot;https://cdn.qiita.com/emoji/twemoji/unicode/1f440.png&quot;,&quot;name&quot;:&quot;eyes&quot;}],&quot;reactions&quot;:[],&quot;team&quot;:null,&quot;team_membership&quot;:null,&quot;updatable&quot;:false,&quot;url&quot;:&quot;http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5#comment-e06365218e4170b0fefa&quot;,&quot;user&quot;:{&quot;contribution&quot;:0,&quot;created_at&quot;:&quot;2014-12-11T21:40:37+09:00&quot;,&quot;id&quot;:62649,&quot;is_admin&quot;:false,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/62649/profile-images/1473696022&quot;,&quot;suspended&quot;:false,&quot;url_name&quot;:&quot;fragileblu&quot;},&quot;uuid&quot;:&quot;e06365218e4170b0fefa&quot;,&quot;via_email&quot;:false}],&quot;monthly_public_image_uploadable_size_limit&quot;:null,&quot;total_uploaded_public_image_size_in_current_month&quot;:null,&quot;item&quot;:{&quot;id&quot;:314984,&quot;uuid&quot;:&quot;08c6a5f6a571335972d5&quot;,&quot;suspended&quot;:false,&quot;secret&quot;:false},&quot;owner&quot;:{&quot;url_name&quot;:&quot;Ugo-Nama&quot;},&quot;is_team&quot;:false,&quot;is_project&quot;:false,&quot;logged_in&quot;:false,&quot;polling&quot;:false,&quot;mention_candidates&quot;:[{&quot;id&quot;:72529,&quot;url_name&quot;:&quot;Ugo-Nama&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;},{&quot;id&quot;:91643,&quot;url_name&quot;:&quot;tsujimo&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/91643/profile-images/1473705372&quot;},{&quot;id&quot;:96384,&quot;url_name&quot;:&quot;nk_pv&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/96384/profile-images/1473763015&quot;},{&quot;id&quot;:117168,&quot;url_name&quot;:&quot;tochiro0104&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/117168/profile-images/1473713495&quot;},{&quot;id&quot;:62649,&quot;url_name&quot;:&quot;fragileblu&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/62649/profile-images/1473696022&quot;}]}" data-trace="false" data-dom-id="CommentListContainer-react-component-e9165a74-6cda-4e5f-899c-fe4060776eb6"></div>
    <div id="CommentListContainer-react-component-e9165a74-6cda-4e5f-899c-fe4060776eb6"></div>
    
</div></div></div></div></article><div class="js-report-form modal fade reportForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Report article</h4></div><div class="modal-body"><form action="/reports" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="Z+KwS/i+WpbMf80fsRODqd9Xhi9CK+Mr9gDEB02cEOybTcT3jNdfW/p5dvBxTUoRAEPDjs4/0gr6PP8uYnoVpA==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Ugo-Nama/items/08c6a5f6a571335972d5" /><input type="hidden" name="item_uuid" id="item_uuid" value="08c6a5f6a571335972d5" /><p>Help us understand the problem. What is going on with this item?</p><br /><div class="form-group"><ul class="list-unstyled"><li><label><input type="radio" name="report_type" id="report_type_spam" value="spam" required="required" /> It&#39;s spam </label></li><li><label><input type="radio" name="report_type" id="report_type_harassment" value="harassment" required="required" /> It&#39;s abusive or harmful </label></li><li><label><input type="radio" name="report_type" id="report_type_inappropriate_content" value="inappropriate_content" required="required" /> It contains inappropriate content </label></li></ul></div><div class="reportForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary reportForm_submitButton"><i class="fa fa-send"></i> Submit</button></div></form></div></div></div></div><script id="js-item" type="application/json">{ "url": "http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5", "id": 314984, "uuid": "08c6a5f6a571335972d5" }</script><script class="js-user" type="application/json">{&quot;id&quot;:72529,&quot;url_name&quot;:&quot;Ugo-Nama&quot;,&quot;profile_image_url&quot;:&quot;https://qiita-image-store.s3.amazonaws.com/0/72529/profile-images/1473699182&quot;}</script><script language="JavaScript" src="//cdn.bigmining.com/private/js/qiita_bigmining.js" type="text/javascript"></script></div><footer class="footer"><div class="footer_inner"><div class="footer_container"><ul class="footer_links-left"><li class="footer_link"><a class="footer_copyright" href="http://increments.co.jp">© 2011-2017 Increments Inc.</a></li><li class="footer_link"><a href="http://qiita.com/terms">Terms</a></li><li class="footer_link"><a href="http://qiita.com/privacy">Privacy</a></li><li class="footer_link"><a href="http://help.qiita.com">Help</a></li><li class="footer_link"><a href="https://increments.zendesk.com/anonymous_requests/new">Contact</a></li></ul><ul class="footer_links-right"><li class="footer_link"><a href="http://qiita.com/about">About</a></li><li class="footer_link"><a href="/users">Users</a></li><li class="footer_link"><a href="/tags">Tags</a></li><li class="footer_link"><a href="http://blog.qiita.com">Blog</a></li><li class="footer_link"><a href="http://qiita.com/api/v2/docs">API</a></li><li class="footer_link"><a href="https://teams.qiita.com/">Team</a></li><li class="footer_link"><a href="http://kobito.qiita.com">Kobito</a></li><li class="footer_link"><a class="js-public-form-feedback-link" data-target=".js-feedback-form" data-toggle="modal" href=""><i class="fa fa-heart"></i> Feedback <i class="fa fa-caret-down"></i></a></li></ul></div></div></footer><div class="js-feedback-form modal fade feedbackForm"><div class="modal-dialog"><div class="modal-content"><div class="modal-header"><button name="button" type="submit" class="close" data-dismiss="modal">&times;</button><h4 class="modal-title">Feedback</h4></div><div class="modal-body"><form class="js-feedback-form-form" action="/feedbacks" accept-charset="UTF-8" method="post"><input name="utf8" type="hidden" value="&#x2713;" /><input type="hidden" name="authenticity_token" value="kJgjR+irvu+CqR9XaIwqWJ6SGjZOxaCxoqpC6TaaeZFsN1f7nMK7IrSvpLio0uPgQYZfl8LRkZCulnnAGXx82Q==" /><input type="hidden" name="redirect_path" id="redirect_path" value="/Ugo-Nama/items/08c6a5f6a571335972d5" /><div class="form-group"><textarea name="feedback[message]" id="feedback_message" class="form-control js-feedback-form-text-area" placeholder="Please give us any feedback about Qiita." required="required" rows="5">
</textarea></div><div class="feedbackForm_submitButtonContainer"><button name="button" type="submit" class="btn btn-primary feedbackForm_submitButton"><i class="fa fa-send"></i> Submit</button><p class="feedbackForm_note">We don&#39;t reply to any feedback.<br />If you need help with Qiita, please send a support request from <a href="https://increments.zendesk.com/anonymous_requests/new">here</a>.</p></div><div style="position:fixed;top:-99999px;opacity:0.0001;"><input name="feedback[name]" type="text" /></div></form></div></div></div></div><script>// if (window.mixpanel instanceof Element) {
//   window.mixpanel = [];
// }
// (function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
// for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);</script><script src="http://cdn.qiita.com/assets/public-60064a257a213198cd13df56dcca54cf.min.js"></script><script>
  (function () {
    var script = document.getElementsByTagName('script')[0];
    var load = function (src, id) {
      var el = document.createElement('script');
      el.async = true;
      el.src = src;
      el.id = id;
      script.parentNode.insertBefore(el, script);
    };
      // Optimizely
      load('//cdn.optimizely.com/js/52738645.js', 'optimizely-jssdk');
      // Google Analytics
      window._gaq = window._gaq || [];
      var isCareer = location.hostname.split('.')[0] == 'career';
      if (isCareer) {
        window._gaq.push(['_setAccount', 'UA-24675221-11']);
        window._gaq.push(['_setDomainName', 'qiita.com']);
      } else {
        window._gaq.push(['_setAccount', 'UA-24675221-1']);
      }
      window._gaq.push(['_setCustomVar', 1, 'logged_in', 'false', 2]);
      window._gaq.push(['_trackPageview']);
      var src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      load(src, 'google-analytics-jssdk');
    // Google Analytics - Universal Analytics
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-24675221-12', {
          
        });
        ga('set', 'dimension1', 'false');
        ga('set', 'dimension3', 'false');
      ga('require', 'displayfeatures');
      ga('set', 'forceSSL', true);
      ga('send', 'pageview');
    // Google Tag Manager
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      '//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TBQWPN');
  })();
</script>
</body></html>